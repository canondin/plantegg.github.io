<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>plantegg</title>
  <subtitle>java tcp mysql performance network docker Linux</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-07-16T06:27:34.141Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>weibo @plantegg</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>文章索引</title>
    <link href="http://yoursite.com/2117/06/07/%E6%96%87%E7%AB%A0%E7%B4%A2%E5%BC%95index/"/>
    <id>http://yoursite.com/2117/06/07/文章索引index/</id>
    <published>2117-06-07T10:30:03.000Z</published>
    <updated>2020-07-16T06:27:34.141Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文章索引"><a href="#文章索引" class="headerlink" title="文章索引"></a>文章索引</h1><h2 id="重点文章推荐"><a href="#重点文章推荐" class="headerlink" title="重点文章推荐"></a>重点文章推荐</h2><h4 id="《Intel-PAUSE指令变化是如何影响自旋锁以及MySQL的性能的》-从一个参数引起的rt抖动定位到OS锁等待再到CPU-Pause指令，以及不同CPU型号对Pause使用cycles不同的影响，最终反馈到应用层面的rt全过程。在MySQL内核开发的时候考虑了Pause，但是没有考虑不同的CPU型号，所以换了CPU型号后性能差异比较大"><a href="#《Intel-PAUSE指令变化是如何影响自旋锁以及MySQL的性能的》-从一个参数引起的rt抖动定位到OS锁等待再到CPU-Pause指令，以及不同CPU型号对Pause使用cycles不同的影响，最终反馈到应用层面的rt全过程。在MySQL内核开发的时候考虑了Pause，但是没有考虑不同的CPU型号，所以换了CPU型号后性能差异比较大" class="headerlink" title="《Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的》 从一个参数引起的rt抖动定位到OS锁等待再到CPU Pause指令，以及不同CPU型号对Pause使用cycles不同的影响，最终反馈到应用层面的rt全过程。在MySQL内核开发的时候考虑了Pause，但是没有考虑不同的CPU型号，所以换了CPU型号后性能差异比较大"></a><a href="https://plantegg.github.io/2019/12/16/Intel%20PAUSE%E6%8C%87%E4%BB%A4%E5%8F%98%E5%8C%96%E6%98%AF%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E8%87%AA%E6%97%8B%E9%94%81%E4%BB%A5%E5%8F%8AMySQL%E7%9A%84%E6%80%A7%E8%83%BD%E7%9A%84/" target="_blank" rel="external">《Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的》 从一个参数引起的rt抖动定位到OS锁等待再到CPU Pause指令，以及不同CPU型号对Pause使用cycles不同的影响，最终反馈到应用层面的rt全过程。在MySQL内核开发的时候考虑了Pause，但是没有考虑不同的CPU型号，所以换了CPU型号后性能差异比较大</a></h4><p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/d567449fe52725a9d0b9d4ec9baa372c.png" alt="image.png"></p>
<h4 id="10倍性能提升全过程-在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，一个性能全栈工程师如何发现各种问题的。"><a href="#10倍性能提升全过程-在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，一个性能全栈工程师如何发现各种问题的。" class="headerlink" title="10倍性能提升全过程 在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，一个性能全栈工程师如何发现各种问题的。"></a><a href="https://plantegg.github.io/2018/01/23/10+%E5%80%8D%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87%E5%85%A8%E8%BF%87%E7%A8%8B/" target="_blank" rel="external">10倍性能提升全过程</a> 在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，一个性能全栈工程师如何发现各种问题的。</h4><p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/05703c168e63e96821ea9f921d83712b.png" alt="image.png"></p>
<h4 id="就是要你懂TCP–半连接队列和全连接队列：偶发性的连接reset异常、重启服务后短时间的连接异常，通过一篇文章阐明TCP连接的半连接队列和全连接队大小是怎么影响连接创建的，以及用什么工具来观察队列有没有溢出、连接为什么会RESET"><a href="#就是要你懂TCP–半连接队列和全连接队列：偶发性的连接reset异常、重启服务后短时间的连接异常，通过一篇文章阐明TCP连接的半连接队列和全连接队大小是怎么影响连接创建的，以及用什么工具来观察队列有没有溢出、连接为什么会RESET" class="headerlink" title="就是要你懂TCP–半连接队列和全连接队列：偶发性的连接reset异常、重启服务后短时间的连接异常，通过一篇文章阐明TCP连接的半连接队列和全连接队大小是怎么影响连接创建的，以及用什么工具来观察队列有没有溢出、连接为什么会RESET"></a><a href="https://plantegg.github.io/2017/06/07/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E5%8D%8A%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97/" target="_blank" rel="external">就是要你懂TCP–半连接队列和全连接队列：偶发性的连接reset异常、重启服务后短时间的连接异常，通过一篇文章阐明TCP连接的半连接队列和全连接队大小是怎么影响连接创建的，以及用什么工具来观察队列有没有溢出、连接为什么会RESET</a></h4><p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2020/png/33359/1579241362064-807d8378-6c54-4a2c-a888-ff2337df817c.png" alt="image.png" style="zoom:80%;"></p>
<h4 id="就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的"><a href="#就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的" class="headerlink" title="就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的"></a><a href="https://plantegg.github.io/2019/09/28/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E6%80%A7%E8%83%BD%E5%92%8C%E5%8F%91%E9%80%81%E6%8E%A5%E6%94%B6Buffer%E7%9A%84%E5%85%B3%E7%B3%BB/" target="_blank" rel="external">就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的</a></h4><p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/e177d59ecb886daef5905ed80a84dfd2.png" alt=""></p>
<h4 id="就是要你懂网络–一个网络包的旅程：教科书式地阐述书本中的路由、网关、子网、Mac地址、IP地址是如何一起协作让网络包最终传输到目标机器上。-同时可以跟讲这块的RFC1180比较一下，RFC1180-写的确实很好，清晰简洁，图文并茂，结构逻辑合理，但是对于90-的程序员没有什么卵用，看完几周后就忘得差不多，因为他不是从实践的角度来阐述问题，中间没有很多为什么，所以一般资质的程序员看完当时感觉很好，实际还是不会灵活运用"><a href="#就是要你懂网络–一个网络包的旅程：教科书式地阐述书本中的路由、网关、子网、Mac地址、IP地址是如何一起协作让网络包最终传输到目标机器上。-同时可以跟讲这块的RFC1180比较一下，RFC1180-写的确实很好，清晰简洁，图文并茂，结构逻辑合理，但是对于90-的程序员没有什么卵用，看完几周后就忘得差不多，因为他不是从实践的角度来阐述问题，中间没有很多为什么，所以一般资质的程序员看完当时感觉很好，实际还是不会灵活运用" class="headerlink" title="就是要你懂网络–一个网络包的旅程：教科书式地阐述书本中的路由、网关、子网、Mac地址、IP地址是如何一起协作让网络包最终传输到目标机器上。  同时可以跟讲这块的RFC1180比较一下，RFC1180 写的确实很好，清晰简洁，图文并茂，结构逻辑合理，但是对于90%的程序员没有什么卵用，看完几周后就忘得差不多，因为他不是从实践的角度来阐述问题，中间没有很多为什么，所以一般资质的程序员看完当时感觉很好，实际还是不会灵活运用"></a><a href="https://plantegg.github.io/2019/05/15/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E7%BD%91%E7%BB%9C--%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C%E5%8C%85%E7%9A%84%E6%97%85%E7%A8%8B/" target="_blank" rel="external">就是要你懂网络–一个网络包的旅程：教科书式地阐述书本中的路由、网关、子网、Mac地址、IP地址是如何一起协作让网络包最终传输到目标机器上。</a>  同时可以跟讲这块的<a href="https://tools.ietf.org/html/rfc1180" target="_blank" rel="external">RFC1180</a>比较一下，RFC1180 写的确实很好，清晰简洁，图文并茂，结构逻辑合理，但是对于90%的程序员没有什么卵用，看完几周后就忘得差不多，因为他不是从实践的角度来阐述问题，中间没有很多为什么，所以一般资质的程序员看完当时感觉很好，实际还是不会灵活运用</h4><p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/8f5d8518c1d92ed68d23218028e3cd11.png" alt=""></p>
<h4 id="从网络路由连通性的原理上来看负载均衡lvs的DR、NAT、FullNAT到底搞了些什么鬼，以及为什么要这么搞，和带来的优缺点：《就是要你懂负载均衡–lvs和转发模式》"><a href="#从网络路由连通性的原理上来看负载均衡lvs的DR、NAT、FullNAT到底搞了些什么鬼，以及为什么要这么搞，和带来的优缺点：《就是要你懂负载均衡–lvs和转发模式》" class="headerlink" title="从网络路由连通性的原理上来看负载均衡lvs的DR、NAT、FullNAT到底搞了些什么鬼，以及为什么要这么搞，和带来的优缺点：《就是要你懂负载均衡–lvs和转发模式》"></a><a href="https://plantegg.github.io/2019/06/20/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1--lvs%E5%92%8C%E8%BD%AC%E5%8F%91%E6%A8%A1%E5%BC%8F/" target="_blank" rel="external">从网络路由连通性的原理上来看负载均衡lvs的DR、NAT、FullNAT到底搞了些什么鬼，以及为什么要这么搞，和带来的优缺点：《就是要你懂负载均衡–lvs和转发模式》</a></h4><p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/94d55b926b5bb1573c4cab8353428712.png" alt=""></p>
<h4 id="LVS-20倍的负载不均衡，原来是内核的这个Bug，这个内核bug现在还在，可以稳定重现，有兴趣的话去重现一下，然后对照源代码以及抓包分析一下就清楚了。"><a href="#LVS-20倍的负载不均衡，原来是内核的这个Bug，这个内核bug现在还在，可以稳定重现，有兴趣的话去重现一下，然后对照源代码以及抓包分析一下就清楚了。" class="headerlink" title="LVS 20倍的负载不均衡，原来是内核的这个Bug，这个内核bug现在还在，可以稳定重现，有兴趣的话去重现一下，然后对照源代码以及抓包分析一下就清楚了。"></a><a href="https://plantegg.github.io/2019/07/19/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1--%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95%E5%92%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%9D%87%E8%A1%A1/" target="_blank" rel="external">LVS 20倍的负载不均衡，原来是内核的这个Bug</a>，这个内核bug现在还在，可以稳定重现，有兴趣的话去重现一下，然后对照源代码以及抓包分析一下就清楚了。</h4><h4 id="就是要你懂TCP–握手和挥手，不是你想象中三次握手、四次挥手就理解了TCP，本文从握手的本质–握手都做了什么事情、连接的本质是什么等来阐述握手、挥手的原理"><a href="#就是要你懂TCP–握手和挥手，不是你想象中三次握手、四次挥手就理解了TCP，本文从握手的本质–握手都做了什么事情、连接的本质是什么等来阐述握手、挥手的原理" class="headerlink" title="就是要你懂TCP–握手和挥手，不是你想象中三次握手、四次挥手就理解了TCP，本文从握手的本质–握手都做了什么事情、连接的本质是什么等来阐述握手、挥手的原理"></a><a href="https://plantegg.github.io/2017/06/02/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E8%BF%9E%E6%8E%A5%E5%92%8C%E6%8F%A1%E6%89%8B/" target="_blank" rel="external">就是要你懂TCP–握手和挥手，不是你想象中三次握手、四次挥手就理解了TCP，本文从握手的本质–握手都做了什么事情、连接的本质是什么等来阐述握手、挥手的原理</a></h4><p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/6d66dadecb72e11e3e5ab765c6c3ea2e.png" alt=""></p>
<h4 id="nslookup-OK-but-ping-fail–看看老司机是如何解决问题的，解决问题的方法肯定比知识点重要多了，同时透过一个问题怎么样通篇来理解一大块知识，让这块原理真正在你的只是提示中扎根下来"><a href="#nslookup-OK-but-ping-fail–看看老司机是如何解决问题的，解决问题的方法肯定比知识点重要多了，同时透过一个问题怎么样通篇来理解一大块知识，让这块原理真正在你的只是提示中扎根下来" class="headerlink" title="nslookup OK but ping fail–看看老司机是如何解决问题的，解决问题的方法肯定比知识点重要多了，同时透过一个问题怎么样通篇来理解一大块知识，让这块原理真正在你的只是提示中扎根下来"></a><a href="https://plantegg.github.io/2019/01/09/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82ping--nslookup-OK-but-ping-fail/" target="_blank" rel="external">nslookup OK but ping fail–看看老司机是如何解决问题的，解决问题的方法肯定比知识点重要多了，同时透过一个问题怎么样通篇来理解一大块知识，让这块原理真正在你的只是提示中扎根下来</a></h4><p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/ca466bb6430f1149958ceb41b9ffe591.png" alt=""></p>
<h4 id="如何在工作中学习-一篇很土但是很务实可以复制的方法论文章。不要讲举一反三、触类旁通，谁都知道要举一反三、触类旁通，但是为什么我总是不能够举一反三、触类旁通？"><a href="#如何在工作中学习-一篇很土但是很务实可以复制的方法论文章。不要讲举一反三、触类旁通，谁都知道要举一反三、触类旁通，但是为什么我总是不能够举一反三、触类旁通？" class="headerlink" title="如何在工作中学习 一篇很土但是很务实可以复制的方法论文章。不要讲举一反三、触类旁通，谁都知道要举一反三、触类旁通，但是为什么我总是不能够举一反三、触类旁通？"></a><a href="https://plantegg.github.io/2018/05/23/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%B7%A5%E4%BD%9C%E4%B8%AD%E5%AD%A6%E4%B9%A0/" target="_blank" rel="external">如何在工作中学习</a> 一篇很土但是很务实可以复制的方法论文章。不要讲举一反三、触类旁通，谁都知道要举一反三、触类旁通，但是为什么我总是不能够举一反三、触类旁通？</h4><h2 id="性能相关"><a href="#性能相关" class="headerlink" title="性能相关"></a>性能相关</h2><h4 id="就是要你懂TCP–半连接队列和全连接队列-偶发性的连接reset异常、重启服务后短时间的连接异常"><a href="#就是要你懂TCP–半连接队列和全连接队列-偶发性的连接reset异常、重启服务后短时间的连接异常" class="headerlink" title="就是要你懂TCP–半连接队列和全连接队列  偶发性的连接reset异常、重启服务后短时间的连接异常"></a><a href="https://plantegg.github.io/2017/06/07/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E5%8D%8A%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97/" target="_blank" rel="external">就是要你懂TCP–半连接队列和全连接队列</a>  偶发性的连接reset异常、重启服务后短时间的连接异常</h4><h4 id="就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的-发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响"><a href="#就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的-发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响" class="headerlink" title="就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的  发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响"></a><a href="https://plantegg.github.io/2019/09/28/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E6%80%A7%E8%83%BD%E5%92%8C%E5%8F%91%E9%80%81%E6%8E%A5%E6%94%B6Buffer%E7%9A%84%E5%85%B3%E7%B3%BB/" target="_blank" rel="external">就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的</a>  发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响</h4><h4 id="就是要你懂TCP–性能优化大全"><a href="#就是要你懂TCP–性能优化大全" class="headerlink" title="就是要你懂TCP–性能优化大全"></a><a href="https://plantegg.github.io/2019/06/21/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%A4%A7%E5%85%A8/" target="_blank" rel="external">就是要你懂TCP–性能优化大全</a></h4><h4 id="就是要你懂TCP–TCP性能问题-Nagle算法和delay-ack"><a href="#就是要你懂TCP–TCP性能问题-Nagle算法和delay-ack" class="headerlink" title="就是要你懂TCP–TCP性能问题 Nagle算法和delay ack"></a><a href="https://plantegg.github.io/2018/06/14/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E6%9C%80%E7%BB%8F%E5%85%B8%E7%9A%84TCP%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98/" target="_blank" rel="external">就是要你懂TCP–TCP性能问题</a> Nagle算法和delay ack</h4><h4 id="10倍性能提升全过程-在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，看看一个性能全栈工程师如何在各种工具加持下发现各种问题的。"><a href="#10倍性能提升全过程-在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，看看一个性能全栈工程师如何在各种工具加持下发现各种问题的。" class="headerlink" title="10倍性能提升全过程 在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，看看一个性能全栈工程师如何在各种工具加持下发现各种问题的。"></a><a href="https://plantegg.github.io/2018/01/23/10+%E5%80%8D%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87%E5%85%A8%E8%BF%87%E7%A8%8B/" target="_blank" rel="external">10倍性能提升全过程</a> 在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，看看一个性能全栈工程师如何在各种工具加持下发现各种问题的。</h4><h2 id="网络相关基础知识"><a href="#网络相关基础知识" class="headerlink" title="网络相关基础知识"></a>网络相关基础知识</h2><h4 id="就是要你懂网络–一个网络包的旅程"><a href="#就是要你懂网络–一个网络包的旅程" class="headerlink" title="就是要你懂网络–一个网络包的旅程"></a><a href="https://plantegg.github.io/2019/05/15/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E7%BD%91%E7%BB%9C--%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C%E5%8C%85%E7%9A%84%E6%97%85%E7%A8%8B/" target="_blank" rel="external">就是要你懂网络–一个网络包的旅程</a></h4><h4 id="通过案例来理解MSS、MTU等相关TCP概念"><a href="#通过案例来理解MSS、MTU等相关TCP概念" class="headerlink" title="通过案例来理解MSS、MTU等相关TCP概念"></a><a href="https://plantegg.github.io/2018/05/07/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E9%80%9A%E8%BF%87%E6%A1%88%E4%BE%8B%E6%9D%A5%E5%AD%A6%E4%B9%A0MSS%E3%80%81MTU/" target="_blank" rel="external">通过案例来理解MSS、MTU等相关TCP概念</a></h4><h4 id="就是要你懂TCP–握手和挥手"><a href="#就是要你懂TCP–握手和挥手" class="headerlink" title="就是要你懂TCP–握手和挥手"></a><a href="https://plantegg.github.io/2017/06/02/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E8%BF%9E%E6%8E%A5%E5%92%8C%E6%8F%A1%E6%89%8B/" target="_blank" rel="external">就是要你懂TCP–握手和挥手</a></h4><h4 id="wireshark-dup-ack-issue-and-keepalive"><a href="#wireshark-dup-ack-issue-and-keepalive" class="headerlink" title="wireshark-dup-ack-issue and keepalive"></a><a href="https://plantegg.github.io/2017/06/02/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--wireshark-dup-ack-issue/" target="_blank" rel="external">wireshark-dup-ack-issue and keepalive</a></h4><h4 id="一个没有遵守tcp规则导致的问题"><a href="#一个没有遵守tcp规则导致的问题" class="headerlink" title="一个没有遵守tcp规则导致的问题"></a><a href="https://plantegg.github.io/2017/08/03/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E4%B8%80%E4%B8%AA%E6%B2%A1%E6%9C%89%E9%81%B5%E5%AE%88tcp%E8%A7%84%E5%88%99%E5%AF%BC%E8%87%B4%E7%9A%84%E9%97%AE%E9%A2%98/" target="_blank" rel="external">一个没有遵守tcp规则导致的问题</a></h4><h2 id="DNS相关"><a href="#DNS相关" class="headerlink" title="DNS相关"></a>DNS相关</h2><h4 id="就是要你懂DNS–一文搞懂域名解析相关问题"><a href="#就是要你懂DNS–一文搞懂域名解析相关问题" class="headerlink" title="就是要你懂DNS–一文搞懂域名解析相关问题"></a><a href="https://plantegg.github.io/2019/06/09/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82DNS--%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/" target="_blank" rel="external">就是要你懂DNS–一文搞懂域名解析相关问题</a></h4><h4 id="nslookup-OK-but-ping-fail"><a href="#nslookup-OK-but-ping-fail" class="headerlink" title="nslookup OK but ping fail"></a><a href="https://plantegg.github.io/2019/01/09/nslookup-OK-but-ping-fail/" target="_blank" rel="external">nslookup OK but ping fail</a></h4><h4 id="Docker中的DNS解析过程"><a href="#Docker中的DNS解析过程" class="headerlink" title="Docker中的DNS解析过程"></a><a href="https://plantegg.github.io/2019/01/12/Docker%E4%B8%AD%E7%9A%84DNS%E8%A7%A3%E6%9E%90%E8%BF%87%E7%A8%8B/" target="_blank" rel="external">Docker中的DNS解析过程</a></h4><h4 id="windows7的wifi总是报DNS域名异常无法上网"><a href="#windows7的wifi总是报DNS域名异常无法上网" class="headerlink" title="windows7的wifi总是报DNS域名异常无法上网"></a><a href="https://plantegg.github.io/2019/01/10/windows7%E7%9A%84wifi%E6%80%BB%E6%98%AF%E6%8A%A5DNS%E5%9F%9F%E5%90%8D%E5%BC%82%E5%B8%B8%E6%97%A0%E6%B3%95%E4%B8%8A%E7%BD%91/" target="_blank" rel="external">windows7的wifi总是报DNS域名异常无法上网</a></h4><h2 id="LVS-负载均衡"><a href="#LVS-负载均衡" class="headerlink" title="LVS 负载均衡"></a>LVS 负载均衡</h2><h4 id="就是要你懂负载均衡–lvs和转发模式"><a href="#就是要你懂负载均衡–lvs和转发模式" class="headerlink" title="就是要你懂负载均衡–lvs和转发模式"></a><a href="https://plantegg.github.io/2019/06/20/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1--lvs%E5%92%8C%E8%BD%AC%E5%8F%91%E6%A8%A1%E5%BC%8F/" target="_blank" rel="external">就是要你懂负载均衡–lvs和转发模式</a></h4><h4 id="就是要你懂负载均衡–负载均衡调度算法和为什么不均衡"><a href="#就是要你懂负载均衡–负载均衡调度算法和为什么不均衡" class="headerlink" title="就是要你懂负载均衡–负载均衡调度算法和为什么不均衡"></a><a href="https://plantegg.github.io/2019/07/19/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1--%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95%E5%92%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%9D%87%E8%A1%A1/" target="_blank" rel="external">就是要你懂负载均衡–负载均衡调度算法和为什么不均衡</a></h4><h2 id="网络工具"><a href="#网络工具" class="headerlink" title="网络工具"></a>网络工具</h2><h4 id="就是要你懂Unix-Socket-进行抓包解析"><a href="#就是要你懂Unix-Socket-进行抓包解析" class="headerlink" title="就是要你懂Unix Socket 进行抓包解析"></a><a href="https://plantegg.github.io/2019/04/04/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E6%8A%93%E5%8C%85--Unix-Socket%E6%8A%93%E5%8C%85/" target="_blank" rel="external">就是要你懂Unix Socket 进行抓包解析</a></h4><h4 id="就是要你懂网络监控–ss用法大全"><a href="#就是要你懂网络监控–ss用法大全" class="headerlink" title="就是要你懂网络监控–ss用法大全"></a><a href="https://plantegg.github.io/2019/07/12/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E7%BD%91%E7%BB%9C%E7%9B%91%E6%8E%A7--ss%E7%94%A8%E6%B3%95%E5%A4%A7%E5%85%A8/" target="_blank" rel="external">就是要你懂网络监控–ss用法大全</a></h4><h4 id="就是要你懂抓包–WireShark之命令行版tshark"><a href="#就是要你懂抓包–WireShark之命令行版tshark" class="headerlink" title="就是要你懂抓包–WireShark之命令行版tshark"></a><a href="https://plantegg.github.io/2019/06/21/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E6%8A%93%E5%8C%85--WireShark%E4%B9%8B%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%89%88tshark/" target="_blank" rel="external">就是要你懂抓包–WireShark之命令行版tshark</a></h4><h4 id="netstat-timer-keepalive-explain"><a href="#netstat-timer-keepalive-explain" class="headerlink" title="netstat timer keepalive explain"></a><a href="https://plantegg.github.io/2017/08/28/netstat%20--timer/" target="_blank" rel="external">netstat timer keepalive explain</a></h4><h4 id="Git-HTTP-Proxy-and-SSH-Proxy"><a href="#Git-HTTP-Proxy-and-SSH-Proxy" class="headerlink" title="Git HTTP Proxy and SSH Proxy"></a><a href="https://plantegg.github.io/2018/03/14/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82git%E4%BB%A3%E7%90%86--%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AEgit%20Proxy/" target="_blank" rel="external">Git HTTP Proxy and SSH Proxy</a></h4>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;文章索引&quot;&gt;&lt;a href=&quot;#文章索引&quot; class=&quot;headerlink&quot; title=&quot;文章索引&quot;&gt;&lt;/a&gt;文章索引&lt;/h1&gt;&lt;h2 id=&quot;重点文章推荐&quot;&gt;&lt;a href=&quot;#重点文章推荐&quot; class=&quot;headerlink&quot; title=&quot;重点文章推
    
    </summary>
    
      <category term="TCP" scheme="http://yoursite.com/categories/TCP/"/>
    
    
      <category term="performance" scheme="http://yoursite.com/tags/performance/"/>
    
      <category term="network" scheme="http://yoursite.com/tags/network/"/>
    
      <category term="tcpdump" scheme="http://yoursite.com/tags/tcpdump/"/>
    
      <category term="LVS" scheme="http://yoursite.com/tags/LVS/"/>
    
      <category term="TCP queue" scheme="http://yoursite.com/tags/TCP-queue/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2020/08/29/%E7%AD%94%E7%96%91%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/"/>
    <id>http://yoursite.com/2020/08/29/答疑问题汇总/</id>
    <published>2020-08-29T09:39:11.175Z</published>
    <updated>2019-10-25T05:15:38.885Z</updated>
    
    <content type="html"><![CDATA[<h1 id="答疑问题汇总"><a href="#答疑问题汇总" class="headerlink" title="答疑问题汇总"></a>答疑问题汇总</h1><hr>
<h2 id="天津一网通-vxlan-网络始终不通，宿主机能抓到发出去的包，但是抓不到回复包。对端容器所在的宿主机抓不到进来的包"><a href="#天津一网通-vxlan-网络始终不通，宿主机能抓到发出去的包，但是抓不到回复包。对端容器所在的宿主机抓不到进来的包" class="headerlink" title="天津一网通 vxlan 网络始终不通，宿主机能抓到发出去的包，但是抓不到回复包。对端容器所在的宿主机抓不到进来的包"></a>天津一网通 vxlan 网络始终不通，宿主机能抓到发出去的包，但是抓不到回复包。对端容器所在的宿主机抓不到进来的包</h2><blockquote>
<p>一定是网络上把这个包扔掉了</p>
</blockquote>
<h4 id="证明问题"><a href="#证明问题" class="headerlink" title="证明问题"></a>证明问题</h4><ul>
<li>先选择两台宿主机，停掉上面的 ovs 容器(腾出4789端口)</li>
<li>一台宿主机上执行： nc -l -u 4789 //在4789端口上启动udp服务</li>
<li>另外一台主机上执行： nc -u 第一台宿主机的IP 4789 //从第二台宿主机连第一台的4789端口</li>
<li>从两边都发送一些内容看看，看是否能到达对方</li>
</ul>
<p><strong>如果通过nc发送的东西也无法到达对方（跟方舟没有关系了）那么就是链路上的问题</strong></p>
<hr>
<h2 id="天津一网通-vxlan-网络能通，但是pca容器初始化的时候失败"><a href="#天津一网通-vxlan-网络能通，但是pca容器初始化的时候失败" class="headerlink" title="天津一网通 vxlan 网络能通，但是pca容器初始化的时候失败"></a>天津一网通 vxlan 网络能通，但是pca容器初始化的时候失败</h2><p>通过报错信息发现pca容器访问数据库SocketTimeout，同时看到异常信息都是Timeout大于15分钟以上了。</p>
<h4 id="需找问题"><a href="#需找问题" class="headerlink" title="需找问题"></a>需找问题</h4><ul>
<li>先在 pca容器和数据库容器互相 ping 证明网络没有问题，能够互通</li>
<li>在 pca 容器中通过mysql 命令行连上 mysql，并创建table，insert一些记录，结果也没有问题</li>
<li>抓包发现pca容器访问数据库的时候在重传包（以往经验）</li>
</ul>
<p><img src="http://img4.tbcdn.cn/L1/461/1/1d010b9937198aee9e798bb02913603874f19ddc" alt="screenshot"></p>
<h4 id="细化证明问题"><a href="#细化证明问题" class="headerlink" title="细化证明问题"></a>细化证明问题</h4><ul>
<li>ping -s -M 尝试发送1460大小的包</li>
<li>检查宿主机、容器MTU设置</li>
</ul>
<p><strong>确认问题在宿主机网卡MTU设置为1350</strong>，从而导致容器发出的包被宿主机网卡丢掉</p>
<h2 id="居然之家通过vpn部署好中间件后，修改笔记本的dns设置后通过浏览器来访问中间件的console，但是报找不到server。同时在cmd中ping-这个域名能通，但是nslookup解析不了这个域名"><a href="#居然之家通过vpn部署好中间件后，修改笔记本的dns设置后通过浏览器来访问中间件的console，但是报找不到server。同时在cmd中ping-这个域名能通，但是nslookup解析不了这个域名" class="headerlink" title="居然之家通过vpn部署好中间件后，修改笔记本的dns设置后通过浏览器来访问中间件的console，但是报找不到server。同时在cmd中ping 这个域名能通，但是nslookup解析不了这个域名"></a>居然之家通过vpn部署好中间件后，修改笔记本的dns设置后通过浏览器来访问中间件的console，但是报找不到server。同时在cmd中ping 这个域名能通，但是nslookup解析不了这个域名</h2><p>ping 这个域名能通，但是nslookup不行，基本可以确认网络没有大问题，之所以ping可以nslookup不行，是因为他们底层取dns server的逻辑不一样。</p>
<p>先检查dns设置：</p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/d4634f74c0b0b38f784a1657864d5089.png" alt="image.png"><br>如上图，配置的填写</p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/d4a9cddf56d23059f98850c7c0bcf067.png" alt="image.png"></p>
<p>多出来一个127.0.0.1肯定有问题，明明配置的时候只填了114.114.114.114. nslookup、浏览器默认把域名解析丢给了127.0.0.1，但是 ping丢给了114.114.114.114，所以看到如上描述的结果。</p>
<p>经过思考发现应该是本机同时运行了easyconnect（vpn软件），127.0.0.1 是他强行塞进来的。马上停掉easyconnect再ipconfig /all 验证一下这个时候的dns server，果然127.0.0.1不见了。</p>
<h2 id="中航信windows下通过方舟dns域名解析不了方舟域名，但是宿主机上可以。windows机器能ping通dns-server-ip-但是nslookup-解析不了域名，显示request-time-out"><a href="#中航信windows下通过方舟dns域名解析不了方舟域名，但是宿主机上可以。windows机器能ping通dns-server-ip-但是nslookup-解析不了域名，显示request-time-out" class="headerlink" title="中航信windows下通过方舟dns域名解析不了方舟域名，但是宿主机上可以。windows机器能ping通dns server ip, 但是nslookup 解析不了域名，显示request time out"></a>中航信windows下通过方舟dns域名解析不了方舟域名，但是宿主机上可以。windows机器能ping通dns server ip, 但是nslookup 解析不了域名，显示request time out</h2><p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/548975c04a8587e0fb33b5722b1a38f2.png" alt="image.png"></p>
<p>能ping通说明网络能通，但是dns域名要能解析依赖于：</p>
<ul>
<li>网络能通</li>
<li>dns server上有dns服务（53udp端口）</li>
<li>中间的防火墙对这个udp53端口放行了</li>
</ul>
<p>这里的问题非常明显是中间的防火墙没放行 udp 53端口</p>
<h2 id="方舟环境在ECS底座上DNS会继承rotate模式，导致域名解析不正常，ping-域名不通，但是nslookup能通"><a href="#方舟环境在ECS底座上DNS会继承rotate模式，导致域名解析不正常，ping-域名不通，但是nslookup能通" class="headerlink" title="方舟环境在ECS底座上DNS会继承rotate模式，导致域名解析不正常，ping 域名不通，但是nslookup能通"></a>方舟环境在ECS底座上DNS会继承rotate模式，导致域名解析不正常，ping 域名不通，但是nslookup能通</h2><p><a href="https://www.atatech.org/articles/93688" target="_blank" rel="external">nslookup 域名结果正确，但是 ping 域名失败</a></p>
<h2 id="民生银行POC环境物理机搬迁到新机房后网络不通，通过在物理机上抓包，抓不到任何容器的包"><a href="#民生银行POC环境物理机搬迁到新机房后网络不通，通过在物理机上抓包，抓不到任何容器的包" class="headerlink" title="民生银行POC环境物理机搬迁到新机房后网络不通，通过在物理机上抓包，抓不到任何容器的包"></a>民生银行POC环境物理机搬迁到新机房后网络不通，通过在物理机上抓包，抓不到任何容器的包</h2><p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/896f8f14d3be725515f192ed64542cb0.png" alt="image.png"></p>
<p><strong>如图所示容器中发了 arp包（IP 10.100.2.2 寻找10.100.2.1 的mac地址），这个包从bond0 网卡发出去了，也是带的正确的 vlanid 1011，但是交换机没有回复，那么就一定是交换机上vlan配置不对，需要找分配这个vlan的网工来检查交换机的配置</strong></p>
<font color="red" size="4"><strong>能抓到进出的容器包–外部环境正确，方舟底座的问题</strong></font>

<font color="red" size="4"><strong>不能抓到出去的容器包–方舟底座的问题</strong></font>

<font color="red" size="4"><strong>能抓到出去的容器包，抓不到回来的包–外部环境的问题</strong></font>

<p>所以这里是方舟底座的问题。检查ovs、vlan插件一切都正常，见鬼了</p>
<p>检查宿主机网卡状态，发现没插网线，<strong>如果容器所用的宿主机网卡没有插网线，那么ovs不会转发任何包到宿主机网卡</strong>。</p>
<h2 id="一台应用服务器无法访问部分drds-server"><a href="#一台应用服务器无法访问部分drds-server" class="headerlink" title="一台应用服务器无法访问部分drds-server"></a><a href="https://aone.alibaba-inc.com/task/9753887" target="_blank" rel="external">一台应用服务器无法访问部分drds-server</a></h2><p>应用机器： 10.100.10.201 这台机器抛502异常比较多，进一步诊断发现 ping youku.tddl.tbsite.net 的时候解析到 10.100.53.15/16就不通</p>
<p>直接ping 10.100.53.15/16 也不通，经过诊断发现是交换机上记录了两个 10.100.10.201的mac地址导致网络不通。</p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/9deff3045e3213df81c3ad785cfddefa.gif" alt="youku-mac-ip.gif"></p>
<p><strong>上图是不通的IP，下图是正常IP</strong></p>
<p>经过调查发现是土豆业务也用了10.100.10.201这个IP导致交换机的ARP mac table冲突，土豆删除这个IP后故障就恢复了。</p>
<h3 id="当时交换机上发现的两条记录："><a href="#当时交换机上发现的两条记录：" class="headerlink" title="当时交换机上发现的两条记录："></a>当时交换机上发现的两条记录：</h3><pre><code>00:18:51:38:b1:cd 10.100.10.201 
8c:dc:d4:b3:af:14 10.100.10.201
</code></pre><h2 id="针对中移在线这个客户默认修改了umask导致黑屏脚本权限不够，部署中途不断卡壳，直接在黑屏脚本中修复了admin这个用户的umask"><a href="#针对中移在线这个客户默认修改了umask导致黑屏脚本权限不够，部署中途不断卡壳，直接在黑屏脚本中修复了admin这个用户的umask" class="headerlink" title="针对中移在线这个客户默认修改了umask导致黑屏脚本权限不够，部署中途不断卡壳，直接在黑屏脚本中修复了admin这个用户的umask"></a>针对中移在线这个客户默认修改了umask导致黑屏脚本权限不够，部署中途不断卡壳，直接在黑屏脚本中修复了admin这个用户的umask</h2><ol>
<li>客户环境的 umask 是 0027 会导致所有copy文件的权限都不对了</li>
<li>因为admin没权限执行 /bin/jq 导致daemon.json是空的</li>
<li>/etc/docker/daemon.json 文件是空的，docker启动报错</li>
</ol>
<h2 id="修复centos下udp和批量处理脚本因为环境变量的确实不能执行modprobe和ping等等命令的问题，同时将alios的这块修复逻辑放到了方舟安装脚本中，init的时候会先把这个问题修复"><a href="#修复centos下udp和批量处理脚本因为环境变量的确实不能执行modprobe和ping等等命令的问题，同时将alios的这块修复逻辑放到了方舟安装脚本中，init的时候会先把这个问题修复" class="headerlink" title="修复centos下udp和批量处理脚本因为环境变量的确实不能执行modprobe和ping等等命令的问题，同时将alios的这块修复逻辑放到了方舟安装脚本中，init的时候会先把这个问题修复"></a>修复centos下udp和批量处理脚本因为环境变量的确实不能执行modprobe和ping等等命令的问题，同时将alios的这块修复逻辑放到了方舟安装脚本中，init的时候会先把这个问题修复</h2><p><a href="https://www.atatech.org/articles/105673" target="_blank" rel="external">Linux环境变量问题汇总</a></p>
<h2 id="Centos系统重启后-etc-resolv-conf总是被还原，开始以为是系统Bug，研究后发现是可以配置的，dhcp默认会每次重启后拉取DNS自动更新-etc-resolv-conf"><a href="#Centos系统重启后-etc-resolv-conf总是被还原，开始以为是系统Bug，研究后发现是可以配置的，dhcp默认会每次重启后拉取DNS自动更新-etc-resolv-conf" class="headerlink" title="Centos系统重启后 /etc/resolv.conf总是被还原，开始以为是系统Bug，研究后发现是可以配置的，dhcp默认会每次重启后拉取DNS自动更新 /etc/resolv.conf"></a>Centos系统重启后 /etc/resolv.conf总是被还原，开始以为是系统Bug，研究后发现是可以配置的，dhcp默认会每次重启后拉取DNS自动更新 /etc/resolv.conf</h2><h2 id="MonkeyKing-burn-cpu-mkt-burncpu-sh-脚本在方舟服务器上运行一段时间后，进程不见了，MK团队认为是方舟杀掉了他们。"><a href="#MonkeyKing-burn-cpu-mkt-burncpu-sh-脚本在方舟服务器上运行一段时间后，进程不见了，MK团队认为是方舟杀掉了他们。" class="headerlink" title="MonkeyKing burn cpu:  mkt-burncpu.sh 脚本在方舟服务器上运行一段时间后，进程不见了，MK团队认为是方舟杀掉了他们。"></a>MonkeyKing burn cpu:  mkt-burncpu.sh 脚本在方舟服务器上运行一段时间后，进程不见了，MK团队认为是方舟杀掉了他们。</h2><p>好奇心看代码、<strong>看openssl测试输出日志</strong>：内部调用 openssl speed 测试cpu的速度，这个测试一轮跑完了opessl就结束了，本身不是死循环一直跑, 不是方舟杀掉的</p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/7860a67e52c0de0febd7ec944a4b1517.png" alt="image.png"></p>
<h2 id="重庆长安汽车部署过程中愚公不能正常启动，怀疑是依赖的zk问题，zk网络访问正常"><a href="#重庆长安汽车部署过程中愚公不能正常启动，怀疑是依赖的zk问题，zk网络访问正常" class="headerlink" title="重庆长安汽车部署过程中愚公不能正常启动，怀疑是依赖的zk问题，zk网络访问正常"></a>重庆长安汽车部署过程中愚公不能正常启动，怀疑是依赖的zk问题，zk网络访问正常</h2><p>尝试telnet zk发现不通，客户现场安装了kerberos导致telnet测试有问题（telnet被kerberos替换过）,换一个其他环境的telnet就可以了（md5sum、telnet –help）</p>
<h2 id="开发反应两个容器之间的网络不稳定，偶尔报连不上某些容器"><a href="#开发反应两个容器之间的网络不稳定，偶尔报连不上某些容器" class="headerlink" title="开发反应两个容器之间的网络不稳定，偶尔报连不上某些容器"></a><a href="https://aone.alibaba-inc.com/issue/10403085" target="_blank" rel="external">开发反应两个容器之间的网络不稳定，偶尔报连不上某些容器</a></h2><p>主要是出现在tlog-console访问hbase容器的时候报连接异常</p>
<ol>
<li>在 task_1114_g0_tlog-console_tlog_1（10.16.11.131） 的56789 端口上启动了一个简单的http服务，然后从 task_1114_g0_tlog-hbase_tlog（10.16.11.108） 每秒钟去访问一下10.16.11.131:56789 , 如果丢包率很高的时候服务 10.16.11.131:56789 也很慢或者访问不到就是网络问题，否则就有可能是hbase服务不响应导致的丢包、网络不通（仅仅是影响hbase服务） </li>
<li>反过来在hbase上同样启动http服务，tlog-console不停地去get</li>
<li>整个过程我的http服务响应非常迅速稳定，从没出现过异常</li>
<li>在重现问题侯，贺飞发现 是tlog线程数目先增多，retran才逐渐增高的， retran升高，并没有影响在那台机器上ping 或者telnet hbase的服务</li>
<li>最终确认跟容器、网络无关，是应用本身的问题，交由产品开发继续解决</li>
</ol>
<h4 id="最终开发确认网络没有问题后一门心思闷头自查得出结论："><a href="#最终开发确认网络没有问题后一门心思闷头自查得出结论：" class="headerlink" title="最终开发确认网络没有问题后一门心思闷头自查得出结论："></a>最终开发确认网络没有问题后一门心思闷头自查得出结论：</h4><p>信息更新：</p>
<p>问题：<br>tlog-console进程线程数多，卡在连接hbase上的问题</p>
<p>直接原因：</p>
<ol>
<li>tlog-console有巡检程序，每m分钟会检查运行超过n秒的线程，并且中断这个线程； 这个操作直接导致hbase客户端在等待hbaseserver返回数据的时候被中断，这种中断会经常发生，累积久了，就会打爆tlog-console服务的线程数目，这时候，tlogconsole机器的retran就会变多，连接hbaseserver就会出问题， 具体的机理不明</li>
</ol>
<p>解决问题的有效操作：</p>
<ol>
<li>停止对tlog-console的巡检程序后，问题没有发生过</li>
</ol>
<p>其他潜在问题，这些问题是检查问题的时候，发现的其他潜在问题，已经反馈给tlog团队：<br>a. Htable实例不是线程安全,有逻辑多线程使用相同的htable实例<br>b. 程序中有new HTable 不close的路径</p>
<h2 id="中国石化私有云DRDS扩容总是报资源不足，主要是因为有些drds-server容器指定了–cpu-shares-128-相当于4Core-导致物理机CPU不够。现场将所有容器的–cpu-shares改成2后修复这个问题，但是最终需要产品方不要使用这个参数，不但会导致CPU不足，还会导致可能部分容器竞争不到CPU，进而出现很难排查的诡异问题"><a href="#中国石化私有云DRDS扩容总是报资源不足，主要是因为有些drds-server容器指定了–cpu-shares-128-相当于4Core-导致物理机CPU不够。现场将所有容器的–cpu-shares改成2后修复这个问题，但是最终需要产品方不要使用这个参数，不但会导致CPU不足，还会导致可能部分容器竞争不到CPU，进而出现很难排查的诡异问题" class="headerlink" title="中国石化私有云DRDS扩容总是报资源不足，主要是因为有些drds-server容器指定了–cpu-shares=128(相当于4Core), 导致物理机CPU不够。现场将所有容器的–cpu-shares改成2后修复这个问题，但是最终需要产品方不要使用这个参数，不但会导致CPU不足，还会导致可能部分容器竞争不到CPU，进而出现很难排查的诡异问题"></a>中国石化私有云DRDS扩容总是报资源不足，主要是因为有些drds-server容器指定了–cpu-shares=128(相当于4Core), 导致物理机CPU不够。现场将所有容器的–cpu-shares改成2后修复这个问题，但是最终需要产品方不要使用这个参数，不但会导致CPU不足，还会导致可能部分容器竞争不到CPU，进而出现很难排查的诡异问题</h2><h2 id="mq-diamond的异常日志总是打爆磁盘。mq-diamond-容器一天输出500G日志的问题，本质是调用的依赖不可用了，导致mq-diamond-频繁输出日志，两天就用掉了1T磁盘"><a href="#mq-diamond的异常日志总是打爆磁盘。mq-diamond-容器一天输出500G日志的问题，本质是调用的依赖不可用了，导致mq-diamond-频繁输出日志，两天就用掉了1T磁盘" class="headerlink" title="mq-diamond的异常日志总是打爆磁盘。mq-diamond 容器一天输出500G日志的问题，本质是调用的依赖不可用了，导致mq-diamond 频繁输出日志，两天就用掉了1T磁盘."></a>mq-diamond的异常日志总是打爆磁盘。mq-diamond 容器一天输出500G日志的问题，本质是调用的依赖不可用了，导致mq-diamond 频繁输出日志，两天就用掉了1T磁盘.</h2><p>这里有两个问题需要处理：</p>
<ol>
<li>mq-diamond 依赖的服务可用； </li>
<li>mq-diamond 自身保护，不要被自己的日志把磁盘撑爆了  </li>
</ol>
<p>对于问题二修改log4j来保护；对于问题1查看异常内容，mq-diamond尝试连接server：ip1,ip2,ip3 正常这里应该是一个ip而不是三个ip放一起。判断是mq-diamond从mq-cai获取diamond iplist有问题，这个iplist应该放在三行，但是实际被放到了1行，用逗号隔开</p>
<p>手工修改这个文件，放到三行，问题没完，还是异常，我自己崩溃没管。最后听ma-diamond的开发讲他们取iplist的url比较特殊，是自己定义的，所以我修改的地方不起作用。<strong>反思，为什么异常的时候不去看看Nginx的access日志？</strong></p>
<h2 id="内核migration进程bug导致宿主机Load非常高，同时CPU-idle也很高（两者矛盾）"><a href="#内核migration进程bug导致宿主机Load非常高，同时CPU-idle也很高（两者矛盾）" class="headerlink" title="内核migration进程bug导致宿主机Load非常高，同时CPU idle也很高（两者矛盾）"></a>内核migration进程bug导致宿主机Load非常高，同时CPU idle也很高（两者矛盾）</h2><p><a href="https://aone.alibaba-inc.com/issue/12510664" target="_blank" rel="external">内核migration进程bug导致对应的CPU核卡死</a>（图一），这个核上的所有进程得不到执行（Load高，CPU没有任何消耗， 图二），直到内核进程 watchdog 发现这个问题并恢复它。</p>
<p>出现这个bug后的症状，通过top命令看到CPU没有任何消耗但是Load偏高，如果应用进程恰好被调度到这个出问题的CPU核上，那么这个进程会卡住（大概20秒）没有任何响应，比如 ping 进程（图三图四），watchdog恢复这个问题后，多个网络包在同一时间全部通。其实所影响的不仅仅是网络卡顿，中间件容器里面的服务如果调度到这个CPU核上同样得不到执行，从外面就是感觉容器不响应了</p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/f4843725cf82e257fa14fd3742c2f9ce.png" alt="image.png"></p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/11d6db76c6de822385c0f63d2bf6eb03.png" alt="image.png"></p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/ac9e2eb1b01976cefa1b74dcddd23885.png" alt="image.png"></p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/371870b7db916e3edf515beec3a80bda.png" alt="image.png"></p>
<p>拿如上证据求助内核开发</p>
<p>关键信息在这里：<br>代码第297行<br>2017-09-15T06:52:37.820783+00:00 ascliveedas4.sgdc kernel: [598346.499872] WARNING: at net/sched/sch_generic.c:297 dev_watchdog+0x270/0x280()<br>2017-09-15T06:52:37.820784+00:00 ascliveedas4.sgdc kernel: [598346.499873] NETDEV WATCHDOG: ens2f0 (ixgbe): transmit queue 28 timed out</p>
<p>kernel version: kernel-3.10.0-327.22.2.el7.src.rpm</p>
<pre><code>265 static void dev_watchdog(unsigned long arg)
266 {
267 struct net_device *dev = (struct net_device *)arg;
268
269 netif_tx_lock(dev);
270 if (!qdisc_tx_is_noop(dev)) {
271 if (netif_device_present(dev) &amp;&amp;
272 netif_running(dev) &amp;&amp;
273 netif_carrier_ok(dev)) {
274 int some_queue_timedout = 0;
275 unsigned int i;
276 unsigned long trans_start;
277
278 for (i = 0; i &lt; dev-&gt;num_tx_queues; i++) {
279 struct netdev_queue *txq;
280
281 txq = netdev_get_tx_queue(dev, i);
282 /*
283  * old device drivers set dev-&gt;trans_start
284  */
285 trans_start = txq-&gt;trans_start ? : dev-&gt;trans_start;
286 if (netif_xmit_stopped(txq) &amp;&amp;
287 time_after(jiffies, (trans_start +
288  dev-&gt;watchdog_timeo))) {
289 some_queue_timedout = 1;
290 txq-&gt;trans_timeout++;
291 break;
292 }
293 }
294
295 if (some_queue_timedout) {
296 WARN_ONCE(1, KERN_INFO &quot;NETDEV WATCHDOG: %s (%s): transmit queue %u timed out\n&quot;,
297dev-&gt;name, netdev_drivername(dev), i);
298 dev-&gt;netdev_ops-&gt;ndo_tx_timeout(dev);
299 }
300 if (!mod_timer(&amp;dev-&gt;watchdog_timer,
301round_jiffies(jiffies +
302  dev-&gt;watchdog_timeo)))
303 dev_hold(dev);
304 }





$ cat  kernel_log.0915
2017-09-15T02:19:55.975310+00:00 ascliveedas4.sgdc kernel: [582026.288227] openvswitch: netlink: Key type 62 is out of range max 22
2017-09-15T03:49:41.312168+00:00 ascliveedas4.sgdc kernel: [587409.546584] md: md0: data-check interrupted.
2017-09-15T06:52:37.820782+00:00 ascliveedas4.sgdc kernel: [598346.499865] ------------[ cut here ]------------
2017-09-15T06:52:37.820783+00:00 ascliveedas4.sgdc kernel: [598346.499872] WARNING: at net/sched/sch_generic.c:297 dev_watchdog+0x270/0x280()
2017-09-15T06:52:37.820784+00:00 ascliveedas4.sgdc kernel: [598346.499873] NETDEV WATCHDOG: ens2f0 (ixgbe): transmit queue 28 timed out
2017-09-15T06:52:37.820784+00:00 ascliveedas4.sgdc kernel: [598346.499916] Modules linked in: 8021q garp mrp xt_nat veth xt_addrtype ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 iptable_filter xt_conntrack nf_nat nf_conntrack bridge stp llc tcp_diag udp_diag inet_diag binfmt_misc overlay() vfat fat intel_powerclamp coretemp intel_rapl kvm_intel kvm crc32_pclmul ghash_clmulni_intel aesni_intel lrw gf128mul glue_helper ablk_helper cryptd raid10 ipmi_devintf iTCO_wdt iTCO_vendor_support sb_edac lpc_ich hpwdt edac_core hpilo i2c_i801 ipmi_si sg mfd_core pcspkr ioatdma ipmi_msghandler acpi_power_meter shpchp wmi pcc_cpufreq openvswitch libcrc32c nfsd auth_rpcgss nfs_acl lockd grace sunrpc ip_tables ext4 mbcache jbd2 sd_mod crc_t10dif crct10dif_generic mgag200 syscopyarea sysfillrect sysimgblt drm_kms_helper ixgbe crct10dif_pclmul ahci ttm crct10dif_common igb crc32c_intel mdio libahci ptp drm pps_core i2c_algo_bit libata i2c_core dca dm_mirror dm_region_hash dm_log dm_mod
2017-09-15T06:52:37.820786+00:00 ascliveedas4.sgdc kernel: [598346.499928] CPU: 10 PID: 123 Comm: migration/10 Tainted: G L ------------ T 3.10.0-327.22.2.el7.x86_64#1
2017-09-15T06:52:37.820787+00:00 ascliveedas4.sgdc kernel: [598346.499929] Hardware name: HP ProLiant DL160 Gen9/ProLiant DL160 Gen9, BIOS U20 12/27/2015
2017-09-15T06:52:37.820788+00:00 ascliveedas4.sgdc kernel: [598346.499935]  ffff88207fc43d88 000000001cdfb0f1 ffff88207fc43d40 ffffffff816360fc
2017-09-15T06:52:37.820789+00:00 ascliveedas4.sgdc kernel: [598346.499939]  ffff88207fc43d78 ffffffff8107b200 000000000000001c ffff881024660000
2017-09-15T06:52:37.820790+00:00 ascliveedas4.sgdc kernel: [598346.499942]  ffff881024654f40 0000000000000040 000000000000000a ffff88207fc43de0
2017-09-15T06:52:37.820791+00:00 ascliveedas4.sgdc kernel: [598346.499943] Call Trace:
2017-09-15T06:52:37.820792+00:00 ascliveedas4.sgdc kernel: [598346.499952]  &lt;IRQ&gt;  [&lt;ffffffff816360fc&gt;] dump_stack+0x19/0x1b
2017-09-15T06:52:37.820794+00:00 ascliveedas4.sgdc kernel: [598346.499956]  [&lt;ffffffff8107b200&gt;] warn_slowpath_common+0x70/0xb0
2017-09-15T06:52:37.820795+00:00 ascliveedas4.sgdc kernel: [598346.499959]  [&lt;ffffffff8107b29c&gt;] warn_slowpath_fmt+0x5c/0x80
2017-09-15T06:52:37.820795+00:00 ascliveedas4.sgdc kernel: [598346.499964]  [&lt;ffffffff8154d4f0&gt;] dev_watchdog+0x270/0x280
2017-09-15T06:52:37.820796+00:00 ascliveedas4.sgdc kernel: [598346.499966]  [&lt;ffffffff8154d280&gt;] ? dev_graft_qdisc+0x80/0x80
2017-09-15T06:52:37.820797+00:00 ascliveedas4.sgdc kernel: [598346.499972]  [&lt;ffffffff8108b0a6&gt;] call_timer_fn+0x36/0x110
2017-09-15T06:52:37.820798+00:00 ascliveedas4.sgdc kernel: [598346.499974]  [&lt;ffffffff8154d280&gt;] ? dev_graft_qdisc+0x80/0x80
2017-09-15T06:52:37.820799+00:00 ascliveedas4.sgdc kernel: [598346.499977]  [&lt;ffffffff8108dd97&gt;] run_timer_softirq+0x237/0x340
2017-09-15T06:52:37.820800+00:00 ascliveedas4.sgdc kernel: [598346.499980]  [&lt;ffffffff81084b0f&gt;] __do_softirq+0xef/0x280
2017-09-15T06:52:37.820801+00:00 ascliveedas4.sgdc kernel: [598346.499985]  [&lt;ffffffff81103360&gt;] ? cpu_stop_should_run+0x50/0x50
2017-09-15T06:52:37.820801+00:00 ascliveedas4.sgdc kernel: [598346.499988]  [&lt;ffffffff8164819c&gt;] call_softirq+0x1c/0x30
2017-09-15T06:52:37.820802+00:00 ascliveedas4.sgdc kernel: [598346.499994]  [&lt;ffffffff81016fc5&gt;] do_softirq+0x65/0xa0
2017-09-15T06:52:37.820803+00:00 ascliveedas4.sgdc kernel: [598346.499996]  [&lt;ffffffff81084ea5&gt;] irq_exit+0x115/0x120
2017-09-15T06:52:37.820804+00:00 ascliveedas4.sgdc kernel: [598346.499999]  [&lt;ffffffff81648e15&gt;] smp_apic_timer_interrupt+0x45/0x60
2017-09-15T06:52:37.820805+00:00 ascliveedas4.sgdc kernel: [598346.500003]  [&lt;ffffffff816474dd&gt;] apic_timer_interrupt+0x6d/0x80
2017-09-15T06:52:37.820813+00:00 ascliveedas4.sgdc kernel: [598346.500007]  &lt;EOI&gt;  [&lt;ffffffff811033df&gt;] ? multi_cpu_stop+0x7f/0xf0
2017-09-15T06:52:37.820815+00:00 ascliveedas4.sgdc kernel: [598346.500010]  [&lt;ffffffff81103666&gt;] cpu_stopper_thread+0x96/0x170
</code></pre><h2 id="鸡汤"><a href="#鸡汤" class="headerlink" title="鸡汤"></a>鸡汤</h2><p><a href="https://www.zhihu.com/question/39430220/answer/81648584" target="_blank" rel="external">Do More， Do Better， Do exercise</a>（<strong>口号和实践</strong>）</p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/dd8b77138555d5a23563f5691a60e2dd.png" alt="image.png"></p>
<h2 id="最后讲两个超级大Case：卡行天下、汇通天下"><a href="#最后讲两个超级大Case：卡行天下、汇通天下" class="headerlink" title="最后讲两个超级大Case：卡行天下、汇通天下"></a>最后讲两个超级大Case：卡行天下、<a href="https://aone.alibaba-inc.com/task/10409778" target="_blank" rel="external">汇通天下</a></h2>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;答疑问题汇总&quot;&gt;&lt;a href=&quot;#答疑问题汇总&quot; class=&quot;headerlink&quot; title=&quot;答疑问题汇总&quot;&gt;&lt;/a&gt;答疑问题汇总&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;天津一网通-vxlan-网络始终不通，宿主机能抓到发出去的包，但是抓不到回复包。对端容器
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2020/08/29/btrace/"/>
    <id>http://yoursite.com/2020/08/29/btrace/</id>
    <published>2020-08-29T09:39:11.171Z</published>
    <updated>2019-10-25T05:15:37.147Z</updated>
    
    <content type="html"><![CDATA[<h1 id="btrace-startup"><a href="#btrace-startup" class="headerlink" title="btrace startup"></a>btrace startup</h1><p>直接下载安装 <a href="https://github.com/jbachorik/btrace/releases/tag/v1.3.4" target="_blank" rel="external">github release</a> 即可。或者从oss下载：</p>
<pre><code>osscmd get oss://fzpackages/ark/btrace-bin-1.3.4.zip ./btrace-bin-1.3.4.zip
</code></pre><p>解压后即可直接运行(8817是目标进程pid)： </p>
<pre><code>./bin/btrace 8817 ./BTraceNew.java
</code></pre><p>BTraceNew.java 代码如下：</p>
<pre><code>import com.sun.btrace.annotations.*;
import static com.sun.btrace.BTraceUtils.*;
import com.sun.btrace.BTraceUtils;
import com.sun.btrace.BTraceUtils.Strings;

import java.net.*;
import java.lang.reflect.Field;

@BTrace
public class BTraceNew {

@OnMethod(clazz=&quot;java.lang.StackTraceElement&quot;, method=&quot;&lt;init&gt;&quot;)
public static void onCreateInit(@Self Object me) {
      println(&quot;\n==== init ====&quot;);
      BTraceUtils.println(BTraceUtils.timestamp() );
      BTraceUtils.println(BTraceUtils.Time.millis() );
      println(concat(&quot;Socket closing:&quot;, str(me)));
      println(concat(&quot;thread: &quot;, str(currentThread())));
      printFields(me);
      jstack();
}

//    @OnMethod(clazz = &quot;java.lang.Throwable&quot;, method=&quot;&lt;init&gt;&quot;)
//    @OnMethod(clazz=&quot;java.lang.StackTraceElement&quot;, method=&quot;/.*/&quot;)
public static void onCreateNew(@Self Object me) {
      println(&quot;\n==== call new ====&quot;);
      BTraceUtils.println(BTraceUtils.timestamp() );
      BTraceUtils.println(BTraceUtils.Time.millis() );
      println(concat(&quot;Socket closing:&quot;, str(me)));
      println(concat(&quot;thread: &quot;, str(currentThread())));
      printFields(me);
      jstack();
}

@OnMethod(clazz=&quot;+java.net.Socket&quot;, method=&quot;close&quot;)
public static void onSocketClose(@Self Object me) {
      println(&quot;\n==== java.net.Socket#close ====&quot;);
      BTraceUtils.println(BTraceUtils.timestamp() );
      BTraceUtils.println(BTraceUtils.Time.millis() );
      println(concat(&quot;Socket closing:&quot;, str(me)));
      println(concat(&quot;thread: &quot;, str(currentThread())));
      printFields(me);
      jstack();
}

}
</code></pre><h2 id="btrace代码自动生成工具"><a href="#btrace代码自动生成工具" class="headerlink" title="btrace代码自动生成工具"></a>btrace代码自动生成工具</h2><p><a href="https://btrace.org/btrace/" target="_blank" rel="external">https://btrace.org/btrace/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;btrace-startup&quot;&gt;&lt;a href=&quot;#btrace-startup&quot; class=&quot;headerlink&quot; title=&quot;btrace startup&quot;&gt;&lt;/a&gt;btrace startup&lt;/h1&gt;&lt;p&gt;直接下载安装 &lt;a href=&quot;https:
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2020/08/29/tcprt/"/>
    <id>http://yoursite.com/2020/08/29/tcprt/</id>
    <published>2020-08-29T09:39:11.171Z</published>
    <updated>2020-06-22T06:33:02.241Z</updated>
    
    <content type="html"><![CDATA[<p>from：<a href="http://baike.corp.taobao.com/index.php/CS_RD/CDN/AppAccelerate/tcprt" target="_blank" rel="external">http://baike.corp.taobao.com/index.php/CS_RD/CDN/AppAccelerate/tcprt</a></p>
<p>最新文档：<br><a href="http://gitlab.alibaba-inc.com/alicdn/netOpt-doc/tcprt/tcprt-wiki.md" target="_blank" rel="external">http://gitlab.alibaba-inc.com/alicdn/netOpt-doc/tcprt/tcprt-wiki.md</a> </p>
<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>1 TCPRT介绍<br>1.1 TCPRT的实现原理<br>1.2 日志信息介绍<br>2 部署和使用<br>3 Q&amp;A</p>
<h3 id="TCPRT介绍"><a href="#TCPRT介绍" class="headerlink" title="TCPRT介绍"></a>TCPRT介绍</h3><p>TCPRT是一个面向web的从tcp层的角度监控和分析网络服务质量和网络覆盖的超轻量级的分析工具； TCPRT在单机进出流量超过10G，单机4w qps的情况下， cpu消耗不超过1%； 经过不断的演化，TCPRT现在具有两个功能： 1.网络服务质量的实时监控；包括 用户的请求文件的平均响应时间（rt），平均服务器响应时间（servertime），平均时延rtt，平均丢包率 等； 2.详细的网络服务质量数据采集：包括用户访问的每一条请求的响应时间（rt），服务器响应时间（servertime），时延rtt，丢包率等；</p>
<p>其中 rt的含义是指从 服务器接收到请求到服务器发送给用户的数据包全部被确认所花费的时间；</p>
<blockquote>
<p>servertime的含义是 从服务器接收到请求到服务器发送第一个数据包的时间；   （如果请求的是小文件，这个时间==应用服务器的rt）</p>
</blockquote>
<p><strong>目前增加了一版新架构实现的TCPRT，在功能上增加了域名粒度的实时访问数据。即可以查看一个周期内访问某个特定域名的rt,servertime rtt 等数据，并且方便更新。</strong></p>
<h3 id="TCPRT的实现原理"><a href="#TCPRT的实现原理" class="headerlink" title="TCPRT的实现原理"></a>TCPRT的实现原理</h3><p>tcprt采集的是主要有两方面的数据，一方面是用户下载文件实际所需要的时间，在现在的情况下，对于web用户来说，每个请求都是串行的每个请求下载完以后，才会有下一个请求，所以，我们只需要记录请求到达服务器的时间和发送的所有数据被确认的时间，就能知道这个请求用户实际上花了多长时间去下载， 后续如果pipeline 普及的话，很抱歉，基于网络四层的统计方法都要失效；另一方面是网络覆盖的数据，主要包括丢包个数、发送的有效数据、rtt（数据包一来一回的时间）；</p>
<p>tcprt利用的是内核的拥塞控制模块来采集数据的，tcp协议栈处理过程中，在很多特殊事件会调用拥塞控制模块，拥塞控制模块的处理函数又可以拿到这个连接的所有信息（sock），这里面包含了丢包、rtt等信息，因此 tcprt不需要做任何的分析工作，只需要采集数据即可；</p>
<p>同时，触发 拥塞控制模块的事件中，又包括了几个我们需要的时间点，包括请求到达， 第一个数据包发送，连接关闭；所以我们只需要做简单的if判断就能够获取服务质量相关的数据；</p>
<p><strong>新架构TCPRT 是在tcp 协议栈处理过程的几个地方添加了钩子，通过在钩子中做 rt 等数据的采集即可，所以只有特定的内核才能支持。实现域名粒度的数据采集是通过对TCP数据包进行分析，获取其域名并在内核使用hash 链表进行数据的统计。</strong></p>
<h3 id="日志信息介绍"><a href="#日志信息介绍" class="headerlink" title="日志信息介绍"></a>日志信息介绍</h3><p>tcprt会输出两种类型的日志</p>
<ul>
<li>实时的监控数据， tcprt会把实时的监控日志打印到 /sys/kernel/debug/rt-network-real* 中，日志格式如下所示：</li>
</ul>
<p>1379387910 all 80 332 5 55 80 2 27340 101712</p>
<p>时间 所有域名 端口 rt servertime 丢包率 rtt 失败率 平均文件大小 这段时间内的访问次数；</p>
<ul>
<li>详细的用户访问日志 tcprt把用户详细日志打印到 /sys/kernel/debug/rt-network-logs,访问的结果分为三类， 访问成功，访问失败、没有数据输出，分别对应的日志 第一位为 r w n 如果用户访问成功 ，打出的日志格式如下：</li>
</ul>
<p>r 1379063895 171.214.168.205:12486 80 24185 637 71 0 1 175 1024</p>
<p>访问成功 时间 ip：port 服务器访问端口 文件大小 rt rtt 重传包个数 连接的第n个请求 首字节 mss</p>
<p>w 1379063952 111.227.56.204:52025 80 174240 13134 88 11 28 6 0 2880</p>
<p>访问失败 时间 ip:port 服务器访问端口 被确认的文件大小 总下载时间 rtt 初始窗口 丢包个数 连接的第n个请求 首字节时间 总发送的文件大小</p>
<p>n 1379063952 111.227.56.204:52025 80 1 2000</p>
<p>访问没有数据返回 时间 ip：port 服务器访问端口 连接的第n个请求 等待时间</p>
<h2 id="新架构TCPRT-的日志格式"><a href="#新架构TCPRT-的日志格式" class="headerlink" title="新架构TCPRT 的日志格式"></a>新架构TCPRT 的日志格式</h2><ol>
<li><p>实时的监控数据， tcprt会把实时的监控日志打印到 /sys/kernel/debug/tcp-watch-real<em> 中，下面的日志都是一个周期内（比如一分钟）的平均值<br>输出到/sys/kernel/debug/tcp-watch-real</em>文件中：<br>(1)基于端口的日志格式如下所示：<br>1379387910 all-new 80 332 5 55 80 2 27340 101712 0 0<br>时间 所有域名 端口 rt servertime 丢包率 rtt 失败率 平均文件大小 这段时间内的访问次数 https 建连时间 https 响应时间<br>(2)基于域名的日志格式如下所示：<br>1408327819 gdz03.md.alicdn.com 21 13 0 0 0 0 1 0 20182 32 1<br>时间 域名 RT 首字节时间 上传时间 https建连 https响应 丢包率 rtt 失败率 每个请求的平均文件大小 这段时间内请求个数 连接个数 平均建连时间 首播时间<br>输出到/sys/kernel/debug/tcp-watch-back<em>文件中：<br>(3) 回源到C段的 日志格式<br>1408329123 140.205.134 0 136 693 136<br>时间 IP 前三段 总重传个数 总包个数 总rtt时间 总rtt个数（总连接个数）<br>输出到/sys/kernel/debug/tcp-watch-front</em>文件中：<br>（4）从L2输出到L1的C段日志格式<br>1408329123 140.205.134 0 136 693 136<br>时间 IP 前三段 总重传个数 总包个数 总rtt时间 总rtt个数（总连接个数）</p>
</li>
<li><p>详细的用户访问日志 tcprt把用户详细日志打印到 /sys/kernel/debug/tcp-watch-log*,访问的结果分为五类， 访问成功、访问失败、没有数据输出、每连接数据统计、回源数据，分别对应的日志 第一位为 r_new w_new n_new c_new back如果用户访问成功 ，打出的日志格式如下：<br>(1)r_new 1408283588 img02.taobaocdn.com 115.238.23.16:34375 81 77480 145 1 0 8 57 0 112 1448<br>访问成功 时间 请求域名 ip：port 服务器访问端口 下载文件大小 rt rtt 重传包个数 连接的第n个请求 首字节时间 上传时间 上传文件大小 mss<br>更新：<br>r_new 1484190246 192.168.122.118 192.168.122.77:56966 80 1048854 14 1 0 1 3 0 115 1460 1048854 14 3 0 1<br>访问成功 时间 请求域名 ip：port 服务器访问端口 下载文件大小 rt rtt 重传包个数 连接的第n个请求 首字节时间 上传时间 上传文件大小 mss 连接建立到目前发送数据量 纯数据发送时间 首播时间 空等时间总和 tos<br>(2) w_new 1379063952 img02.taobaocdn.com 111.227.56.204:52025 80 1740 13134 88 11 2 6 0 112 2880<br>访问失败 时间 请求域名ip:port 服务器访问端口 被确认的文件大小 总下载时间 rtt 重传包个数 连接的第n个请求 首字节时间 上传时间 上传文件大小 已发送未确认文件大小<br>更新： w_new 1486624545 cdn.hc.org 115.238.23.194:22086 443 0 5 0 0 2 5 40 174 294 129 6 0 0 2021 1<br>访问失败 请求域名ip:port 服务器访问端口 被确认的文件大小 总下载时间 rtt 重传包个数 连接的第n个请求 首字节时间 上传时间 上传文件大小 已发送未确认文件大小 连接建立以来确认的字节数 纯数据发送时间 首播时间 空等时间 写缓存排队字节量 tos<br>$1 w_new 访问失败<br>$2 1486624545 时间<br>$3 cdn.hc.org 域名<br>$4 115.238.23.194:22086 ip:port<br>$5 443 服务器访问端口<br>$6 0 被确认文件大小<br>$7 5 总下载时间<br>$8 0 rtt<br>$9 0 重传包个数<br>$10 2 连接的第n个请求<br>$11 5 首字节时间<br>$12 40 上传时间<br>$13 174 上传文件大小<br>$14 294 已发送未确认文件大小<br>$15 129 连接建立以来确认的字节数<br>$16 6 纯数据发送时间<br>$17 0 首播时间<br>$18 0 空等时间<br>$19 2021 写缓存排队字节量<br>$20 1 tos<br>(3)n_new 1379063952 img02.taobaocdn.com 111.227.56.204:52025 80 1 4 200 2000<br>访问没有数据返回 时间 请求域名 ip:port 服务器访问端口 连接的第n个请求 上传时间 上传文件大小 等待时间<br>(4)c_new 1408327083 cdn.hc.org 115.238.23.16:40323 81 193 0 0 1 57 6 0 0 1448<br>连接数据 时间 访问域名 ip:port 服务器访问端口 下载文件大小 重传个数 rtt 请求个数 上传数据大小 建连时间 https建连时间 https响应时间 mss<br>更新：<br>c_new 1484190246 192.168.122.118 192.168.122.77:56966 80 1048854 0 1 1 116 0 0 0 1460 14 1<br>连接数据 时间 访问域名 ip:port 服务器访问端口 下载文件大小 重传个数 rtt 请求个数 上传数据大小 建连时间 https建连时间 https响应时间 mss 纯数据发送时间 tos<br>(5)back 1408327455 140.205.134.10:80 33484 290 0 5 1452<br>回源请求 时间 ip:port 请求端口 发送大小 重传个数 rtt mss</p>
</li>
<li>/proc/tcp_rt_file 中是基于域名的实时首字节展示<br>1408327819 total_statis 1061629323 81324204<br>统计开始时间 所有请求的统计 总首字节时间 总请求数<br>1408327819 cb.alimama.cn 2268 1463<br>统计开始时间 域名 总首字节时间 总请求数<br>部署和使用<br>安装： sudo yum install t-cdn-tcprt -b test</li>
</ol>
<p>开启： /etc/init.d/tcprt start</p>
<p>停止： /etc/init.d/tcprt stop</p>
<p>老版本使用方案：</p>
<p>启动后，请根据你所属的网络和应用配置如下参数：</p>
<p>1）机器是否在 fullnat后面： /sys/module/tcp_rt_base/parameters/nat 如果在fullnat 后面 建议设成1；</p>
<p>echo 1 &gt; /sys/module/tcp_rt_base/parameters/nat</p>
<p>如果不在fullnat 后面要设成0 否者的话取不到数据</p>
<p>echo 0 &gt; /sys/module/tcp_rt_base/parameters/nat</p>
<p>2）你需要监听的端口，tcprt现在支持 最多3个端口的监控， /sys/module/tcp_rt_statis/parameters/port_nummber 设置 需要监控的端口数 /sys/module/tcp_rt_statis/parameters/port0（1、2） 为需要监控的端口 比如，我需要监控 80 81 82 3个端口，那么可以这么设置</p>
<p>echo 80 &gt; /sys/module/tcp_rt_statis/parameters/port0</p>
<p>echo 81 &gt; /sys/module/tcp_rt_statis/parameters/port1</p>
<p>echo 82 &gt; /sys/module/tcp_rt_statis/parameters/port2</p>
<p>echo 3 &gt; /sys/module/tcp_rt_statis/parameters/port_nummber</p>
<p>3) 数据采集的间隔： /sys/module/tcp_rt_statis/parameters/check_interval 如果你希望 1分钟统计一次</p>
<p>echo 60 &gt; /sys/module/tcp_rt_statis/parameters/check_interval</p>
<p>建议不要低于1分钟</p>
<p>以上配置都需要sudo 权限；</p>
<p>tcprt提供了两个配置范例，分别用于cdn和源站 /usr/local/tcprt/cdn.sh</p>
<p>echo 0 &gt; /sys/module/tcp_rt_base/parameters/nat</p>
<p>echo 60 &gt; /sys/module/tcp_rt_statis/parameters/check_interval</p>
<p>/usr/local/tcprt/source.sh</p>
<p>echo 1 &gt; /sys/module/tcp_rt_base/parameters/nat</p>
<p>echo 300 &gt; /sys/module/tcp_rt_statis/parameters/check_interval</p>
<p>/usr/local/tcprt/tcp_rt_a_log_copy.sh &gt; /dev/null 2&gt;&amp;1 &amp; /usr/local/tcprt/tcp_rt_a_real_copy.sh &gt; /dev/null 2&gt;&amp;1 &amp;</p>
<p>tcprt 还提供了两个日志采集的范例脚本 分别在： /usr/local/tcprt/tcp_rt_a_log_copy.sh /usr/local/tcprt/tcp_rt_a_real_copy.sh 分别是 详细日志和实时监控日志的采集脚本； 如果单机的qps数很高， 强烈建议不要用/usr/local/tcprt/tcp_rt_a_log_copy.sh 采集实时日志； 日志采集的数据会把数据存到 /home/admin/logs/tcprt/ 目录下 实时监控日志 会存到/home/admin/logs/tcp_rt/tcp_rt.statis文件里；</p>
<p>新版本使用方案：<br>你需要监听的端口，tcprt现在支持 最多6个端口的监控，假如需要监听80 443 81 三个端口可以执行<br>echo 80,443,81 &gt;/sys/module/tcp_watch/parameters/port_arr<br>日志的采集可以直接使用<br>cat /sys/kernel/debug/tcp-watch-log<em> &gt; logfile ,<br>cat /sys/kernel/debug/tcp-watch-real</em> &gt; realfile<br>tcp-watch-log<em> tcp-watch-real</em>中的数据只能读取一次，读取后不存在了。</p>
<h3 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h3><ol>
<li><p>setsockopt(c-&gt;fd, IPPROTO_TCP, TCP_DOMAIN, peer-&gt;host.data, peer-&gt;host.len); 失败</p>
<p>1）检查uname -r，是不是2.6.32-220.23.2.ali878.tcp1.34.el6.x86_64或2.6.32-220.23.2.ali878.tcp1.54.el6.x86_64，如果不是，则不是常规部署的cdn tcpkernel。</p>
<pre><code>tcprt需适配这两个特定内核版本。其余版本的支持情况，需要人工check。         
</code></pre><p>2）setDOMAIN需要tcprt hotfix版本的支持。检查t-cdn-tcprt版本，是不是最新的，否则更新。  </p>
<pre><code>http://rpm.corp.taobao.com/find.php?q=t-cdn-tcprt，找test/最先版的。部署和使用: http://baike.corp.taobao.com/index.php/CS_RD/CDN/AppAccelerate/tcprt。    
</code></pre><p>3）检查配置端口。  </p>
</li>
<li><p>tcprt采集的rtt</p>
<p>tcprt采集以请求为单位，rtt是该次请求中，出现过的最小rtt值。 </p>
</li>
</ol>
<h2 id="TCP-RT-log"><a href="#TCP-RT-log" class="headerlink" title="TCP RT log"></a>TCP RT log</h2><pre><code>V5 R 1575428543 880110 172.18.34.6:48414 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 1471 1 0 4 1296 0 37 0 1460
V5 R 1575428543 880977 172.18.34.6:48422 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 98 621 1 0 0 451 0 0 0 1460
V5 R 1575428543 877650 172.18.34.6:48410 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 4180 1 0 3 4013 0 21 0 1460
V5 R 1575428543 881614 172.18.34.6:48422 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 993 1 0 1 813 0 99 0 1460
V5 R 1575428543 881832 172.18.34.6:48410 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 843 1 0 4 664 0 39 0 1460
V5 R 1575428543 881582 172.18.34.6:48414 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 1272 1 0 5 1097 0 21 0 1460
V5 R 1575428543 881576 172.18.34.6:48416 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 4434 1 0 4 4265 0 37 0 1460
V5 R 1575428543 882855 172.18.34.6:48414 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 3369 1 0 6 3175 0 39 0 1460
V5 R 1575428543 886012 172.18.34.6:48416 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 575 1 0 5 402 0 21 0 1460
V5 R 1575428543 879994 172.18.34.6:48418 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 1506 6698 1 0 2 6523 0 919 0 1460
V5 R 1575428543 886731 172.18.34.6:48418 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 1140 1 0 3 955 0 19 0 1460
V5 R 1575428543 886436 172.18.34.6:48424 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 98 1573 0 0 0 1410 0 0 0 1460
V5 R 1575428543 887872 172.18.34.6:48418 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 401 1 0 4 230 0 37 0 1460
V5 R 1575428543 886588 172.18.34.6:48416 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 1918 1 0 6 1734 0 39 0 1460
V5 R 1575428543 888024 172.18.34.6:48424 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 1007 0 0 1 816 0 99 0 1460
V5 R 1575428543 882609 172.18.34.6:48422 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 1506 6388 1 0 2 6199 0 919 0 1460
V5 R 1575428543 889038 172.18.34.6:48422 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 536 1 0 3 361 0 19 0 1460
V5 R 1575428543 889575 172.18.34.6:48422 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 291 1 0 4 119 0 37 0 1460
V5 R 1575428543 889867 172.18.34.6:48422 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 325 1 0 5 156 0 21 0 1460
V5 R 1575428543 888275 172.18.34.6:48418 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 1953 1 0 5 1778 0 21 0 1460
V5 R 1575428543 889986 172.18.34.6:48428 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 98 311 0 0 0 145 0 0 0 1460
V5 R 1575428543 890193 172.18.34.6:48422 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 593 1 0 6 399 0 39 0 1460
V5 R 1575428543 890230 172.18.34.6:48418 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 2046 1 0 6 1855 0 39 0 1460
V5 R 1575428543 889032 172.18.34.6:48424 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 1506 5406 0 0 2 5251 0 919 0 1460
</code></pre><p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/c6389d75e8469fc75e5005d0311c9524.png" alt="image.png"></p>
<p>格式<a href="https://www.atatech.org/articles/68880" target="_blank" rel="external">说明</a></p>
<p>DebugFS，顾名思义，是一种用于内核调试的虚拟文件系统，内核开发者通过debugfs和用户空间交换数据。类似的虚拟文件系统还有procfs和sysfs等，这几种虚拟文件系统都并不实际存储在硬盘上，而是Linux内核运行起来后才建立起来。</p>
<p>通常情况下，最常用的内核调试手段是printk。但printk并不是所有情况都好用，比如打印的数据可能过多，我们真正关心的数据在大量的输出里不是那么一目了然；或者我们在调试时可能需要修改某些内核变量，这种情况下printk就无能为力，而如果为了修改某个值重新编译内核或者驱动又过于低效，此时就需要一个临时的文件系统可以把我们需要关心的数据映射到用户空间。在过去，procfs可以实现这个目的，到了2.6时代，新引入的sysfs也同样可以实现，但不论是procfs或是sysfs，用它们来实现某些debug的需求，似乎偏离了它们创建的本意。比如procfs，其目的是反映进程的状态信息；而sysfs主要用于Linux设备模型。不论是procfs或是sysfs的接口应该保持相对稳定，因为用户态程序很可能会依赖它们。当然，如果我们只是临时借用procfs或者sysfs来作debug之用，在代码发布之前将相关调试代码删除也无不可。但如果相关的调试借口要在相当长的一段时间内存在于内核之中，就不太适合放在procfs和sysfs里了。故此，debugfs应运而生。</p>
<p>[toc]<br>原版链接: <a href="http://baike.corp.taobao.com/index.php/CS_RD/CDN/AppAccelerate/tcprt" target="_blank" rel="external">http://baike.corp.taobao.com/index.php/CS_RD/CDN/AppAccelerate/tcprt</a>  </p>
<h1 id="TCPRT介绍-1"><a href="#TCPRT介绍-1" class="headerlink" title="TCPRT介绍"></a>TCPRT介绍</h1><p>TCPRT是一个面向web的从tcp层的角度监控和分析网络服务质量和网络覆盖的超轻量级的分析工具；<br>TCPRT在单机进出流量超过10G，单机4w qps的情况下， cpu消耗不超过1%；<br>经过不断的演化，TCPRT现在具有两个功能：<br>1.网络服务质量的实时监控；包括 用户的请求文件的平均响应时间（rt），平均服务器响应时间（servertime），平均时延rtt，平均丢包率 等；<br>2.详细的网络服务质量数据采集：包括用户访问的每一条请求的响应时间（rt），服务器响应时间（servertime），时延rtt，丢包率等；  </p>
<p>其中 rt的含义是指从 服务器接收到请求到服务器发送给用户的数据包全部被确认所花费的时间；<br>servertime的含义是 从服务器接收到请求到服务器发送第一个数据包的时间；   （如果请求的是小文件，这个时间==应用服务器的rt）    </p>
<p><strong>目前增加了一版新架构实现的TCPRT，在功能上增加了域名粒度的实时访问数据。即可以查看一个周期内访问某个特定域名的rt,servertime rtt 等数据，并且方便更新。</strong></p>
<h2 id="1-TCPRT的实现原理"><a href="#1-TCPRT的实现原理" class="headerlink" title="1. TCPRT的实现原理"></a>1. TCPRT的实现原理</h2><p>tcprt采集的是主要有两方面的数据  </p>
<p>一方面是用户下载文件实际所需要的时间，<br>在现在的情况下，对于web用户来说，每个请求都是串行的每个请求下载完以后，<br>才会有下一个请求，所以，我们只需要记录请求到达服务器的时间和发送的所有数据被确认的时间，<br>就能知道这个请求用户实际上花了多长时间去下载，<br>后续如果pipeline 普及的话，很抱歉，基于网络四层的统计方法都要失效；    </p>
<p>另一方面是网络覆盖的数据，主要包括丢包个数、发送的有效数据、rtt（数据包一来一回的时间）；<br>tcprt利用的是内核的拥塞控制模块来采集数据的，tcp协议栈处理过程中，<br>在很多特殊事件会调用拥塞控制模块，拥塞控制模块的处理函数又可以拿到这个连接的所有信息（sock），<br>这里面包含了丢包、rtt等信息，因此 tcprt不需要做任何的分析工作，只需要采集数据即可；<br>同时，触发 拥塞控制模块的事件中，又包括了几个我们需要的时间点，包括请求到达，<br>第一个数据包发送，连接关闭；所以我们只需要做简单的if判断就能够获取服务质量相关的数据；    </p>
<p><strong>新架构TCPRT 是在tcp 协议栈处理过程的几个地方添加了钩子，通过在钩子中做 rt 等数据的采集即可，<br>所以只有特定的内核才能支持。实现域名粒度的数据采集是通过对TCP数据包进行分析，获取其域名并在内核使用hash 链表进行数据的统计。</strong>  </p>
<h2 id="2-日志信息介绍"><a href="#2-日志信息介绍" class="headerlink" title="2. 日志信息介绍"></a>2. 日志信息介绍</h2><h3 id="2-1-原始版本"><a href="#2-1-原始版本" class="headerlink" title="2.1 原始版本"></a>2.1 原始版本</h3><p>tcprt会输出两种类型的日志  </p>
<ul>
<li><p>2.1.1 实时的监控数据<br>tcprt会把实时的监控日志打印到 /sys/kernel/debug/rt-network-real* 中，日志格式如下所示：  </p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1379387910 all 80 332 5 55 80 2 27340 101712  </div><div class="line">时间 所有域名 端口 rt servertime 丢包率 rtt 失败率 平均文件大小 这段时间内的访问次数；</div></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>该日志，tsar还在用。</strong>    </p>
<p>~~2.详细的用户访问日志 tcprt把用户详细日志打印到 /sys/kernel/debug/rt-network-logs,    ~~  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">访问的结果分为三类， 访问成功，访问失败、没有数据输出，分别对应的日志 第一位为 r w n 如果用户访问成功 ，打出的日志格式如下：  </div><div class="line">r 1379063895 171.214.168.205:12486 80 24185 637 71 0 1 175 1024</div><div class="line">访问成功 时间 ip：port 服务器访问端口 文件大小 rt rtt 重传包个数 连接的第n个请求 首字节 mss</div><div class="line">w 1379063952 111.227.56.204:52025 80 174240 13134 88 11 28 6 0 2880</div><div class="line">访问失败 时间 ip:port 服务器访问端口 被确认的文件大小 总下载时间 rtt 初始窗口 丢包个数 连接的第n个请求 首字节时间 总发送的文件大小</div><div class="line">n 1379063952 111.227.56.204:52025 80 1 2000</div><div class="line">访问没有数据返回 时间 ip：port 服务器访问端口 连接的第n个请求 等待时间</div></pre></td></tr></table></figure>
<h3 id="2-2-新架构TCPRT的日志格式"><a href="#2-2-新架构TCPRT的日志格式" class="headerlink" title="2.2 新架构TCPRT的日志格式"></a>2.2 新架构TCPRT的日志格式</h3><h4 id="2-2-1-实时的监控数据"><a href="#2-2-1-实时的监控数据" class="headerlink" title="2.2.1 实时的监控数据"></a>2.2.1 实时的监控数据</h4><p>tcprt会把实时的监控日志打印到 /sys/kernel/debug/tcp-watch-real* 中，下面的日志都是一个周期内（比如一分钟）的平均值  </p>
<h5 id="2-2-1-1-输出到-sys-kernel-debug-tcp-watch-real-文件中"><a href="#2-2-1-1-输出到-sys-kernel-debug-tcp-watch-real-文件中" class="headerlink" title="2.2.1.1 输出到/sys/kernel/debug/tcp-watch-real*文件中"></a>2.2.1.1 输出到/sys/kernel/debug/tcp-watch-real*文件中</h5><p><strong>这份文件，tengine读，采集给heka，到天眼业务状态的外网丢包率和内网丢包率页面。</strong>  </p>
<ul>
<li>2.2.1.1.1 基于端口的日志格式如下所示  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1379387910 all-new 80 332 5 55 80 2 27340 101712 0 0 23 120</div><div class="line">时间 所有域名 端口 rt servertime 丢包率 rtt 失败率 平均文件大小 这段时间内的访问次数 https 建连时间 https 响应时间 srtt(毫秒) rtt方差(毫秒)</div></pre></td></tr></table></figure>
</code></pre><ul>
<li>2.2.1.1.2 基于域名的日志格式如下所示  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1408327819 gdz03.md.alicdn.com 21 13 0 0 0 0 1 0 20182 32 1 23 120</div><div class="line">时间 域名 RT 首字节时间 上传时间 https建连 https响应 丢包率 rtt 失败率 每个请求的平均文件大小 这段时间内请求个数 连接个数 平均建连时间 首播时间 srtt(毫秒) rtt方差(毫秒)</div></pre></td></tr></table></figure>
</code></pre><h5 id="2-2-1-2-输出到-sys-kernel-debug-tcp-watch-back-文件中"><a href="#2-2-1-2-输出到-sys-kernel-debug-tcp-watch-back-文件中" class="headerlink" title="2.2.1.2 输出到/sys/kernel/debug/tcp-watch-back*文件中"></a>2.2.1.2 输出到/sys/kernel/debug/tcp-watch-back*文件中</h5><ul>
<li>2.2.1.2.1 回源到C段的 日志格式  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1408329123 140.205.134 0 136 693 136 </div><div class="line">时间 IP 前三段 总重传个数 总包个数 总rtt时间 总rtt个数（总连接个数）</div></pre></td></tr></table></figure>
</code></pre><h5 id="2-2-1-3-输出到-sys-kernel-debug-tcp-watch-front-文件中"><a href="#2-2-1-3-输出到-sys-kernel-debug-tcp-watch-front-文件中" class="headerlink" title="2.2.1.3 输出到/sys/kernel/debug/tcp-watch-front*文件中"></a>2.2.1.3 输出到/sys/kernel/debug/tcp-watch-front*文件中</h5><ul>
<li>2.2.1.3.1 从L2输出到L1的C段日志格式  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1408329123 140.205.134 0 136 693 136</div><div class="line">时间 IP 前三段 总重传个数 总包个数 总rtt时间 总rtt个数（总连接个数）</div></pre></td></tr></table></figure>
</code></pre><h4 id="2-2-2-详细的用户访问日志"><a href="#2-2-2-详细的用户访问日志" class="headerlink" title="2.2.2 详细的用户访问日志"></a>2.2.2 详细的用户访问日志</h4><p>tcprt把用户详细日志打印到 /sys/kernel/debug/tcp-watch-log*,访问的结果分为五类，<br>访问成功、访问失败、没有数据输出、每连接数据统计、回源数据，<br>分别对应的日志 第一位为 r_new w_new n_new c_new back如果用户访问成功 ，打出的日志格式如下：  </p>
<ul>
<li>2.2.2.1 r_new 访问成功  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">r_new 1408283588 img02.taobaocdn.com 115.238.23.16:34375 81 77480 145 1 0 8 57 0 112 1448</div><div class="line">访问成功 时间 请求域名 ip：port 服务器访问端口 下载文件大小 rt rtt 重传包个数 连接的第n个请求 首字节时间 上传时间 上传文件大小 mss</div><div class="line">更新：</div><div class="line">r_new 1484190246 192.168.122.118 192.168.122.77:56966 80 1048854 14 1 0 1 3 0 115 1460 1048854 14 3 0 1 </div><div class="line">访问成功 时间 请求域名 ip：port 服务器访问端口 下载文件大小 rt rtt 重传包个数 连接的第n个请求 首字节时间 上传时间 上传文件大小 mss 连接建立到目前发送数据量 纯数据发送时间 首播时间 空等时间总和 tos</div></pre></td></tr></table></figure>
</code></pre><ul>
<li>2.2.2.2 w_new 访问失败   </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">w_new 1379063952 img02.taobaocdn.com 111.227.56.204:52025 80 1740 13134 88 11 2 6 0 112 2880</div><div class="line">访问失败 时间 请求域名ip:port 服务器访问端口 被确认的文件大小 总下载时间 rtt 重传包个数 连接的第n个请求 首字节时间 上传时间 上传文件大小 已发送未确认文件大小</div><div class="line">更新： </div><div class="line">w_new 1486624545 cdn.hc.org 115.238.23.194:22086 443 0 5 0 0 2 5 40 174 294 129 6 0 0 2021 1 </div><div class="line">访问失败 请求域名ip:port 服务器访问端口 被确认的文件大小 总下载时间 rtt 重传包个数 连接的第n个请求 首字节时间 上传时间 上传文件大小 已发送未确认文件大小 连接建立以来确认的字节数 纯数据发送时间 首播时间 空等时间 写缓存排队字节量 tos </div><div class="line">$1 w_new 访问失败 </div><div class="line">$2 1486624545 时间 </div><div class="line">$3 cdn.hc.org 域名 </div><div class="line">$4 115.238.23.194:22086 ip:port </div><div class="line">$5 443 服务器访问端口 </div><div class="line">$6 0 被确认文件大小 </div><div class="line">$7 5 总下载时间 </div><div class="line">$8 0 rtt </div><div class="line">$9 0 重传包个数 </div><div class="line">$10 2 连接的第n个请求 </div><div class="line">$11 5 首字节时间 </div><div class="line">$12 40 上传时间 </div><div class="line">$13 174 上传文件大小 </div><div class="line">$14 294 已发送未确认文件大小 </div><div class="line">$15 129 连接建立以来确认的字节数 </div><div class="line">$16 6 纯数据发送时间 </div><div class="line">$17 0 首播时间 </div><div class="line">$18 0 空等时间 </div><div class="line">$19 2021 写缓存排队字节量 </div><div class="line">$20 1 tos</div></pre></td></tr></table></figure>
</code></pre><ul>
<li>2.2.2.3 n_new 访问没有数据返回  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">n_new 1379063952 img02.taobaocdn.com 111.227.56.204:52025 80 1 4 200 2000</div><div class="line">访问没有数据返回 时间 请求域名 ip:port 服务器访问端口 连接的第n个请求 上传时间 上传文件大小 等待时间</div></pre></td></tr></table></figure>
</code></pre><ul>
<li>2.2.2.4 c_new 建连数据  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">c_new 1408327083 cdn.hc.org 115.238.23.16:40323 81 193 0 0 1 57 6 0 0 1448</div><div class="line">连接数据 时间 访问域名 ip:port 服务器访问端口 下载文件大小 重传个数 rtt 请求个数 上传数据大小 建连时间 https建连时间 https响应时间 mss</div><div class="line">更新：</div><div class="line">c_new 1484190246 192.168.122.118 192.168.122.77:56966 80 1048854 0 1 1 116 0 0 0 1460 14 1 </div><div class="line">连接数据 时间 访问域名 ip:port 服务器访问端口 下载文件大小 重传个数 rtt 请求个数 上传数据大小 建连时间 https建连时间 https响应时间 mss 纯数据发送时间 tos</div></pre></td></tr></table></figure>
</code></pre><ul>
<li>2.2.2.5 back 回源请求   </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">back 1408327455 140.205.134.10:80 33484 290 0 5 1452</div><div class="line">回源请求 时间 ip:port 请求端口 发送大小 重传个数 rtt mss</div></pre></td></tr></table></figure>
</code></pre><h4 id="2-2-3-proc-tcp-rt-file中是基于域名的实时首字节展示"><a href="#2-2-3-proc-tcp-rt-file中是基于域名的实时首字节展示" class="headerlink" title="2.2.3 /proc/tcp_rt_file中是基于域名的实时首字节展示"></a>2.2.3 /proc/tcp_rt_file中是基于域名的实时首字节展示</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">1408327819 total_statis 1061629323 81324204</div><div class="line">统计开始时间 所有请求的统计 总首字节时间 总请求数 </div><div class="line">1408327819 cb.alimama.cn 2268 1463</div><div class="line">统计开始时间 域名 总首字节时间 总请求数</div></pre></td></tr></table></figure>
<h3 id="2-3-支持主动查询的日志格式"><a href="#2-3-支持主动查询的日志格式" class="headerlink" title="2.3 支持主动查询的日志格式"></a>2.3 支持主动查询的日志格式</h3><h4 id="2-3-1-支持方式"><a href="#2-3-1-支持方式" class="headerlink" title="2.3.1 支持方式"></a>2.3.1 支持方式</h4><p>提供getsockopt接口，参数level＝SOL_TCP, optionname＝TCP_OPT_TCPRT,数据定义如下：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line">#ifndef SOL_TCP</div><div class="line">#define SOL_TCP 6</div><div class="line">#endif</div><div class="line"></div><div class="line">#define TCP_OPT_TCPRT 21</div><div class="line"></div><div class="line">struct tcprt_info &#123;</div><div class="line">    __u32	tcpi_srtt;                    /* 当前协议栈计算出的平滑rtt */</div><div class="line">	__u32	tcpi_send_bytes;              /* 发送数据大小(byte), 区间值，每调用一次，都表示和上次调用间隔这段区间的发送数据大小（不包含重传数据）*/</div><div class="line">	__u32	tcpi_retrans_segs;            /* 重传包数，区间值，每调用一次，都表示和上次调用间隔这段区间的重传包数 */</div><div class="line">	__u32	tcpi_snd_mss;                 /* 发送端的tcp mss */</div><div class="line">	__u32	tcpi_wmem_queued;             /* 发送队列总大小(byte)，包含发送队列中skb负荷大小，以及sk_buff、sk_shared_info结构体、协议头的额外开销 */</div><div class="line">	__u32	tcpi_rcv_nxt;                 /* 本端希望接收的下一个字节序 */</div><div class="line">	__u32	tcpi_selective_rcvseq0;       /* tp-&gt;selective_acks[0].end_seq */</div><div class="line">	__u32	tcpi_min_cwnd;                /* 调用区间内的最小拥塞窗口 */</div><div class="line">	__u32	tcpi_min_wnd;                 /* 调用区间内的最小对端接收窗口	*/</div><div class="line">	__u32	tcpi_max_inflight;            /* 调用区间内的最大发送但未确认包数 */</div><div class="line">	</div><div class="line">	__u8	tcpi_max_backoff;             /* 调用区间内发生的 RTO 最大退避次数 */</div><div class="line">	__u8	tcpi_backoff;                 /* 最近一次发生的 RTO 退避次数*/</div><div class="line">	__u8	tcpi_state;                   /* 当前tcp 栈的状态机 */</div><div class="line">	__u8	tcpi_ca_state;                /* 当前tcp 栈的拥塞状态机 */</div><div class="line">	</div><div class="line">	__u8	tcpi_probes;                  /* 当前正在发生零窗口探测的次数 */</div><div class="line">	__u8	tcpi_retransmits;             /* 当前重传定时器正在触发的次数 */</div><div class="line">	__u8	tcpi_options;                 /* tcp 会话协商的tcp 选项 */</div><div class="line">	__u8	tcpi_snd_wscale : 4, tcpi_rcv_wscale : 4;  /* 发送端，接收端通告的 ws 大小 */</div><div class="line"></div><div class="line">    __u16	tcpi_pad;</div><div class="line">	__u16	tcpi_rinit_maxmss;            /* tcp 会话协商的对端初始最大 mss */</div><div class="line">	__u32	tcpi_rinit_maxrwnd;           /* tcp 会话协商的对端初始最大接收窗口*/</div><div class="line"></div><div class="line">	__u32	tcpi_cwnd;                   /* 当前协议栈的拥塞窗口 */</div><div class="line">	__u32	tcpi_snd_wnd;                /* 当前协议栈的对端接收窗口 */</div><div class="line">	__u32	tcpi_inflight;               /* 当前发送未确认包的数目 */</div><div class="line">	__u32	tcpi_srttvar;                /* 最近采样的 rtt 平均偏差， 用来衡量 rtt 的抖动*/</div><div class="line">	__u32	tcpi_max_srttvar;            /* 采样周期内的最大 mdev */</div><div class="line">	__u32	tcpi_minrtt;                 /* 全局最小采样rtt */</div><div class="line"></div><div class="line">	__u32	tcpi_minsrtt;                /* 调用区间内的最小平滑rtt */</div><div class="line">	__u32	tcpi_min_rto;                /* 调用区间内的最小 rto */</div><div class="line">	__u32	tcpi_max_rto;                /* 调用区间内的最大 rto */</div><div class="line"></div><div class="line">	__u32	tcpi_max_wnd;	             /* 调用区间内的最大期望接收窗口	*/</div><div class="line">	__u32	tcpi_min_rewnd;              /* 调用区间内的最小剩余对端可用接收窗口 */</div><div class="line">	__u32	tcpi_max_rewnd;              /* 调用区间内的最大剩余对端可用接收窗口 */</div><div class="line">	__u32	tcpi_min_recwnd;             /* 调用区间内的最小剩余发送窗口 */</div><div class="line">	__u32	tcpi_max_recwnd;             /* 调用区间内的最大剩余发送窗口 */</div><div class="line">	__u32	tcpi_min_sendq_bytes;        /* 调用区间内发送队列排队的数据包大小(bytes) */</div><div class="line">	</div><div class="line">	__u32	tcpi_sample_intv;	         /* 采样间隔 */</div><div class="line">	__u32	tcpi_delivery_rate;          /* 调用区间内的发送速率 */</div><div class="line">	__u32	tcpi_ack_bytes;              /* 调用区间内对端累积确认的字节数 */</div><div class="line"></div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<h4 id="2-3-2-使用示例"><a href="#2-3-2-使用示例" class="headerlink" title="2.3.2 使用示例"></a>2.3.2 使用示例</h4><p>每次调用返回前次调用到现在的统计数据，使用示例：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">int fd;</div><div class="line">struct tcprt_info info;</div><div class="line">unsigned optlen = sizeof(info);</div><div class="line">int ret;</div><div class="line">fd = socket(PF_INET, SOCK_STREAM, 0);</div><div class="line">ret = getsockopt(fd, SOL_TCP, TCP_OPT_TCPRT, &amp;info, &amp;optlen);</div><div class="line">if(ret &lt; 0)&#123;</div><div class="line">    fprintf(stderr, &quot;getsockopt return %d\n&quot;, ret);</div><div class="line">    return ret;</div><div class="line">&#125;</div><div class="line"></div><div class="line">注意：</div><div class="line">1. 每调用一次getsockopt，该sk相关的tcprt测量值，都会重新赋值一次 ！这个接口是用来获取区间值的！  </div><div class="line">2. 线上端口配置，默认配置 1935 1936 1937 80端口，目前只允许出现在live_port_arr配置项，否则会影响采集数据的其它逻辑 ！</div><div class="line">3. live_port_arr 最大配置端口数 10个，配置文件/etc/tcp_watch.conf;如果需要配置增加端口，可联系网络团队支持</div></pre></td></tr></table></figure>
<p>在tengine中，可以使用上述方式来获取网络监控数据   </p>
<h2 id="3-部署和使用"><a href="#3-部署和使用" class="headerlink" title="3. 部署和使用"></a>3. 部署和使用</h2><p>安装： sudo yum install t-cdn-tcprt -b test<br>开启： /etc/init.d/tcprt start<br>停止： /etc/init.d/tcprt stop  </p>
<h3 id="3-1-老版本使用方案"><a href="#3-1-老版本使用方案" class="headerlink" title="3.1 老版本使用方案"></a>3.1 老版本使用方案</h3><p>启动后，请根据你所属的网络和应用配置如下参数：</p>
<ul>
<li><p>3.1.1 机器是否在 fullnat后面：<br>/sys/module/tcp_rt_base/parameters/nat 如果在fullnat 后面 建议设成1；<br>echo 1 &gt; /sys/module/tcp_rt_base/parameters/nat<br>如果不在fullnat 后面要设成0 否者的话取不到数据<br>echo 0 &gt; /sys/module/tcp_rt_base/parameters/nat  </p>
</li>
<li><p>3.1.2 你需要监听的端口，tcprt现在支持 最多3个端口的监控， </p>
</li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">/sys/module/tcp_rt_statis/parameters/port_nummber 设置 需要监控的端口数 </div><div class="line">/sys/module/tcp_rt_statis/parameters/port0（1、2） 为需要监控的端口</div></pre></td></tr></table></figure>
</code></pre><p>比如，我需要监控 80 81 82 3个端口，那么可以这么设置  </p>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">echo 80 &gt; /sys/module/tcp_rt_statis/parameters/port0</div><div class="line">echo 81 &gt; /sys/module/tcp_rt_statis/parameters/port1</div><div class="line">echo 82 &gt; /sys/module/tcp_rt_statis/parameters/port2</div><div class="line">echo 3 &gt; /sys/module/tcp_rt_statis/parameters/port_nummber</div></pre></td></tr></table></figure>
</code></pre><ul>
<li>3.1.3 数据采集的间隔：   </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">/sys/module/tcp_rt_statis/parameters/check_interval 如果你希望 1分钟统计一次</div><div class="line">echo 60 &gt; /sys/module/tcp_rt_statis/parameters/check_interval</div></pre></td></tr></table></figure>


建议不要低于1分钟    
</code></pre><p><strong>以上配置都需要sudo 权限；</strong>  </p>
<ul>
<li>3.1.4 tcprt提供了两个配置范例<br>分别用于cdn和源站   </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">/usr/local/tcprt/cdn.sh</div><div class="line">echo 0 &gt; /sys/module/tcp_rt_base/parameters/nat</div><div class="line">echo 60 &gt; /sys/module/tcp_rt_statis/parameters/check_interval</div></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">/usr/local/tcprt/source.sh</div><div class="line">echo 1 &gt; /sys/module/tcp_rt_base/parameters/nat</div><div class="line">echo 300 &gt; /sys/module/tcp_rt_statis/parameters/check_interval</div><div class="line">/usr/local/tcprt/tcp_rt_a_log_copy.sh &gt; /dev/null 2&gt;&amp;1 &amp;   </div><div class="line">/usr/local/tcprt/tcp_rt_a_real_copy.sh &gt; /dev/null 2&gt;&amp;1 &amp;</div></pre></td></tr></table></figure>
</code></pre><p>tcprt 还提供了两个日志采集的范例脚本 分别在：<br>/usr/local/tcprt/tcp_rt_a_log_copy.sh /usr/local/tcprt/tcp_rt_a_real_copy.sh<br>分别是 详细日志和实时监控日志的采集脚本；<br>如果单机的qps数很高， 强烈建议不要用/usr/local/tcprt/tcp_rt_a_log_copy.sh 采集实时日志；<br>日志采集的数据会把数据存到 /home/admin/logs/tcprt/ 目录下 实时监控日志 会存到/home/admin/logs/tcp_rt/tcp_rt.statis文件里；</p>
<h3 id="3-2-新版本使用方案"><a href="#3-2-新版本使用方案" class="headerlink" title="3.2 新版本使用方案"></a>3.2 新版本使用方案</h3><p>你需要监听的端口，tcprt现在支持 最多6个端口的监控，假如需要监听80 443 81 三个端口可以执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo 80,443,81 &gt;/sys/module/tcp_watch/parameters/port_arr</div></pre></td></tr></table></figure>
<p>日志的采集可以直接使用  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cat /sys/kernel/debug/tcp-watch-log* &gt; logfile , </div><div class="line">cat /sys/kernel/debug/tcp-watch-real* &gt; realfile</div></pre></td></tr></table></figure>
<p>tcp-watch-log* tcp-watch-real*中的数据只能读取一次，读取后不存在了。  </p>
<h1 id="4-Q-amp-A"><a href="#4-Q-amp-A" class="headerlink" title="4. Q&amp;A"></a>4. Q&amp;A</h1><ul>
<li>4.1 setsockopt(c-&gt;fd, IPPROTO_TCP, TCP_DOMAIN, peer-&gt;host.data, peer-&gt;host.len); 失败  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">1）检查uname -r，是不是2.6.32-220.23.2.ali878.tcp1.34.el6.x86_64或2.6.32-220.23.2.ali878.tcp1.54.el6.x86_64，如果不是，则不是常规部署的cdn tcpkernel。</div><div class="line">      tcprt需适配这两个特定内核版本。其余版本的支持情况，需要人工check。         </div><div class="line">2）setDOMAIN需要tcprt hotfix版本的支持。检查t-cdn-tcprt版本，是不是最新的，否则更新。  </div><div class="line">      http://rpm.corp.taobao.com/find.php?q=t-cdn-tcprt，找test/最先版的。  </div><div class="line">      部署和使用: http://baike.corp.taobao.com/index.php/CS_RD/CDN/AppAccelerate/tcprt。    </div><div class="line">3）检查配置端口。</div></pre></td></tr></table></figure>
</code></pre><ul>
<li>4.2 tcprt采集的rtt  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tcprt采集以请求为单位，rtt是该次请求中，出现过的最小rtt值。</div></pre></td></tr></table></figure>
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;from：&lt;a href=&quot;http://baike.corp.taobao.com/index.php/CS_RD/CDN/AppAccelerate/tcprt&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://baike.corp.taob
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2020/08/29/%E5%82%85%E7%9B%9B%E8%AE%A4%E7%9F%A5%E4%B8%89%E9%83%A8%E6%9B%B2%E5%AE%8C%E6%95%B4%E7%89%88%EF%BC%9A%E4%BA%BA%E4%B8%8E%E4%BA%BA%E6%9C%80%E5%A4%A7%E7%9A%84%E5%B7%AE%E5%88%AB%E6%98%AF%E8%AE%A4%E7%9F%A5/"/>
    <id>http://yoursite.com/2020/08/29/傅盛认知三部曲完整版：人与人最大的差别是认知/</id>
    <published>2020-08-29T09:39:11.171Z</published>
    <updated>2019-10-25T05:15:39.223Z</updated>
    
    <content type="html"><![CDATA[<p>在今天这样一个急剧变化的时代，每一个行业的认知都在迅速更新迭代，跨界也越来越普遍。</p>
<p>谁能正确建立和升级认知，谁就能脱颖而出。猎豹 CEO 傅盛他将认知梳理为三部曲：成长就是认知升级；管理本质就是认知管理；战略就是格局+破局。</p>
<p>认知升级：坚信大趋势，对外求教，活在当下、面向未来。认知管理：学会逆向思考，分清优先级。战略升级：建立行业格局，养成格局和破局结合的思维习惯。</p>
<p>我们将三部曲整合，希望能够成体系地输出有价值的内容，愿大家能够从中有所启发。</p>
<p>文 | 傅盛  授权发布</p>
<h2 id="一、所谓成长就是认知升级"><a href="#一、所谓成长就是认知升级" class="headerlink" title="一、所谓成长就是认知升级"></a>一、所谓成长就是认知升级</h2><p>我一直在思索，怎么才能让一家公司更快地成长？一个人怎么才能从一群人的竞争当中脱颖而出？</p>
<h3 id="1、人的四种认知状态"><a href="#1、人的四种认知状态" class="headerlink" title="1、人的四种认知状态"></a>1、人的四种认知状态</h3><p>最近我看了一幅图，我在其上加了一个数字注脚。</p>
<p><img src="https://private-alipayobjects.alipay.com/alipay-rmsdeploy-image/skylark/png/109/71fdf2e5260360e3.png" alt="71fdf2e5260360e3.png"> </p>
<p>这是一个人认知的四种状态——“不知道自己不知道”，“知道自己不知道”，“知道自己知道”和“不知道自己知道”，也是人的四种境界。我将其简单翻译为：</p>
<p>不知道自己不知道——以为自己什么都知道，自以为是的认知状态。<br>知道自己不知道——有敬畏之心，开始空杯心态，准备丰富自己的认知。<br>知道自己知道——抓住了事情的规律，提升了自己的认知。<br>不知道自己知道——永远保持空杯心态，认知的最高境界。</p>
<p>现在我终于意识到，人和人根本的区别就在于这四种状态。更可怕的是，95%的人都处在第一个状态，甚至更多。这也就是为什么碌碌无为的人是大多数。视而不见，只会失去升级的可能性。只有自我否定，保持空杯心态，一个人才有可能真正成长，实现跨越。</p>
<p>今天，我们处在一个大拐弯的时代，每一个行业的认知都在迅速叠加，跨界越来越普遍。如果不保持这种“自我否定”的认知状态，很难完成对快速变化的行业的认知。</p>
<h3 id="2、人和人最大的差别是认知"><a href="#2、人和人最大的差别是认知" class="headerlink" title="2、人和人最大的差别是认知"></a>2、人和人最大的差别是认知</h3><p>认知，几乎是人和人之间唯一的本质差别。技能的差别是可量化的，技能再多累加，也就是熟练工种。而认知的差别是本质的，是不可量化的。—-<strong>解读：技能指的是具体知识点，认知指的是道理原理本质，从道理、原理可以举一反三知道很多技能。但是一般人只看原理是没有体感的，无法从原理衍生出技能，恰恰是技能先得有几个，然后由技能引申出来原理和知识点，这样叫融会贯通。99%的人要通过技能来帮助理解原理和知识点。</strong></p>
<p>记得在香港约过一次马云喝茶，他几乎不用电子邮件，当时我就琢磨，他为什么能去指挥那么大一个帝国？后来发现，他在不断观察行业变化，从变化里找关键切入点，找资源和人配合。</p>
<p>人和人比拼的，是对一件事情的理解和对行业的洞察。执行很重要，但执行本质是为了实践认知。</p>
<p>有时候，我也会鉴古通今地去琢磨，为什么鸦片战争时期，大清帝国输那么惨？简单说，一群怀揣现代物理学认知的人，打败了另一群信奉四书五经认知的人。其实就是两种不同认知的较量。</p>
<h3 id="3、认知升级的两个误区"><a href="#3、认知升级的两个误区" class="headerlink" title="3、认知升级的两个误区"></a>3、认知升级的两个误区</h3><p>真正的认知需要通过行动展现，行动一旦缺失，认知容易陷入误区。我总结了两个可能遇到的误区，未必全面，抛砖引玉：</p>
<p><strong>误区一：以为自己知道，远远不如以为自己不知道</strong></p>
<p>自以为是，是自我认知升级的死敌。回想奇虎当年搜索没干过百度，老周一度总结为不小心把公司卖了，百度没卖。他没想到，当时李彦宏对搜索的认知远高于他。老周不愿打硬仗，不愿打重型战役，不愿搞大研发，不相信算法。那时候卖，本质上是打不下去了。可他不这么认为。</p>
<p>自我否定，就是假设自己无知，是自我认知升级的唯一路径。不做痛苦的自我否定，认知上不了一个新台阶。即使正确信息摆在面前，你也会视而不见。这基本是区别英雄和凡人唯一的机会了。</p>
<p><strong>误区二：以为自己认为重要和真的认为重要，往往不是一回事</strong></p>
<p>有一个词叫自我迷惑。自认为觉得很重要，但根本没把它转化成真正的行动。</p>
<p>我最大的反思就是对头条的理解。两年多前，我说头条就是移动端的搜索。我的反思是什么？我当时认为挺重要的，一直到15年四季度，我才开始召集人马，着手海外头条的业务。</p>
<p>这是认知里经常出现的一个误区：以为自己认为很重要和真的认为很重要，往往不是一回事。</p>
<p>不行动的认知，就是伪认知。炫耀自己知道，有什么用？一个浪潮打过来，认知就没了，如同没有。真正的认知，必须知行结合。</p>
<h3 id="4、认知升级的三剂解药"><a href="#4、认知升级的三剂解药" class="headerlink" title="4、认知升级的三剂解药"></a>4、认知升级的三剂解药</h3><p>把一件事情转化成行动，难度之大。认知到行动，中间有巨大损耗。我给认知升级开了三剂解药：</p>
<p><strong>解药一：坚信大趋势</strong></p>
<p>想法要立刻转为行动。坚信大趋势，坚信这家公司的各种认知决定。不要简单的批判，你一定要相信那些行业领头人。他们拿到的信息肯定比你多，处理信息的能力比你强，他们的认知不是现阶段的你所能赶得上的。不理解，就执行，在执行中理解。</p>
<p>盲目坚信，立即行动，在行动中形成认知。不要怕死，早死早超生。去年，我想出做机器人的决定，几乎没人认为可行。我就想，先去找人，坚信趋势，立即行动。那种情况，不做，更没有机会，只能是大量时间的损耗。</p>
<p>不行动，是最糟糕的。行动，才有可能证伪。坐而论道，没有意义。</p>
<p><strong>解药二：对外求教，不做井底之蛙</strong></p>
<p>有一个对外求教的心态，非常重要。对外求教，是为了扩展你的视野。要找到带路党，吃过猪肉不一样。他们比你强不是他们聪明，而是有着你不知道的认知。</p>
<p>当年我和徐鸣做可牛影像，我们的口号是我们来了。我们的技术水平，做过的客户端体验，见啥灭啥。我们来这个行业了，谁还活得下去？结果，美图秀秀把我们打得内心都快崩溃了。</p>
<p>这是我们特别容易陷入的一种状态：以自我为中心，不向外看。面对新事物，很多人甚至连尝试和对外沟通的欲望都没有。完全不知道外面发生什么。</p>
<p>强调一点：认知理解与聪明度无关。只有从认知角度，而不从聪明角度，去理解这个世界，理解所在行业，你才会有更多不一样的认知，才能看到更多别人看不到以及顽固不愿去理解的机会。</p>
<p>越是处在绝路的团队，越是往外看得多。</p>
<p><strong>解药三：活在当下，面向未来</strong></p>
<p>活在当下，恐惧时，想想错了又如何？多错才有机会对。这是我给自己的一个思维训练。当你面对一些事情，想想最坏的结果是什么？想完你会发现，最坏结果与你内心的恐惧，根本不在一个量级。</p>
<p>恐惧就是恐惧本身。不肯尝试的本质是不敢面对所谓失败。但，这个失败的后果是什么？很少有人认真思考过。其实绝大部分失败是没有后果的。</p>
<p>再就是面向未来，纠结时，想想五年后会怎样？会不会被淘汰掉？如果五年后，你跟这个时代已形同陌路，这才是最可怕的。行业变化之快，超出我们想象。</p>
<p><strong>所谓成长就是认知升级</strong></p>
<p>所谓成长，并不来自于所谓的位高权重，不来自于所谓的财富积累，也不来自于你掌握的某一个单项技能。</p>
<p>行业里有很多这样的人。比如史玉柱。最惨时，欠了一屁股债，什么都没有。靠一个脑白金，重新崛起。因为他在整个营销上的认知水平，领先了一个时代。即便失去所有财富，甚至所有队伍，就凭他对营销的理解，也是那个时代无人可望其项背的。只要他活着，随时可翻身。</p>
<p>绝境当中，他真正拥有的核心武器，根本不是资源，而是认知。</p>
<p>想想腾讯这些年的发展。3Q大战前，腾讯讲花瓣策略——我的花朵上长出很多花瓣，每个花瓣都能干掉你。这种态势，使得腾讯在每个领域都不得不与当时各个领域最先进的认知打，非常困难；3Q大战后，腾讯的策略改为生态链。第一个投资的就是猎豹，接着又投了一批公司。</p>
<p>结果如何呢？腾讯市值从300亿美金涨到了今天的2000多亿美金。《福布斯》杂志曾经做了一个年龄在四十岁以下的四十位中国商业精英年度排行榜“40 Under 40”，第一位是刘强东，其次是王小川、我、姚劲波。马化腾当时留言说，前五个当中，四个都跟我有关系，很荣幸。</p>
<p>此后，我一直思考，其实马化腾通过投资形成的生态系，帮助他建立了足够的行业认知。因为，当他投资了这些公司之后，他就不是在跟一个普通的产品经理聊了，而是跟刘强东聊电商，跟王小川聊搜索，跟猎豹聊国际化。</p>
<p>所以，腾讯用这样一种开放的生态策略，本质拿到的不是投资挣来的钱，而是投资挣来的认知。它和大家建立友好的关系，怎么会不知道出行市场的规模、方向和目标呢？怎么会不知道国际化有哪些机会呢？核心就在于行业认知。</p>
<p>回顾360崛起给我带来了什么？简单说，就是让我拿到行业最一手的认知——一种对于安全行业，对于客户端软件，对于互联网怎么颠覆一个行业的认知。</p>
<p>离开360时，我的前东家只给了我1块钱，但朋友安慰说，没关系，你积累的认知，别人都拿不走，将来所有东西都会还给你。我就带着一个认知，走上了茫茫创业人海。</p>
<p>就因为这么一个小小的认知。</p>
<p>我才能那么坚决做Clean Master。没人知道APP全球化怎么做，好多单词也不认识，没做过商业。当时几乎对所谓国际化，对美国市场一无所知的情况下，做了All in Clean Master的决定。就因为有那样的认知。</p>
<p>因为我就信一点——360单点撬动一个行业的事情，在我面前真真切切发生过，而我就是亲历者。</p>
<p><strong>所谓成长，就来自于认知。</strong></p>
<p>有时候，想起这个时代，我脑海常浮现一个历史典故——煮酒论英雄。两人坐在那，一个指点群雄，一个一味谦恭。</p>
<p>问天下英雄是谁？曹操说唯使君与操耳，刘备吓得筷子都掉了。纵然天下各种人物拥有千军万马，但曹操真正顾忌的却是这个正在种菜的刘备。虽然刘备寄人篱下，内心却是匡扶汉室的认知。</p>
<p>后面的历史走向也正如以上的认知讨论一样。天下大势，何其复杂；即便如此，也能简化到最关键的点，即关键人的关键认知。</p>
<p>而认知的本质就是做决定。人和人一旦产生认知差别，就会做出完全不一样的决定。而这些决定，就是你和这些人最大的区别。你拥有的资源、兵力，都不重要，核心是你脑海里的大图和你认知的能力。</p>
<p>单点也好，势能也好，猎豹能有一点成绩，本质是在移动互联网巨大红利增长之下，做了一个未必那么关键但很正确的决定。那个时候，我们开了先河。但这个认知，不足以支撑我们变成一家卓越而伟大的公司。我们依然需要认知升级。而猎豹和我在过去一年获得的最大财富，就是开始了认知升级。</p>
<p>我开始思考一句话：一个人卓越，造就不了一家卓越的公司；一群人卓越，才能造就一家卓越的公司。而卓越的核心是一家公司和一群人的认知升级，否则不可能真的上新台阶。</p>
<p>只会陷入死循环：认知不统一，事情推不动。推不动的本质是大家没有建立对这件事重要性的认知。看不见也罢，顽固拒绝也罢，都不可怕。最可怕的是，我们不知道“我们不知道”。</p>
<p>如果一个人，不断想学习，想了解，去反思；空杯心态，放下恐惧，不拒绝改变。认知升级，其实也就是捅破那层窗户纸。成长如是。</p>
<h2 id="二、管理本质就是认知管理"><a href="#二、管理本质就是认知管理" class="headerlink" title="二、管理本质就是认知管理"></a>二、管理本质就是认知管理</h2><p>我曾经讲过管理三段论——目标、路径、资源。找到一个目标，想清楚路径，再投入资源。</p>
<p>但它有一个大前提叫——有判断力。没有判断力，搞错了目标，路径切不进去，资源就调不动。而这种判断力的本质，就是认知。</p>
<p>这个时代，管理不是执行管理，不是组织结构管理，而是你比别人更理解一件事情。管理的本质就是一种认知管理。</p>
<p>领导力的核心不是所谓的高情商，而是在大格局下构建对整个行业的认知体系，用大趋势做出正确的判断和聪明的决策。</p>
<p>在这个大的认知体系下，管理又可被细化为“信息、时间、人”三个维度：怎样利用“信息”做出正确的决定，怎样通过抓关键让“时间”更高效，怎样运用简化管理“人”。</p>
<p>我总结了“一体两翼”和“三个管理维度”，逐步解答以上问题。</p>
<h3 id="一体：构建领导者的认知体系"><a href="#一体：构建领导者的认知体系" class="headerlink" title="一体：构建领导者的认知体系"></a>一体：构建领导者的认知体系</h3><p>一个优秀的领导，必须在核心点上拥有覆盖队伍的认知体系。一个人的认知要大于一个队伍，大于一群人。这样的领导，才有真正存在的价值。</p>
<p>我跟腾讯总裁刘炽平交流，他说，互联网特别像一部武侠史，一群人打不过一个武林高手。你堆再多的人，不如来一个武艺出众的人。本质上，就是这个人，在这个点上的认知体系，超越了一个庞大的队伍。</p>
<p>一年半前，我见过百度的人，我跟他们说，头条就是移动上的搜索。他们不以为然。说百度有头条不具备的即时搜索，头条离了很远。一直到去年五月，百度才上了信息流。这就是两家产品在认知上的差距。</p>
<p>领导者在核心点上必须有一个强大的认知体系覆盖所有人，才值得成为一个领导，才得以做出正确的决定，才能够带领整个队伍走到一条正确的道路。如果认知错了，即使每天鸡飞狗跳，也做不成真正有效的管理。绩效，期望，只是配合手段。核心是必须有这样的认知体系。</p>
<p>所谓认知体系，是在脑海里有完整的认知框架，才能做出正确的判断。脑海要有一些抽象的框架图。比如简单、差异化、跨界、大趋势，还有时代热点等等。脑海里不断要有这样的框架。看到一个点，就拿这个框架去套。</p>
<p>怎么建立这种框架呢？首先，对市场和产品的深入了解是认知体系的基础。用产品，抓细节，像用户一样思考问题。其次，真的要去和市场上吃过猪肉的人多聊天。看看别人在干什么，这很重要。其三，切忌以听报告的方式建立认知。有些领导，派两个实习生做个调查报告，看一眼，得出一个结论。非常要命。本质上是用实习生的认知取代团队认知。</p>
<p>就像Snap CEO说，Snap chat不是聊天工具，而是改变新一代美国年轻人的沟通方式。现在它把chat去掉了，不再简单做熟人关系，而是围绕摄像头建立内容，这就与Facebook非常不一样了。Snap让Facebook头疼，不是源于钱更多，或队伍更强，而是源于对社交的认知不一样。今天就是这样一个时代——谁建立正确认知，谁就脱颖而出。</p>
<h3 id="两翼-认知管理的两剂良药"><a href="#两翼-认知管理的两剂良药" class="headerlink" title="两翼:认知管理的两剂良药"></a>两翼:认知管理的两剂良药</h3><p>大的认知体系构建之后，具体问题是否有具体方法论作支撑——比如，事情太多管不过来怎么办？做了那么多总被老板批怎么办？做得辛苦不出绩效怎么办？对此，我开了两剂良药：</p>
<p><strong>解药一：学会逆向思考，如果花的时间少一半，事情能否做得更好</strong></p>
<p>记得有段时间，我非常忙，各种会议满天飞，效率很低。我当时就想，难道当年乔布斯会比我更忙吗？归根到底还是我的管理方法不对。于是我不停追问自己，如果我现在让工作时间少一半，能不能做得更好？</p>
<p>当我的脑海里不断浮现这个问题时，我突然意识到——我忙的根源其实就在于自己认为太多事情都很重要。</p>
<p>怎么让管理变得更有效率？本质是减少真正所谓管理的量，增加判断的量。增加帮团队在关键点做决定和梳理目标的量。</p>
<p>核心是转换思维，培养做判断的能力。而不是勤勉工作的能力。勤勉工作只是基础。假设一下，如果只花一半时间，事情能不能做得更好？顺着这个方向想，很多事情就会不断要求去划分优先级。</p>
<p><strong>解药二：战略的略是忽略，不敢忽略，本质是分不清优先级</strong></p>
<p>绝大部分人觉得战略的重心是“战”，其实是“忽略”。忽略就是能放弃什么。</p>
<p>不敢忽略，本质就是分不清优先级。怎么去建立优先级？分清优先级的前提是认知清晰。你脑海里有一个格局，叫大趋势。要知道什么是更重要的。找到最关键的点，牢牢不放。不是最关键的点，学会妥协和让步。有时候，我们思维会有盲点。原因在于：视野不够宽，反思不够频。</p>
<p>人和人最大的区别就在于思维格局。什么是中层？什么是创始人？两者区别就在于：一个是迷恋具体情况，我在努力工作；一个是高低结合，我既能努力工作，又能不断花时间去反思，去判断，去拿到认知。而且清楚知道，低的目的是高。即我的每一个执行，本质上又在建立我的认知。</p>
<h3 id="三个管理维度：信息、时间、人"><a href="#三个管理维度：信息、时间、人" class="headerlink" title="三个管理维度：信息、时间、人"></a>三个管理维度：信息、时间、人</h3><p>宏观层面，领导者要构建对行业的认知体系；那么微观层面，执行操作时，怎样才能做到更聪明的工作？怎样找到那件最重要的事？我从信息、时间、人三个维度剖析管理方法。</p>
<p>先说信息维度。人的本质就是一个CPU。运算能力再强，没有足够的数据输入也不会有产出。有足够大的信息输入，足够高的反思频度，你才会有足够的信息输出，也才会产生格局，做出正确判断。搜集信息的目的，不是为了保持自身现状，而是为了进一步成长获得新能力，使我们做出正确的判断和聪明的决策。</p>
<p>信息怎么输入：</p>
<p>第一，深入分析对手。花时间加大信息搜集，磨刀不误砍柴工。</p>
<p>第二，定期遍访行业。了解对手，否则会失去行业认知。</p>
<p>第三，不断招聘行业里的人。他们不仅是来工作的，也会带来行业里的认知。<br>一个领导者的本质就是做正确的决定。只要你拿到足够的信息，就能做出正确的决定，执行将容易十倍，矛盾也会迎刃而解。有一句话叫主将无能累死三军。执行很难的本质是没有做出正确的决定。</p>
<p>再来说时间维度，管理上最重要的资源就是领导人时间。时间的分配，表明了一个领导者对实际情况的优先级判断。</p>
<p>我们经常自认为一件事情很重要，回头一看，根本没花足够时间，没放足够人力，没放足够资源。时间都去哪儿了？</p>
<p>反问下自己：</p>
<p>时间是如何分配的？<br>构建格局上花了多少时间？<br>信息输入花了多少时间？<br>关键人身上花了多少时间？</p>
<p>是不是偶尔想到了，去思考一下，还是变成一种工作习惯？<br>猎豹最早能杀出来的一个核心原因，就是我们当时乐此不疲地参加各种展会，跟各种人聊，建立了对国际APP市场的认知。</p>
<p>我作为CEO亲自站台，亲自抗资料，住很远的酒店，转地铁转三次，走路还得十几分钟，听起来都很辛苦。但一有展会，我就会带人去。而且经常是规模性的。</p>
<p>看上去花了这么多时间，但实际上建立了认知，找到了简单的切入点。于是，整个公司的工作都变得简单了，反而为其他事务节约了时间。</p>
<p>时间是最重要的资源。经常有同事问我，你天天管公司，介绍新文章，还玩无人机和王者荣耀，时间怎么用的？其实很简单。我每天都会想：有哪几个关键的会，关键的人，关键点是什么。</p>
<p>讲完信息和时间，回到人的维度。一句话：学会通过管一个人达到管一片人的目的。</p>
<p>首先，管理一个人，解决一大片。我们很容易陷入一种状况，搞一堆人，这个要照顾，那个要照顾。团队初创时，可扁平化。但越往后，越成熟的业务，越要找一个堪以重用的人。重用到什么程度？为了这一个人，能把整个团队重构掉。</p>
<p>要简化对人的管理，找到关键人。在关键人身上花足够多的时间，把足够的认知传递给这个关键人，让他做一群人的决定。切忌多人平行站位的职责不清。多人负责，容易变成每个人都不满意。给一个人足够授权，职责清晰，简化管理，即便没有达到预期，更换人时他也更容易接受。</p>
<p>其次就是简化目标。不要给一个人多目标。领导最大的职责是帮员工找到一个简单的目标。这就考验领导，能否构建纵深的行业认知体系，找出那个关键目标，帮助团队简化目标，建立一个正确统一的目标认知。团队目标越简单，越明确，越容易达成一致。</p>
<p>现在我做管理，就简化成“定目标”和“找关键人”。目标要简单清晰，人要能挺身而出，超出预期，一战就要解决问题。</p>
<p>好领导，首先是打胜仗。开战之前，就要有七分胜算。这七分胜，就靠认知。打的就是认知之仗。看上去千军万马在打，本质是两个将领脑海里的战争格局。胜负基本已定。</p>
<p>先胜后战，胜算就在认知。管理如是。</p>
<h2 id="三、战略就是格局-破局"><a href="#三、战略就是格局-破局" class="headerlink" title="三、战略就是格局+破局"></a>三、战略就是格局+破局</h2><p>两年前，我写过一篇《一家公司CEO该如何做战略》。应该说，上市以来，我花在战略思考的时间，不少。伴随猎豹前进中遇到的一些问题，我的思考也在不断深入。</p>
<p>我曾经说过战略三部曲是预测-破局点-All in。破局的单点，被很多人讨论。但，预测这件事，给忽略了。然而，预测背后就是格局观。怎么理解？</p>
<p>今天的移动互联网，纯粹靠一个单点爆发，已经很难了。过去十年，甚至二十年，互联网是一片蛮荒之地，需求稀缺，人才也稀缺。只要你投身这个行业，随便找个单点开始深耕；只要你还算努力和坚韧，运气也不太差，就有机会做家不错的公司。所以，那个时候不需要预测，需要的就是干干干！</p>
<p>于是，我们都觉得自己很牛，以为世界就是这样。我们以为不断努力努力再努力，加班加班再加班，逼疯自己，逼死对手，战争就结束了，却没有意识到——我们站在互联网这个正确的格局和风口上，做什么事情都是对的，而我们正是那群幸运的猪。</p>
<p>不幸的是，这个世界是不连续的。经过二十年的发展，今天，互联网已经是一个传统行业。风停了，放眼望去，到处是血海竞争，乌压压一片创业大军。勤奋依然很重要，但聪明的勤奋才是关键。这个时候，就要求我们想清楚，行业里的大风在哪里，并做出预测。</p>
<p>因此，你的脑海里必须有一个对于这个行业越来越清晰的认知格局脑图。哪里已经是过度竞争，哪里刚兴起却没人察觉，三四线城市网民的不同在哪，互联网与哪个行业、以哪种形式的结合会有机会等等。</p>
<p>我们需要在这样的大格局下，在过去积累的认知红利之上，重新构建新的认知体系，制定战略的新打法，去更大的空间，寻找新的破局点和机会。</p>
<p><strong>战略认知=格局思考</strong></p>
<p>过去，我讲过一句话，叫“现象即规律”。现在，我把它解释得更清楚一点，叫“没有偶然，只有必然，所有单点都是大趋势下的必然。”</p>
<p>一个现象，它发生的时候，一定有大趋势支撑它。没有孤立的单点，本质都是大趋势下的单点的必然。</p>
<p>以前，我们就站在肥沃的土地上，不用深入思考就可有所作为；今天，挤进来的人越来越多，思考某个现象为什么突然生机勃勃，它背后反应的规律是什么，怎么利用这个规律帮助自己找到下一个肥沃的土壤，就变得非常关键了。</p>
<p>我曾经一度认为：</p>
<p>美国人强调“think different”很有情怀，后来才发现，本质不是情怀，而是为了减少竞争成本。因为美国创业者们比我们更早进入血海竞争阶段，“勤奋+努力+不要命”已经很难产生质的差别了，才逼迫他们用“更勤奋的思考”来避免高成本的竞争，从而降低失败概率。<br>创业必须讲究方法论，必须讲究不同情形下的不同方法。今天互联网的竞争格局，远远不是十年前的样子。我们必须think different，而think different的前提，就是要有行业格局认知，看清大趋势，在大趋势下做判断。</p>
<p>所谓战略，就是在这样的格局认知下，找到破局点，制定路线图，投入资源。如果不去建立这样的认知，公司很容易陷入一些误区。</p>
<p><strong>战略认知的两个误区</strong></p>
<p>第一个误区是：见招拆招，啥热做啥，啥熟悉做啥。这是懒惰思考，不愿意认知升级的表现。结果就是越做越多，越做越累，越做越委屈。</p>
<p>我拿自己做例子。</p>
<p>一年多前，我跟几个高管反思，猎豹今天是不是做挺多的？我们在过去野蛮生长中，不断强调快速执行，做热点，认为只要做好这个点就会有机会。没有拿整个大趋势，或者叫大格局下的战略，做一个大方向下的可串联的点。</p>
<p>但事实上，整个互联网的竞争加剧比我们想象中来得快。如果每个单点，不是在一个大格局下的累加，以致每个单点都会遇到对手强大的竞争，很难长大。</p>
<p>我们老说战略的懒惰，就这个意思。看到一个机会，扑上去。看到另一个机会，再扑上去。看上去每天都在努力工作，但回头一看，各种方向上布满了各种产品，彼此不能借力，也没办法真正在单点上聚焦。</p>
<p>第二个误区是：做产品的方法论依然停留在5年前，认为抓一个简单功能热点就颠覆格局。</p>
<p>我经常在微博上收到各种私信，说他有个点子，要颠覆腾讯，颠覆阿里，问要不要见他？我基本不回，为什么？</p>
<p>因为我不觉得存在这样的点。如果20年前，想个网址导航，还有可能成为hao123。几个人在屋子里攒出一个聊天APP，还有机会是QQ，逆向一个Dos3.0，写个WPS，一不小心就成为一家上市公司。那是一个多么美好的时代。</p>
<p>但这个时代真的过去了。移动互联网的APP，不稀缺了，越来越少的APP可异军突起了。APP呈现越来越强的头部效应。而头部的那些大佬们，已经总结了一整套如何面对单点突破的小对手。尽管你精干灵活，但他们会用生态、用流量、用更强大的研发力量碾压你。</p>
<p>只把一个单点做到极致就能创造奇迹的时代，真的过去了。</p>
<p>为什么会出现这种情况？归根到底，还是以前互联网不被重视，具备互联网技能和认知的人有限，所以你做了，别人要么看不起，要么做不来。而今天，“互联网就是金矿”的认知，已经深入人心；互联网相关从业者，浩浩荡荡。这好像一碗好吃的牛肉面，如果利润很高，如果能做出来的人已经一大片，就更不要提那些巨头们了。</p>
<p>怎么办？你必须结合趋势，结合整个战略思考，把所有东西累加进去。容易打的仗，已经打完了。要花足够的精力和时间，去构建自己的行业格局认知。在看上去繁杂纷扰的信息中，不断深度思考，加大自己的认知优势，然后在熙熙攘攘的人流中找到不为人知的机会，趁着大家还不够懂，突然发起战役，全力以赴。</p>
<p>这就回到开头的问题，我们究竟该如何构建这样一种格局思考的能力？</p>
<p>举个例子：回顾几个破局点</p>
<p>先拿我自己做例子，回顾几个破局点的思考路径。</p>
<p>怎么产生Clean Master这一个点？我当时觉得，中国互联网APP水平还是不错的。拉了一个技术同事到美国开会，请了一个挺牛的人介绍安卓。讲完后，我的同事说，这人水平一般。</p>
<p>我想，这位同事虽然狂了点，但说得很中肯。而这位牛人，已经是一家60人硅谷创业公司的CTO。我们做了一些动效给他，他还很疑惑怎么做到的。我发现，中美之间并不存在技术代差。</p>
<p>于是，我开始思考，中国APP能否走出去。最后我作出了一个大趋势的判断——中国APP已经领先全球了，我们已具备这样的基础。这个判断做出以后，后面的事情就简单了。</p>
<p>包括猎豹上市后，该干什么？当时我的判断是，移动互联网颠覆格局的机会基本过去了，要想找到猎豹十倍增长的机会，必须从未来着手。所以那时，我跑硅谷，跑以色列，去世界各地看创业团队。</p>
<p>在国内搞傅盛战队，办紫牛基金，只投非纯粹互联网的早期创业项目，想知道年轻创业者在干什么，互联网跨界到底怎么发生的。</p>
<p>在这个过程中，我慢慢发现了人工智能和机器人。尤其深度学习，开始在很多创业方向中都有体现。它把很多过去跨行业的算法统一了起来，并且效果上有了质的飞跃，这让我非常着迷。</p>
<p>而机器人又是个工具，和猎豹过去做的事情不谋而合。它到底长什么样子，谁也不知道。这就有机会去定义。作为一个产品经理，要去定义机器人的交互是怎样的，人的感受是怎样的？我觉得这个特别吸引我，也符合我的特质。</p>
<p>与此同时，能把基于深度学习产生的视觉、听觉技术革命，与机器人产业结合起来，更是今天很少人意识到的巨大机会。</p>
<p>这个行业认知建立以后，执行也就变得简单了。去年年初，我开始下大力气组建人工智能和机器人团队。在整个行业暴热，开启人才大战的前夜，我们组建了一支精壮的国际化队伍，还全力以赴研发了产品。很快大家就会看到我们即将发布的AI产品，多少也算领先了业界一步。</p>
<p>回想当初，我说自己要倾家荡产做机器人，很多人还认为我只是蹭个热点，搞个宣传。但把真实情况串起来后，你会发现，我是经过了深思熟虑，花了足够时间认真思考的。如同上文所述，这个单点，它不是偶然，而是我整体思路的必然。</p>
<p>所以，一个点不是孤立存在的。你的脑海里有没有一个大格局？这个大格局是什么？这很重要。</p>
<p>回到根本：怎样做战略？</p>
<p>首先，脑海要有大格局。大格局就是对这个行业深入的、清晰的认知。</p>
<p>你心中要不断问这个问题——在这个行业里，什么才是真正的机会？什么才是下一个趋势？这样，你才有机会，才能判断，这个点对不对。否则，可能消耗了很多人，把什么都投进去，但不形成持续爆发增长。因为它不是大趋势下的点。本质就不该做。</p>
<p>我们需要花足够的时间去了解行业，去思考对手，去观察现象。在获取大量信息后，不断在脑海里做思维推演，去判断。</p>
<p>当然，这些格局认知里，至少你要想到一些关键词，比如全球化、视频、品牌、AI、争夺时长和红利结束等等。整个行业的大格局，没必要从零开始做认知判断，一些看上去很热门的趋势，选择相信，再去分析背后的规律。</p>
<p>其次，养成格局和破局结合的思维习惯。高是什么？你的格局，大风口。破局就是找到与众不同的那个点。二者缺一不可。</p>
<p>过去，我们太强调那个单点的重要性了。大部分人的思维习惯，停留在游击战。有个好想法，做起来。就像游击队，看到有头猪，就抢一把。对手在哪，他们在怎么做？这个领域的趋势会怎样？这个点领先者是不是稍微抄袭下就一样了？基本都不知道，也不去了解。只在为自己的想法激动不已，恨不得明天就赶紧实现。</p>
<p>今天，做一个互联网产品就像做一辆汽车。如果你只有对某个功能（比如汽车安全）的好想法，而不去认真思考资金，工厂，产业链，上下游，品牌定位，对手策略，消费者习惯迁移等因素，你认为你有机会超越宝马、奥迪、奔驰吗？</p>
<p>战略，就是要求我们进行深入的、逼迫自我的思考。真正的超越机会，不是来自于在工作时间上把对手逼死，而是在认知深度上先把自己逼疯。</p>
<p>互联网竞争已经白热化的形势下，做战略的关键点，就在于不断加深自己的认知，找到已经存在但不为人知的那个秘密。而且，这个秘密所能孕育的机会，要足够大；离现有领先者的区域，要足够远。核心是你能否具备超出对手的、对行业的、与众不同的认知。基于这个格局认知，为自己撕开一道突破口。</p>
<p>简单一句话概括——经过充分思考和认真研究后，制定清晰目标以及持续推进的路线图，这应该就是战略的全貌。</p>
<p>同时我也要补充一点：战略是在这个路线图下的势能的累加，不能累加势能的，再有效果的执行，本质都是增加成本。</p>
<p>这就慢慢理解了杰克·韦尔奇说的“数一数二法则”。因为不数一数二，长期没有竞争力，还耗费精力。就不是一个累加势能的点。</p>
<p>如今，猎豹的新Slogan——Make The World Smarter，目标就是要围绕AI方向做累加。猎豹的路线非常清楚，那就是——以人工智能为核心的累加，以6亿月度活跃用户数据为基础，用技术和产品的突进，完成我们整个目标。对目标没有持续、有效累加的事情，都没有意义。</p>
<p>猎豹要做的，是将人工智能跟传统移动互联网的业务相结合。产品创新上不断发力，真正做出一些赶超BAT的AI产品。</p>
<p>回到战略，它的本质是什么？我认为，战略就是一个杠杆。它让你做的每一件事，都放大几倍，几十倍。一旦远离这个杠杆，就变成小公司创业模式。关键是，这种创业模式，又比不过真正的创业公司。</p>
<p>猎豹走到今天，重新回到创业，置身于一处——于大格局之下，寻找新的破局点。战略如是。</p>
<p>来源介绍：盛盛GO（ID：fstalk），猎豹移动联合创始人兼CEO，关注创新创业，人工智能，还有机器人。</p>
<p>from <a href="http://mp.weixin.qq.com/s/uDqm18HdijB59sUQJMckrw" target="_blank" rel="external">http://mp.weixin.qq.com/s/uDqm18HdijB59sUQJMckrw</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在今天这样一个急剧变化的时代，每一个行业的认知都在迅速更新迭代，跨界也越来越普遍。&lt;/p&gt;
&lt;p&gt;谁能正确建立和升级认知，谁就能脱颖而出。猎豹 CEO 傅盛他将认知梳理为三部曲：成长就是认知升级；管理本质就是认知管理；战略就是格局+破局。&lt;/p&gt;
&lt;p&gt;认知升级：坚信大趋势
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2020/08/29/%E8%AE%B0%E4%B8%80%E6%AC%A1%E8%AF%A1%E5%BC%82%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E6%95%85%E9%9A%9C%E7%9A%84%E6%8E%92%E6%9F%A5%E8%BF%87%E7%A8%8B/"/>
    <id>http://yoursite.com/2020/08/29/记一次诡异的数据库故障的排查过程/</id>
    <published>2020-08-29T09:39:11.171Z</published>
    <updated>2019-10-25T05:15:39.337Z</updated>
    
    <content type="html"><![CDATA[<h1 id="记一次诡异的数据库故障的排查过程"><a href="#记一次诡异的数据库故障的排查过程" class="headerlink" title="记一次诡异的数据库故障的排查过程"></a>记一次诡异的数据库故障的排查过程</h1><p>这两周遇到了一个诡异的应用数据库访问现象：某业务通过 TDDL 访问数据库，偶尔会出现超长的访问 RT，导致整个 HSF 服务接口调用超时。从 Eagleeye 调用链分析平台看到的图形是这个样子：</p>
<p><img src="http://img1.tbcdn.cn/L1/461/1/17a3e60a6a1b18a081870b2762bcafce7977a27e" alt="1"></p>
<p>从这张图上可以看出：</p>
<ol>
<li>查询多张表 RT 都超过 500ms；</li>
<li>同样一次调用，另一些表的 RT 只有 1ms；</li>
<li>都是访问同一个数据库；</li>
</ol>
<p><strong>在什么情况下会出现这种现象呢</strong>？ 完全难以捉摸：</p>
<ol>
<li>大约 0.3% 的服务调用会出现这种现象，其他请求完全不受影响；</li>
<li>几乎不受业务访问压力的影响，包括预发环境也能重现这种现象；</li>
<li>所有应用服务器上都能重现；</li>
</ol>
<p>如图：<br><img src="http://img2.tbcdn.cn/L1/461/1/4d553ff978f2e53ad4fa2d2b3bf19c198ff52480" alt="2"> </p>
<p><strong>问题会不会在连接池</strong>？或者是应用、中间件、网络、数据库上有资源瓶颈？</p>
<p>不会。因为所有业务请求都需要同样的资源，大家都在同一个资源池上排队。假设请求是相同的，插队是很少的 —— 如果有人排队等了 500ms 才拿到资源，有人 1ms 就抢到了资源 —— 那么，一定会有等待了 100ms, 200ms, 300ms, 400ms … 的请求，RT 应该呈现平均分布。 </p>
<p>现象显然不符。那什么场景下才会产生明显差异？ 按已有经验，也许是：</p>
<ol>
<li>锁等待：<br>  部分数据频繁加锁，部分请求长时间等待锁。或者非公平的锁等待，插队太多，请求被 “饿死”。</li>
<li>数据不均匀：<br>  部分业务ID 查询返回的数据量很大或者索引扫描的数据库行数很多。</li>
<li>IO 抖动：<br>  高 IO 压力下，命中缓存的请求和需要读磁盘的请求 RT （因为 IO 排队）会差别很大。</li>
</ol>
<p>当然，也不排除是未知的中间件/数据库 bug。</p>
<p>首先，锁等待和 IO 抖动的问题被排除了，因为问题请求都是 SELECT 语句，MySQL InnoDB 支持 MVCC, 查询是不加锁的。此外，数据库压力很低，buf 命中率很高，也不是 IO 的原因：<br><img src="http://img4.tbcdn.cn/L1/461/1/9f4ef8852aa535348e3d477c1f6fbfa2b85b42fe" alt="7"></p>
<p>数据不均匀的可能性也被排除了。高 RT 查询中，ttp_order 表的查询是主键查询，符合条件的数据有且只有一条。并且，仔细检查 Eagleeye 记录发现：相同入参的 HSF 服务调用，也会出现有时快、有时慢的现象。</p>
<p>最后，DBA 在业务数据库上完全看不到慢查询。上文 1）2）3） 三种情况导致的查询 RT 超过 1s 都会记录 MySQL 慢查询日志。从 Eagleeye 可以看到最高 RT 超过 2s，但是在 MySQL 的慢查询日志却没有记录。</p>
<p>已知可能性都被排除了。没办法，只能上应用机器看看日志。</p>
<p><strong>还真的有发现</strong>。从 TDDL 统计日志 /home/admin/logs/tddl/tddl-stat.log 发现，业务数据库包含 8 个分库（分库分表），高 RT 查询都来自其中 2 个分库：</p>
<pre><code>V1 2015-06-24 16:24:24:629 read statistics
 [ appName:TRIP_TTP8_APP dsKey:g1-myb077004_et2_trip_ttp_00_3306 
   times:1779 errors:0 qps:29 rt(avg/min/max):94/0/2164 ]
V1 2015-06-24 16:24:24:630 write statistics 
[ appName:TRIP_TTP8_APP dsKey:g1-myb077004_et2_trip_ttp_00_3306 
   times:6 errors:0 qps:1 rt(avg/min/max):0/0/1 ]
V1 2015-06-24 16:24:24:630 dataSource statistics
 [ appName:TRIP_TTP8_APP dsKey:g1-myb077004_et2_trip_ttp_00_3306 
   poolSize:30 active(now/max):3/9 pooling(now/max):10/13 create:0 destroy:0 errors:0 ]
V1 2015-06-24 16:24:24:630 read statistics
 [ appName:TRIP_TTP8_APP dsKey:g1-myb077004_et2_trip_ttp_01_3306 
   times:1647 errors:0 qps:27 rt(avg/min/max):69/0/2164 ]
V1 2015-06-24 16:24:24:630 write statistics 
[ appName:TRIP_TTP8_APP dsKey:g1-myb077004_et2_trip_ttp_01_3306 
   times:14 errors:0 qps:1 rt(avg/min/max):0/0/1 ]
V1 2015-06-24 16:24:24:630 dataSource statistics 
[ appName:TRIP_TTP8_APP dsKey:g1-myb077004_et2_trip_ttp_01_3306 poolSize:30 active(now/max):2/8 pooling(now/max):11/13 create:0 destroy:0 errors:0 ]
V1 2015-06-24 16:24:24:631 read statistics
 [ appName:TRIP_TTP8_APP dsKey:g1-myb077004_et2_trip_ttp_02_3307 
   times:1829 errors:0 qps:30 rt(avg/min/max):0/0/1 ]
V1 2015-06-24 16:24:24:631 write statistics
 [ appName:TRIP_TTP8_APP dsKey:g1-myb077004_et2_trip_ttp_02_3307 
   times:10 errors:0 qps:1 rt(avg/min/max):0/0/1 ]
V1 2015-06-24 16:24:24:631 dataSource statistics 
[ appName:TRIP_TTP8_APP dsKey:g1-myb077004_et2_trip_ttp_02_3307 
   poolSize:30 active(now/max):0/8 pooling(now/max):8/8 create:0 destroy:0 errors:0 ]
...
</code></pre><p>可以看出 00 和 01 库的查询 RT <code>rt(avg/min/max):94/0/2164</code> 与下面的 02 库 <code>rt(avg/min/max):0/0/1</code> 完全不同，而且每个周期的统计日志重复该现象。</p>
<p>巧合的是，00/01 库位于在同一个 MySql 实例 10.x.77.4:3306 上，应用的其他分库则全部位于同一台物理机的其他实例（10.x.77.4:3307~3309）上。</p>
<p>这样，问题更可能是这个数据库实例造成的。因为 TDDL 中间件处理不同分库没有区别，不会导致 00/01 库出现高 RT 查询，而 02/03 库完全没有问题。</p>
<p>DBA 检查了 10.x.77.4 上的 4 个 MySQL 实例，没有发现 3306 实例配置有特殊差异。从 DBFree 提供的数据库监控看，3306 与其他实例的监控曲线也没有明显的差别。</p>
<p>由于应用端统计到的数据库 RT 很大，而数据库端看到的查询很快。怀疑的目标转移到了网络上。难道是网络问题？抓个包看看。</p>
<p>tcpdump host 10.x.77.4 and port 3306 -s 0 -w xxx.pcap<br><img src="http://img2.tbcdn.cn/L1/461/1/ab9c0dccef6f2c862e3bbd58a6ffdf0d016d032a" alt="8"></p>
<p>Wireshark 的 RTT 统计表明网络延迟很低：<br><img src="http://img1.tbcdn.cn/L1/461/1/2fbb84b615183d35b7a474681e075d3844868f85" alt="9"></p>
<p>抓包的确可以看出有一些 MySQL 请求的 RT 很高（600ms）：<br><img src="http://img4.tbcdn.cn/L1/461/1/9e89e6f2450dff46536d70a9e27876cc8507db06" alt="10"></p>
<p>而大多数请求的 RT 比较正常（3ms）：<br><img src="http://img2.tbcdn.cn/L1/461/1/d507dbf33a6230fcc3adf986af3702c3d2b885f9" alt="11"></p>
<p><strong>但是发现一个特殊现象</strong>：<br>同一条 TCP 连接（stream）的请求，要么都慢（500ms 以上），要么都快（5ms 以下）。<br> <img src="http://img4.tbcdn.cn/L1/461/1/783cc78dba90bebad5e655430bcacd97706a6b89" alt="13"><br>这个状况有点复杂。很少有因素影响单个 TCP 连接的处理效率。<br>硬件 bug？<br>TCP 连接所在的网卡中断处理不均衡？<br>负责处理请求的 MySQL 线程被 Affinity 到繁忙的 CPU 上？</p>
<p>结果都不是。排查网络中间看了眼 TDDL 统计日志，发现高 RT 查询的现象从 16:42 起完全消失了，现在的 00/01 库统计日志和其他分库一样：</p>
<p><code>V1 2015-06-24 17:20:24:629 read statistics 
[ appName:TRIP_TTP8_APP dsKey:g1-myb077004_et2_trip_ttp_00_3306 
times:1826 errors:0 qps:30 rt(avg/min/max):0/0/6 ]
V1 2015-06-24 17:20:24:629 write statistics 
[ appName:TRIP_TTP8_APP dsKey:g1-myb077004_et2_trip_ttp_00_3306 
times:24 errors:0 qps:1 rt(avg/min/max):0/0/1 ]
V1 2015-06-24 17:20:24:630 dataSource statistics 
[ appName:TRIP_TTP8_APP dsKey:g1-myb077004_et2_trip_ttp_00_3306 
poolSize:30 active(now/max):0/7 pooling(now/max):11/11 create:0 destroy:0 errors:0 ]
V1 2015-06-24 17:20:24:630 read statistics 
[ appName:TRIP_TTP8_APP dsKey:g1-myb077004_et2_trip_ttp_01_3306 
times:2393 errors:0 qps:39 rt(avg/min/max):0/0/3 ]
V1 2015-06-24 17:20:24:630 write statistics 
[ appName:TRIP_TTP8_APP dsKey:g1-myb077004_et2_trip_ttp_01_3306 
times:13 errors:0 qps:1 rt(avg/min/max):0/0/3 ]
V1 2015-06-24 17:20:24:630 dataSource statistics 
[ appName:TRIP_TTP8_APP dsKey:g1-myb077004_et2_trip_ttp_01_3306 
poolSize:30 active(now/max):0/7 pooling(now/max):13/13 create:0 destroy:0 errors:0 ]
.....</code></p>
<p>询问业务方，发现 HSF 调用超时的问题也消失了。再询问 DBA ,..  DBA 的确在 16:42 的时候进行了操作：kill 了 10.x.77.4：3306 上的多余 DRC 线程。</p>
<p><strong>什么是 DRC</strong>?  DRC 是一个获取数据库增量数据的服务。它采用 MySQL 主备复制协议，在数据库上启动一个长期运行的 Binlog Dump 线程，持续抓取新增的 Binlog 日志发送给远端的 DRC 节点。</p>
<p>为啥 DRC 会影响连接的处理？ 这以前在 MySQL 上从没有遇到过。因为 DRC 使用 MySQL 的主备复制协议，对数据库的影响理论上跟一台备库没有区别。 尽管 DRC 会长期占用一个服务线程，但是普通 MySQL 是一个线程服务一个连接的调度方式，除了 IO，锁和 CPU 调度，不同连接上的请求不会相互影响。</p>
<p>咨询 DBA 同学才知道：原来业务的 MySQL 采用了 AliSQL 的线程池插件，是线程池 + DRC 才导致了这个问题：</p>
<p><strong>MySQL 线程池</strong> 是个高大上的功能。拥有线程池功能的 MySQL Percona  分支以及企业版声称可以 “提升 MySQL 扩展性 60 倍”。它的好处是让 MySQL 用较少的线程支持更多的连接与并发。因为线程数太多会在锁和 IO 上产生更多的竞争，使得更多的 CPU 资源消耗在上下文切换、spin 和内核调度上。</p>
<p>AliSQL 的线程池做了更多的优化。其中一个优化是对线程进行了分组，每个分组只有少量（2-3 个）线程。这样每个组的资源相互隔离，防止慢查询将全部线程池资源耗尽，导致 MySQL 无法响应正常业务请求。</p>
<p>在分组模式下，如果有新的连接进入 MySQL，新连接会以 HASH 的方式固定分配到其中一个分组，由这个分组内线程响应连接上的请求。当处理结束，响应请求的线程就从连接释放，然后去处理同一分组上的其他连接上的请求。</p>
<p>但是上面已经说过了，DRC 的连接与普通连接不一样。它的请求处理一直不会结束，这样处理 DRC 请求的线程就会一直不释放。而 AliSQL 一个分组内的线程实在太少了。如果正好一个分组分配到 2 个以上的 DRC 连接，该分组内就没有空闲线程来响应其他请求了。因此，被固定分配到这个分组的连接请求得不到线程处理，表现在应用端就是高 RT 查询。</p>
<p><em>注：这里表现出高 RT 而不是超时，原因是 MySQL 线程池有另一个参数 <code>thread_pool_stall_limit</code> 防止线程卡死．请求如果在分组内等待超过 <code>thread_pool_stall_limit</code> 时间没被处理，则会退回传统模式，创建新线程来处理请求。这个参数的默认值是 500ms。 – by @蛰剑</em></p>
<p>BTW，由于请求是堆积在连接 TCP buf 中没人处理，MySQL 端是无法记录到请求的等待时间的。这样，MySQL 的慢查询日志就不会有记录。</p>
<p>知道原因后，解决问题就简单了。临时解决方案是尽量减少单个 MySQL 实例上 DRC 连接的数目。最终解决方案是 AliSQL 进行升级，让 DRC 这样的请求走独立线程分组，或者直接绕开线程池，回到一个线程服务一个连接的模式。</p>
<p>在这个案例里，能最终找到问题原因其实有点靠运气的。但是排查和分析过程比较有意思。</p>
<p>其实一开始的分析已经接近了问题的本质：资源池的均衡问题 —— 如果所有请求都在同一组资源上排队等待，那么 RT 一定是连续分布的。AliSQL 的线程分组功能实际上拆分了资源池，使得不同的请求在不同的资源池上排队。如果这些池中的资源数量不同，那么就会有请求更容易得到资源，有请求得不到资源，出现类似的 RT 分布。</p>
<p>先记到这里，后面遇到有意思的排查再继续分享。</p>
<p>感谢 DBA @朱曜鑫 @朱旭，应用 @神月 @张浚 在排查问题上的协助。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;记一次诡异的数据库故障的排查过程&quot;&gt;&lt;a href=&quot;#记一次诡异的数据库故障的排查过程&quot; class=&quot;headerlink&quot; title=&quot;记一次诡异的数据库故障的排查过程&quot;&gt;&lt;/a&gt;记一次诡异的数据库故障的排查过程&lt;/h1&gt;&lt;p&gt;这两周遇到了一个诡异的应用数
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>MySQL JDBC StreamResult 和 net_write_timeout</title>
    <link href="http://yoursite.com/2020/07/03/MySQL%20JDBC%20StreamResult%20%E5%92%8C%20net_write_timeout/"/>
    <id>http://yoursite.com/2020/07/03/MySQL JDBC StreamResult 和 net_write_timeout/</id>
    <published>2020-07-03T09:30:03.000Z</published>
    <updated>2020-08-02T12:41:18.366Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MySQL-JDBC-StreamResult-和-net-write-timeout"><a href="#MySQL-JDBC-StreamResult-和-net-write-timeout" class="headerlink" title="MySQL JDBC StreamResult 和 net_write_timeout"></a>MySQL JDBC StreamResult 和 net_write_timeout</h1><p>MySQL JDBC 在从 MySQL 拉取数据的时候有三种方式：</p>
<ol>
<li>简单模式，也就是默认模式，数据都先要从MySQL发到client的OS TCP buffer，然后JDBC把 OS buffer读取到JVM内存中，读取到JVM内存的过程中憋着不让client读取，全部读完再通知inputStream.read(). 数据大的话容易导致JVM OOM</li>
<li><strong>useCursorFetch=true</strong>，配合FetchSize，也就是MySQL把查到的数据先缓存到本地磁盘，然后按照FetchSize挨个发给client。这需要占用MySQL很高的IOPS（先写磁盘缓存），其次每次Fetch需要一个RTT，效率不高。</li>
<li>Stream读取，Stream读取是在执行SQL前设置FetchSize：statement.setFetchSize(Integer.MIN_VALUE)，同时确保游标是只读、向前滚动的（为游标的默认值），MySQL JDBC内置的操作方法是将Statement强制转换为：com.mysql.jdbc.StatementImpl，调用其方法：enableStreamingResults()，这2者达到的效果是一致的，都是启动Stream流方式读取数据。这个时候MySQL不停地发数据，inputStream.read()不停地读取。一般来说发数据更快些，很快client的OS TCP recv buffer就满了，这时MySQL停下来等buffer有空闲就继续发数据。等待过程中如果超过 net_write_timeout MySQL就会报错，中断这次查询。</li>
</ol>
<p>从这里的描述来看，数据小的时候第一种方式还能接受，但是数据大了容易OOM，方式三看起来不错，但是要特别注意 net_write_timeout。</p>
<p>1和3对MySQL Server来说处理上没有啥区别，也感知不到这两种方式的不同。只是对1来说从OS Buffer中的数据复制到JVM内存中速度快，JVM攒多了数据内存就容易爆掉；对3来说JDBC一条条将OS Buffer中的数据复制到JVM(内存复制速度快)同时返回给execute挨个处理（慢），一般来说挨个处理要慢一些，这就导致了从OS Buffer中复制数据较慢，容易导致 TCP Receive Buffer满了，那么MySQL Server感知到的就是TCP 传输窗口为0了，导致暂停传输数据。</p>
<p>在数据量很小的时候方式三没什么优势，因为总是多一次set net_write_tiemout，也就是多了一次RTT。</p>
<h2 id="net-write-timeout"><a href="#net-write-timeout" class="headerlink" title="net_write_timeout"></a>net_write_timeout</h2><p>先看下 <a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_net_write_timeout" target="_blank" rel="external"><code>net_write_timeout</code></a>的解释：The number of seconds to wait for a block to be written to a connection before aborting the write. </p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Command-Line Format</td>
<td><code>--net-write-timeout=#</code></td>
</tr>
<tr>
<td>System Variable</td>
<td><code>net_write_timeout</code></td>
</tr>
<tr>
<td>Scope</td>
<td>Global, Session</td>
</tr>
<tr>
<td>Dynamic</td>
<td>Yes</td>
</tr>
<tr>
<td>Type</td>
<td>Integer</td>
</tr>
<tr>
<td>Default Value</td>
<td><code>60</code></td>
</tr>
<tr>
<td>Minimum Value</td>
<td><code>1</code></td>
</tr>
</tbody>
</table>
<p>从JDBC驱动中可以看到，当调用PreparedStatement的executeQuery（）方法的时候，如果我们是去获取流式resultset的话，就会默认执行SET net_write_timeout= ？ 这个命令去重新设置timeout时间。源代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">if (doStreaming &amp;&amp; this.connection.getNetTimeoutForStreamingResults() &gt; 0) &#123;  </div><div class="line">            java.sql.Statement stmt = null;  </div><div class="line">            try &#123;  </div><div class="line">                stmt = this.connection.createStatement();                    ((com.mysql.jdbc.StatementImpl)stmt).executeSimpleNonQuery(this.connection, &quot;SET net_write_timeout=&quot;   </div><div class="line">                        + this.connection.getNetTimeoutForStreamingResults());  </div><div class="line">            &#125; finally &#123;  </div><div class="line">                if (stmt != null) &#123;  </div><div class="line">                    stmt.close();  </div><div class="line">                &#125;  </div><div class="line">            &#125;  </div><div class="line">        &#125;</div></pre></td></tr></table></figure>
<p>而 this.connection.getNetTimeoutForStreamingResults() 默认是600秒，或者在JDBC连接串种通过属性 netTimeoutForStreamingResults 来指定。</p>
<p>一般在数据导出场景中容易出现 net_write_timeout 这个错误，比如这个错误堆栈：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/8fe715d3ebb6929afecd19aadbe53e5e.png" alt=""></p>
<p>或者：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">ErrorMessage:</div><div class="line">Code:[DBUtilErrorCode-07], Description:[读取数据库数据失败. 请检查您的配置的 column/table/where/querySql或者向 DBA 寻求帮助.].  - 执行的SQL为:/*+TDDL(&#123;&apos;extra&apos;:&#123;&apos;MERGE_UNION&apos;:&apos;false&apos;&#125;,&apos;type&apos;:&apos;direct&apos;,&apos;vtab&apos;:&apos;C_CONS&apos;,&apos;dbid&apos;:&apos;EASDB_1514548807024CGYFEASDB_ROQH_0005_RDS&apos;,&apos;realtabs&apos;:[&apos;C_CONS&apos;]&#125;)*/select CONS_ID,CUST_ID,USERFLAG,CONS_NO,CONS_NAME,CUST_QUERY_NO,TMP_PAY_RELA_NO,ORGN_CONS_NO,CONS_SORT_CODE,ELEC_ADDR,TRADE_CODE,ELEC_TYPE_CODE,CONTRACT_CAP,RUN_CAP,SHIFT_NO,LODE_ATTR_CODE,VOLT_CODE,HEC_INDUSTRY_CODE,HOLIDAY,BUILD_DATE,PS_DATE,CANCEL_DATE,DUE_DATE,NOTIFY_MODE,SETTLE_MODE,STATUS_CODE,ORG_NO,RRIO_CODE,CHK_CYCLE,LAST_CHK_DATE,CHECKER_NO,POWEROFF_CODE,TRANSFER_CODE,MR_SECT_NO,NOTE_TYPE_CODE,TMP_FLAG,TMP_DATE,DATA_SRC,USER_EATTR,SHARD_NO,INSERT_TIME from C_CONS  具体错误信息为：Communications link failure</div><div class="line">The last packet successfully received from the server was 7 milliseconds ago.  The last packet sent successfully to the server was 709,806 milliseconds ago. - com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</div><div class="line">The last packet successfully received from the server was 7 milliseconds ago.  The last packet sent successfully to the server was 709,806 milliseconds ago.</div><div class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</div><div class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</div><div class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</div><div class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:377)</div><div class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1036)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3427)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3327)</div><div class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3814)</div><div class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:870)</div><div class="line">	at com.mysql.jdbc.MysqlIO.nextRow(MysqlIO.java:1928)</div><div class="line">	at com.mysql.jdbc.RowDataDynamic.nextRecord(RowDataDynamic.java:378)</div><div class="line">	at com.mysql.jdbc.RowDataDynamic.next(RowDataDynamic.java:358)</div><div class="line">	at com.mysql.jdbc.ResultSetImpl.next(ResultSetImpl.java:6337)</div><div class="line">	at com.alibaba.datax.plugin.rdbms.reader.CommonRdbmsReader$Task.startRead(CommonRdbmsReader.java:275)</div><div class="line">	at com.alibaba.datax.plugin.reader.drdsreader.DrdsReader$Task.startRead(DrdsReader.java:148)</div><div class="line">	at com.alibaba.datax.core.taskgroup.runner.ReaderRunner.run(ReaderRunner.java:62)</div><div class="line">	at java.lang.Thread.run(Thread.java:834)</div><div class="line">Caused by: java.io.EOFException: Can not read response from server. Expected to read 258 bytes, read 54 bytes before connection was unexpectedly lost.</div><div class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:2914)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3387)</div><div class="line">	... 11 more</div></pre></td></tr></table></figure>
<h2 id="一些其他的-Timeout"><a href="#一些其他的-Timeout" class="headerlink" title="一些其他的 Timeout"></a>一些其他的 Timeout</h2><p>connectTimeout：表示等待和MySQL数据库建立socket链接的超时时间，默认值0，表示不设置超时，单位毫秒，建议30000。 JDBC驱动连接属性</p>
<p>socketTimeout：表示客户端和MySQL数据库建立socket后，读写socket时的等待的超时时间，linux系统默认的socketTimeout为30分钟，可以不设置。 JDBC驱动连接属性</p>
<p>Statement Timeout：用来限制statement的执行时长，timeout的值通过调用JDBC的java.sql.Statement.setQueryTimeout(int timeout) API进行设置。不过现在开发者已经很少直接在代码中设置，而多是通过框架来进行设置。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_max_execution_time" target="_blank" rel="external"><code>max_execution_time</code></a>：The execution timeout for <a href="https://dev.mysql.com/doc/refman/5.7/en/select.html" target="_blank" rel="external"><code>SELECT</code></a> statements, in milliseconds. If the value is 0, timeouts are not enabled.  MySQL 属性，可以set修改，一般用来设置一个查询最长不超过多少秒，避免一个慢查询一直在跑，跟statement timeout对应。</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Command-Line Format</td>
<td><code>--max-execution-time=#</code></td>
</tr>
<tr>
<td>System Variable</td>
<td><code>max_execution_time</code></td>
</tr>
<tr>
<td>Scope</td>
<td>Global, Session</td>
</tr>
<tr>
<td>Dynamic</td>
<td>Yes</td>
</tr>
<tr>
<td>Type</td>
<td>Integer</td>
</tr>
<tr>
<td>Default Value</td>
<td><code>0</code></td>
</tr>
</tbody>
</table>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_wait_timeout" target="_blank" rel="external"><code>wait_timeout</code></a> The number of seconds the server waits for activity on a noninteractive connection before closing it. MySQL 属性，一般设置tcp keepalive后这个值基本不会超时。</p>
<p>On thread startup, the session <a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_wait_timeout" target="_blank" rel="external"><code>wait_timeout</code></a> value is initialized from the global <a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_wait_timeout" target="_blank" rel="external"><code>wait_timeout</code></a> value or from the global <a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_interactive_timeout" target="_blank" rel="external"><code>interactive_timeout</code></a> value, depending on the type of client (as defined by the <code>CLIENT_INTERACTIVE</code> connect option to <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-real-connect.html" target="_blank" rel="external"><code>mysql_real_connect()</code></a>). See also <a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_interactive_timeout" target="_blank" rel="external"><code>interactive_timeout</code></a>.</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Command-Line Format</td>
<td><code>--wait-timeout=#</code></td>
</tr>
<tr>
<td>System Variable</td>
<td><code>wait_timeout</code></td>
</tr>
<tr>
<td>Scope</td>
<td>Global, Session</td>
</tr>
<tr>
<td>Dynamic</td>
<td>Yes</td>
</tr>
<tr>
<td>Type</td>
<td>Integer</td>
</tr>
<tr>
<td>Default Value</td>
<td><code>28800</code></td>
</tr>
<tr>
<td>Minimum Value</td>
<td><code>1</code></td>
</tr>
<tr>
<td>Maximum Value (Other)</td>
<td><code>31536000</code></td>
</tr>
<tr>
<td>Maximum Value (Windows)</td>
<td><code>2147483</code></td>
</tr>
</tbody>
</table>
<p>一般来说应该设置： max_execution_time/statement timeout &lt; Tranction Timeout &lt; socketTimeout</p>
<h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><p>设置JDBC参数不合理（queryTimeout=10s，socketTimeout=10s），会导致在异常情况下，第二条get获得了第一条的结果，拿到了错误的数据，数据库则表现正常</p>
<p>socketTimeout触发后，连接抛CommunicationsException（严重异常，触发后连接应该断开）, 但JDBC会检查请求是否被cancle了，如果cancle就会抛出MySQLTimeoutException异常，这是一个普通异常，连接会被重新放回连接池重用（导致下一个获取这个连接的线程可能会得到前一个请求的response）。</p>
<p>queryTimeout（queryTimeoutKillsConnection=True–来强制关闭连接）会触发启动一个新的连接向server发送 kill id的命令，<strong>MySQL5.7增加了max_statement_time/max_execution_time来做到在server上直接检测到这种查询，然后结束掉</strong>。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.atatech.org/articles/122079" target="_blank" rel="external">https://www.atatech.org/articles/122079</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;MySQL-JDBC-StreamResult-和-net-write-timeout&quot;&gt;&lt;a href=&quot;#MySQL-JDBC-StreamResult-和-net-write-timeout&quot; class=&quot;headerlink&quot; title=&quot;MySQL 
    
    </summary>
    
      <category term="MySQL" scheme="http://yoursite.com/categories/MySQL/"/>
    
    
      <category term="MySQL" scheme="http://yoursite.com/tags/MySQL/"/>
    
      <category term="JDBC" scheme="http://yoursite.com/tags/JDBC/"/>
    
      <category term="stream" scheme="http://yoursite.com/tags/stream/"/>
    
      <category term="net_write_timeout" scheme="http://yoursite.com/tags/net-write-timeout/"/>
    
      <category term="timeout" scheme="http://yoursite.com/tags/timeout/"/>
    
  </entry>
  
  <entry>
    <title>如何创建一个自己连自己的TCP连接</title>
    <link href="http://yoursite.com/2020/07/01/%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%87%AA%E5%B7%B1%E8%BF%9E%E8%87%AA%E5%B7%B1%E7%9A%84TCP%E8%BF%9E%E6%8E%A5/"/>
    <id>http://yoursite.com/2020/07/01/如何创建一个自己连自己的TCP连接/</id>
    <published>2020-07-01T09:30:03.000Z</published>
    <updated>2020-07-16T06:07:34.961Z</updated>
    
    <content type="html"><![CDATA[<h1 id="如何创建一个自己连自己的TCP连接"><a href="#如何创建一个自己连自己的TCP连接" class="headerlink" title="如何创建一个自己连自己的TCP连接"></a>如何创建一个自己连自己的TCP连接</h1><blockquote>
<p>能不能建立一个tcp连接， src-ip:src-port 等于dest-ip:dest-port 呢？</p>
</blockquote>
<p>执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># nc 192.168.0.79 18082 -p 18082</div></pre></td></tr></table></figure>
<p>然后就能看到</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># netstat -ant |grep 18082</div><div class="line">tcp        0      0 192.168.0.79:18082      192.168.0.79:18082      ESTABLISHED</div></pre></td></tr></table></figure>
<p>比较神奇，这个连接的srcport等于destport，并且完全可以工作，也能收发数据。这有点颠覆大家的理解，端口能重复使用？</p>
<h2 id="port-range"><a href="#port-range" class="headerlink" title="port range"></a>port range</h2><p>我们都知道linux下本地端口范围由参数控制</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># cat /proc/sys/net/ipv4/ip_local_port_range </div><div class="line">10000	65535</div></pre></td></tr></table></figure>
<p>所以也经常看到一个误解：一台机器上最多能创建65535个TCP连接</p>
<h2 id="到底一台机器上最多能创建多少个TCP连接"><a href="#到底一台机器上最多能创建多少个TCP连接" class="headerlink" title="到底一台机器上最多能创建多少个TCP连接"></a>到底一台机器上最多能创建多少个TCP连接</h2><p>在内存、文件句柄足够的话可以创建的连接是没有限制的，那么/proc/sys/net/ipv4/ip_local_port_range指定的端口范围到底是什么意思呢？</p>
<p>一个TCP连接只要保证四元组(src-ip src-port dest-ip dest-port)唯一就可以了，而不是要求src port唯一，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># netstat -ant |grep 18089</div><div class="line">tcp        0      0 192.168.1.79:18089      192.168.1.79:22         ESTABLISHED</div><div class="line">tcp        0      0 192.168.1.79:18089      192.168.1.79:18080      ESTABLISHED</div><div class="line">tcp        0      0 192.168.0.79:18089      192.168.0.79:22         TIME_WAIT </div><div class="line">tcp        0      0 192.168.1.79:22         192.168.1.79:18089      ESTABLISHED</div><div class="line">tcp        0      0 192.168.1.79:18080      192.168.1.79:18089      ESTABLISHED</div></pre></td></tr></table></figure>
<p>从前三行可以清楚地看到18089被用了三次，第一第二行src-ip、dest-ip也是重复的，但是dest port不一样，第三行的src-port还是18089，但是src-ip变了。</p>
<p>所以一台机器能创建的TCP连接是没有限制的，而ip_local_port_range是指没有bind的时候OS随机分配端口的范围，但是分配到的端口要同时满足五元组唯一，这样 ip_local_port_range 限制的是连同一个目标（dest-ip和dest-port一样）的port的数量（请忽略本地多网卡的情况，因为dest-ip为以后route只会选用一个本地ip）。</p>
<h2 id="自己连自己的连接"><a href="#自己连自己的连接" class="headerlink" title="自己连自己的连接"></a>自己连自己的连接</h2><p>我们来看自己连自己发生了什么</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># strace nc 192.168.0.79 18084 -p 18084</span></div><div class="line">execve(<span class="string">"/usr/bin/nc"</span>, [<span class="string">"nc"</span>, <span class="string">"192.168.0.79"</span>, <span class="string">"18084"</span>, <span class="string">"-p"</span>, <span class="string">"18084"</span>], [/* 31 vars */]) = 0</div><div class="line">brk(NULL)                               = 0x23d4000</div><div class="line">mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f213f394000</div><div class="line">access(<span class="string">"/etc/ld.so.preload"</span>, R_OK)      = -1 ENOENT (No such file or directory)</div><div class="line">open(<span class="string">"/etc/ld.so.cache"</span>, O_RDONLY|O_CLOEXEC) = 3</div><div class="line">fstat(3, &#123;st_mode=S_IFREG|0644, st_size=23295, ...&#125;) = 0</div><div class="line">mmap(NULL, 23295, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f213f38e000</div><div class="line">close(3)                                = 0</div><div class="line">open(<span class="string">"/lib64/libssl.so.10"</span>, O_RDONLY|O_CLOEXEC) = 3</div><div class="line">………………</div><div class="line">munmap(0x7f213f393000, 4096)            = 0</div><div class="line">open(<span class="string">"/usr/share/ncat/ca-bundle.crt"</span>, O_RDONLY) = -1 ENOENT (No such file or directory)</div><div class="line">socket(AF_INET, SOCK_STREAM, IPPROTO_TCP) = 3</div><div class="line">fcntl(3, F_GETFL)                       = 0x2 (flags O_RDWR)</div><div class="line">fcntl(3, F_SETFL, O_RDWR|O_NONBLOCK)    = 0</div><div class="line">setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0</div><div class="line"><span class="built_in">bind</span>(3, &#123;sa_family=AF_INET, sin_port=htons(18084), sin_addr=inet_addr(<span class="string">"0.0.0.0"</span>)&#125;, 16) = 0</div><div class="line">//注意这里<span class="built_in">bind</span>后直接就是connect，没有listen</div><div class="line">connect(3, &#123;sa_family=AF_INET, sin_port=htons(18084), sin_addr=inet_addr(<span class="string">"192.168.0.79"</span>)&#125;, 16) = -1 EINPROGRESS (Operation now <span class="keyword">in</span> progress)</div><div class="line">select(4, [3], [3], [3], &#123;10, 0&#125;)       = 1 (out [3], left &#123;9, 999998&#125;)</div><div class="line">getsockopt(3, SOL_SOCKET, SO_ERROR, [0], [4]) = 0</div><div class="line">select(4, [0 3], [], [], NULL</div></pre></td></tr></table></figure>
<p>抓包看看，正常三次握手，但是syn的seq和syn+ack的seq是一样的</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/341f2891253baa4eebdaeaf34aa60c4b.png" alt="image.png"></p>
<p>这里算是常说的TCP simultaneous open，simultaneous open指的是两个不同port同时发syn建连接。而这里是先创建了一个socket，然后socket bind到18084端口上（作为local port，因为nc指定了local port），然后执行 connect, 连接到的目标也是192.168.0.79:18084，而这个目标正好是刚刚创建的socket，也就是自己连自己（连接双方总共只有一个socket）。因为一个socket充当了两个角色（client、server），这里发syn，自己收到自己发的syn，就相当于两个角色simultaneous open了。</p>
<p>正常一个连接一定需要两个socket参与（这两个socket不一定要在两台机器上），而这个连接只用了一个socket就创建了，还能正常传输数据。但是仔细观察发数据的时候发放的seq增加（注意tcp_len 11那里的seq），收方的seq也增加了11，这是因为本来这就是用的同一个socket。正常两个socket通讯不是这样的。</p>
<p>那么这种情况为什么没有当做bug被处理呢？</p>
<h2 id="TCP-simultanous-open"><a href="#TCP-simultanous-open" class="headerlink" title="TCP simultanous open"></a>TCP simultanous open</h2><p>在tcp连接的定义中，通常都是一方先发起连接，假如两边同时发起连接，也就是两个socket同时给对方发 syn 呢？ 这在内核中是支持的，就叫同时打开（simultaneous open）。</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/b9a0144a3835759c844f697bc45103fa.png" alt="image.png"></p>
<p>​                                                                       摘自《tcp/ip卷1》</p>
<p>可以清楚地看到这个连接建立用了四次握手，然后连接建立了，当然也有 simultanous close(3次挥手成功关闭连接)。如下 net/ipv4/tcp_input.c 的5924行中就说明了允许这种自己连自己的连接（当然也允许simultanous open). 也就是允许一个socket本来应该收到 syn+ack,结果收到了syn的情况，而一个socket自己连自己又是这种情况的特例。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">	static int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,</div><div class="line">                     const struct tcphdr *th)</div><div class="line">	&#123;</div><div class="line">5916         /* PAWS check. */</div><div class="line">5917         if (tp-&gt;rx_opt.ts_recent_stamp &amp;&amp; tp-&gt;rx_opt.saw_tstamp &amp;&amp;</div><div class="line">5918             tcp_paws_reject(&amp;tp-&gt;rx_opt, 0))</div><div class="line">5919                 goto discard_and_undo;</div><div class="line">5920         //在socket发送syn后收到了一个syn(正常应该收到syn+ack),这里是允许的。</div><div class="line">5921         if (th-&gt;syn) &#123;</div><div class="line">5922                 /* We see SYN without ACK. It is attempt of</div><div class="line">5923                  * simultaneous connect with crossed SYNs.</div><div class="line">5924                  * Particularly, it can be connect to self.  //自己连自己</div><div class="line">5925                  */</div><div class="line">5926                 tcp_set_state(sk, TCP_SYN_RECV);</div><div class="line">5927 </div><div class="line">5928                 if (tp-&gt;rx_opt.saw_tstamp) &#123;</div><div class="line">5929                         tp-&gt;rx_opt.tstamp_ok = 1;</div><div class="line">5930                         tcp_store_ts_recent(tp);</div><div class="line">5931                         tp-&gt;tcp_header_len =</div><div class="line">5932                                 sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED;</div><div class="line">5933                 &#125; else &#123;</div><div class="line">5934                         tp-&gt;tcp_header_len = sizeof(struct tcphdr);</div><div class="line">5935                 &#125;</div><div class="line">5936 </div><div class="line">5937                 tp-&gt;rcv_nxt = TCP_SKB_CB(skb)-&gt;seq + 1;</div><div class="line">5938                 tp-&gt;copied_seq = tp-&gt;rcv_nxt;</div><div class="line">5939                 tp-&gt;rcv_wup = TCP_SKB_CB(skb)-&gt;seq + 1;</div><div class="line">5940 </div><div class="line">5941                 /* RFC1323: The window in SYN &amp; SYN/ACK segments is</div><div class="line">5942                  * never scaled.</div><div class="line">5943                  */</div></pre></td></tr></table></figure>
<p>也就是在发送syn进入SYN_SENT状态之后，收到对端发来的syn包后不会RST，而是处理流程如下，调用tcp_set_state(sk, TCP_SYN_RECV)进入SYN_RECV状态，以及调用tcp_send_synack(sk)向对端发送syn+ack。</p>
<h2 id="自己连自己的原理解释"><a href="#自己连自己的原理解释" class="headerlink" title="自己连自己的原理解释"></a>自己连自己的原理解释</h2><p>第一我们要理解Kernel是支持simultaneous open的，也就是说socket发走syn后，本来应该收到一个syn+ack的，但是实际收到了一个syn（没有ack），这是允许的。这叫TCP连接同时打开（同时给对方发syn），四次握手然后建立连接成功。</p>
<p>自己连自己又是simultaneous open的一个特例，特别在这个连接只有一个socket参与，发送、接收都是同一个socket，自然也会是发syn后收到了自己的syn（自己发给自己），然后依照simultaneous open连接也能创建成功。</p>
<h2 id="bind-和-connect、listen"><a href="#bind-和-connect、listen" class="headerlink" title="bind 和 connect、listen"></a>bind 和 connect、listen</h2><p>当对一个TCP socket调用connect函数时，如果这个socket没有bind指定的端口号，操作系统会为它选择一个当前未被使用的端口号，这个端口号被称为ephemeral port, 范围可以在/proc/sys/net/ipv4/ip_local_port_range里查看。假设30000这个端口被选为ephemeral port。</p>
<p>如果这个socket指定了local port那么socket创建后会执行bind将这个socket bind到这个port。比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">socket(AF_INET, SOCK_STREAM, IPPROTO_TCP) = 3</div><div class="line">fcntl(3, F_GETFL)                       = 0x2 (flags O_RDWR)</div><div class="line">fcntl(3, F_SETFL, O_RDWR|O_NONBLOCK)    = 0</div><div class="line">setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0</div><div class="line">bind(3, &#123;sa_family=AF_INET, sin_port=htons(18084), sin_addr=inet_addr(&quot;0.0.0.0&quot;)&#125;, 16) = 0</div></pre></td></tr></table></figure>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/5373ecfe0d4496d106c64d3f370c893c.png" alt="image.png"></p>
<p>然后这个bind到18084 local port的socket又要连接到 18084 port上，而这个18084socket已经bind到了socket（也就是自己），就形成了两个socket 的simultaneous open一样，内核又允许这种simultaneous open，所以就形成了自己连自己，也就是一个socket在自己给自己收发数据，所以看到收方和发放的seq是一样的。</p>
<p>可以用python来重现这个连接连自己的过程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">import socket</div><div class="line">import time</div><div class="line"></div><div class="line">connected=False</div><div class="line">while (not connected):</div><div class="line">        try:</div><div class="line">                sock = socket.socket(socket.AF_INET,socket.SOCK_STREAM)</div><div class="line">                sock.setsockopt(socket.IPPROTO_TCP,socket.TCP_NODELAY,1)</div><div class="line">				sock.bind((&apos;&apos;, 18084))               //sock 先bind到18084</div><div class="line">                sock.connect((&apos;127.0.0.1&apos;,18084))    //然后同一个socket连自己</div><div class="line">                connected=True</div><div class="line">        except socket.error,(value,message):</div><div class="line">                print message</div><div class="line"></div><div class="line">        if not connected:</div><div class="line">                print &quot;reconnect&quot;</div><div class="line">               </div><div class="line">print &quot;tcp self connection occurs!&quot;</div><div class="line">print &quot;netstat -an|grep 18084&quot;</div><div class="line">time.sleep(1800)</div></pre></td></tr></table></figure>
<p>这里connect前如果没有bind那么系统就会从 local port range 分配一个可用port。</p>
<p>bind成功后会将ip+port放入hash表来判重，这就是我们常看到的 Bind to <em>*</em> failed (IOD #1): Address already in use 异常。所以一台机器上，如果有多个ip，是可以将同一个port bind多次的，但是bind的时候如果不指定ip，也就是bind(‘0’, port) 还是会冲突。</p>
<p>connect成功后会将四元组放入ehash来判定连接的重复性。如果connect四元组冲突了就会报如下错误</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># nc 192.168.0.82 8080 -p 29798 -s 192.168.0.79</div><div class="line">Ncat: Cannot assign requested address.</div></pre></td></tr></table></figure>
<h3 id="listen"><a href="#listen" class="headerlink" title="listen"></a>listen</h3><p><img src="D:%5Cali%5Ccase%5Cimage%5Cimage-20200702131215819.png" alt="image-20200702131215819"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://segmentfault.com/a/1190000002396411" target="_blank" rel="external">https://segmentfault.com/a/1190000002396411</a></p>
<p><a href="https://blog.csdn.net/a364572/article/details/40628171" target="_blank" rel="external">linux中TCP的socket、bind、listen、connect和accept的实现</a></p>
<p><a href="https://ops.tips/blog/how-linux-tcp-introspection/" target="_blank" rel="external">How Linux allows TCP introspection The inner workings of bind and listen on Linux.</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;如何创建一个自己连自己的TCP连接&quot;&gt;&lt;a href=&quot;#如何创建一个自己连自己的TCP连接&quot; class=&quot;headerlink&quot; title=&quot;如何创建一个自己连自己的TCP连接&quot;&gt;&lt;/a&gt;如何创建一个自己连自己的TCP连接&lt;/h1&gt;&lt;blockquote&gt;

    
    </summary>
    
      <category term="TCP" scheme="http://yoursite.com/categories/TCP/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="network" scheme="http://yoursite.com/tags/network/"/>
    
      <category term="TCP" scheme="http://yoursite.com/tags/TCP/"/>
    
      <category term="simultaneous" scheme="http://yoursite.com/tags/simultaneous/"/>
    
      <category term="自连接" scheme="http://yoursite.com/tags/%E8%87%AA%E8%BF%9E%E6%8E%A5/"/>
    
  </entry>
  
  <entry>
    <title>TCPRT 案例</title>
    <link href="http://yoursite.com/2020/06/24/TCPRT%20%E6%A1%88%E4%BE%8B/"/>
    <id>http://yoursite.com/2020/06/24/TCPRT 案例/</id>
    <published>2020-06-24T09:30:03.000Z</published>
    <updated>2020-08-26T10:57:01.319Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TCPRT-案例"><a href="#TCPRT-案例" class="headerlink" title="TCPRT 案例"></a>TCPRT 案例</h1><p>在分布式、微服务场景下如果rt出现问题一般都有pinpoint、Dapper、鹰眼之类的工具能帮忙分析是哪个服务慢了，但是如果是网络慢了这些工具就没有办法区分了。</p>
<p>TCPRT是从网络层面把整个服务的响应时间分成：网络传输时间+服务处理时间，这样一旦响应时间慢了很容易看到是网络慢了、丢包、抖动了，还是就是服务处理慢了。</p>
<p>如下案例都有tcpdump抓包来证明TCPRT的正确性，实际上在没有TCPRT之前处理类似问题只能抓包+等待重现，来排查问题。开启TCPRT之后会实时记录每一个请求响应的网络传输时间、服务处理时间、网络丢包率等各种数据，而对性能的影响非常非常小，控制在1%以内。</p>
<p>总的来说TCPRT区分了网络传输时间和服务处理时间，并记录了丢包、重传等各种数据，从此天下无包可抓。</p>
<p>可以在任何节点上部署tcprt，下面案例都只是在tomcat节点上部署了tcprt服务，来监控client到tomcat以及tomcat到后面RDS的响应时间：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/6f6862dec810933f34b7793018cfb0da.png" alt="image.png"></p>
<h2 id="图形化监控数据"><a href="#图形化监控数据" class="headerlink" title="图形化监控数据"></a>图形化监控数据</h2><p>捞取同一段时间内同样压力下tcprt监控数据后展示出来的图形：</p>
<ol>
<li>最开始正常网络情况下QPS 40K，总RT 4.1ms（含网络），业务处理时间3.9ms，两者差就是网络传输时间；</li>
<li>突然网络变差，rtt从0.1ms降到了17ms，相应地QPS降到了8200，总RT 19.9ms（含网络），业务处理时间2.8ms（因为压力变小了，实际业务处理时间也快了）；</li>
<li>接着rtt回到7ms，QPS回升到16400，总RT 9.9ms（含网络），业务处理时间2.82ms；</li>
<li>rtt恢复到0.1ms，同时client到tomcat丢包率为0.1%，QPS 39.5K，总RT 4.1ms（含网络），业务处理时间3.7ms. 对QPS的影响基本可以忽略；</li>
<li>client到tomcat丢包率为1%，QPS 31.8K，总RT 5.08ms（含网络），业务处理时间2.87ms；</li>
<li>tomcat到client和RDS丢包率都为1%，QPS 23.2K，总RT 7.03ms（含网络），业务处理时间4.88ms，tomcat调用RDS的rt也从2.6ms上升到了4.6ms。</li>
</ol>
<p>QPS随着上述6个阶段的变化图（以下所有图形都是在同一时间段所取得）：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/166e466bb43215556ccdf73e8f1476e3.png" alt="image.png"></p>
<p>响应时间和丢包率在上述6个阶段的变化图：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/865355ab81bc4c1804ff26c2ab2ecf23.png" alt="image.png"></p>
<p>应用到tomcat之间的网络消耗展示（逻辑响应时间-逻辑服务时间=网络传输时间）：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/7b755c9d2f19701d1409abfd91dce4c1.png" alt="image.png"></p>
<p>tomcat到RDS之间的时间消耗：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/f18f5c467cc21ead4c15a5e28bf3b8fd.png" alt="image.png"></p>
<p>物理响应时间（RDS响应时间）上升部分是因为RDS的丢包率为1%，tomcat的TCPRT监控上能看到的只是RDS总的响应时间上升了。</p>
<p>通过wireshark看到对应的tomcat和RDS之间的RTT数据：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/949ab8a34c9bece160e8fcff8e76a2d4.png" alt="image.png"></p>
<p>上面都是对所有数据做平均后分析所得，接下来会通过具体的一个请求来分析，同时通过tcpdump抓包来分析对应的数据（对比验证tcprt的可靠性）</p>
<h2 id="深圳盒子慢查询定位"><a href="#深圳盒子慢查询定位" class="headerlink" title="深圳盒子慢查询定位"></a>深圳盒子慢查询定位</h2><p>这个实例没有上线manager，只能人肉分析tcprt日志，先看慢查询日志： </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$tail -10 slow.log</div><div class="line">2020-08-13 14:39:46.289 - [user=zhoubingbing,host=172.30.52.43,port=54754,schema=goodarights_base]  [TDDL] select * from tbl_wm_mcht_invoice_record where mcht_no=&apos;323967&apos; ORDER BY CREATE_TIME desc;#1070#392#111413ba1b1a2000, tddl version: 5.4.3-15844389</div></pre></td></tr></table></figure>
<p> 慢查询对应的tcprt记录，rtt高达35ms（正常都是2ms，而且重传了3次才把response传完），所以最终SQL执行花了差不多1.3秒（慢查询中有记录，1秒多，查询结果390行数据）：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/15731f74f91099774996302da1571a51.png" alt="image.png"></p>
<p>从60秒汇总统计日只看，慢查询发生的时间点 丢包率确实到了 0.1%：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/85a75f19422a838f64ec6dffb4a3837f.png" alt="image.png"></p>
<p>总的平均时间统计，绿框是DRDS处理时间，红框是DRDS处理时间+网络传输时间，这个gap有点大（网络消耗比较高）：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/7c45a9ab360986c7e743de9a687c9549.png" alt="image.png"></p>
<h2 id="丢包造成rt过高"><a href="#丢包造成rt过高" class="headerlink" title="丢包造成rt过高"></a>丢包造成rt过高</h2><p>如下数据中每一行代表一个请求、响应数据，第三行和第八行总rt都比较高，超过200ms（第6列），但是server处理时间在3ms以内（最后一列）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">                                                         总rt    重传       处理时间</div><div class="line">1593677820 311182 10.0.186.70:32610 10.0.186.97:3306 142 5885 113 0 36314730 2758</div><div class="line">1593677820 317068 10.0.186.70:32610 10.0.186.97:3306 142 214047 113 1 36314731 2791 &lt;?</div><div class="line">1593677820 533814 10.0.186.70:32610 10.0.186.97:3306 142 5905 113 0 36314732 2771</div><div class="line">1593677820 539722 10.0.186.70:32610 10.0.186.97:3306 142 5842 113 0 36314733 2714</div><div class="line">1593677820 545565 10.0.186.70:32610 10.0.186.97:3306 142 5898 113 0 36314734 2751</div><div class="line">1593677820 551464 10.0.186.70:32610 10.0.186.97:3306 142 5866 113 0 36314735 2746</div><div class="line">1593677820 557331 10.0.186.70:32610 10.0.186.97:3306 142 213762 113 1 36314736 2627 &lt;?</div><div class="line">1593677820 772815 10.0.186.70:32610 10.0.186.97:3306 142 5845 113 0 36314737 2731</div><div class="line">1593677820 778677 10.0.186.70:32610 10.0.186.97:3306 142 5868 113 0 36314738 2744</div></pre></td></tr></table></figure>
<p>对应这个时间点的抓包，可以看到这两个慢的查询都是因为第一个response发给client后没有收到ack（网络丢包？网络延时高？） 200ms后再发一次response就收到ack。所以总的rt都超过了200ms，其它没丢包的rt都很快。另外抓包的请求和和上面tcprt记录都是全对应的，时间精度都在微秒级。</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/63da5e221c8d2d26bc25f9e50ef35779.png" alt="image.png"></p>
<p>丢包重传的分析</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/88849848f07ca74e000db971e3246def.png" alt="image.png"></p>
<h2 id="丢包率对性能影响"><a href="#丢包率对性能影响" class="headerlink" title="丢包率对性能影响"></a>丢包率对性能影响</h2><p>在client端设置：</p>
<blockquote>
<p>tc qdisc add dev eth0 root netem loss 3% delay 3ms</p>
</blockquote>
<p>这时可以看到tcprt stats统计到的网络丢包率基本在3%左右，第6列是丢包率千分数，可以看到tcprt吐出来的丢包率和我们用tc构造的基本一致：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">$cat tcp-rt/rt-network-stats | awk &apos;&#123; if($6!=0) print $0 &#125;&apos;</div><div class="line">							 丢包率	</div><div class="line">1593677850 all 3306 10749 3217 23 915 0 141 0 45 882895</div><div class="line">1593677911 all 3306 11938 2789 28 1102 0 141 0 45 795169</div><div class="line">1593677972 all 3306 12001 2798 29 1104 0 141 0 45 789972</div><div class="line">1593678034 all 3306 11919 2778 28 1103 0 141 0 45 796126</div><div class="line">1593678095 all 3306 12049 2832 29 1714 0 141 0 45 786661</div><div class="line">1593678157 all 3306 11893 2773 28 3000 0 141 0 45 797950</div><div class="line">1593678218 all 3306 11956 2769 29 3000 0 141 0 45 793544</div><div class="line">1593678280 all 3306 12032 2800 29 3000 0 141 0 45 789403</div><div class="line">1593678341 all 3306 11973 2761 29 3000 0 141 0 45 791959</div><div class="line">1593678403 all 3306 7733 3376 13 1442 0 141 0 45 1237712</div></pre></td></tr></table></figure>
<h2 id="时延增加对数据的影响"><a href="#时延增加对数据的影响" class="headerlink" title="时延增加对数据的影响"></a>时延增加对数据的影响</h2><p>在Server上增加了网络17ms delay，可以看到平均rt增加了34ms，因为Server到P4192增加17，到client增加17，所以总共增加了34ms。另外QPS（最后一列）从200万/60秒降到了26万/60秒</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">$sudo tc qdisc add dev eth0 root netem  delay 17ms</div><div class="line">$tail -40 tcp-rt/rt-network-stats</div><div class="line">1593684362 all 3306 4860 4702 0 97 0 141 0 45 2022008</div><div class="line">1593684362 all P4192 2596 2597 0 2359 0 139 0 139 2022289</div><div class="line">1593684424 all 3306 4652 4493 0 97 0 141 0 45 2112538</div><div class="line">1593684424 all P4192 2596 2597 0 2357 0 139 0 139 2112834</div><div class="line">1593684485 all 3306 4858 4699 0 97 0 141 0 45 2023052</div><div class="line">1593684485 all P4192 2598 2599 0 2358 0 139 0 139 2023316</div><div class="line">---------------这里增加网络 rt 17ms---------</div><div class="line">1593684547 all 3306 7188 4395 0 97 0 141 0 45 1367174</div><div class="line">1593684547 all P4192 2593 2593 0 2355 0 139 0 139 1367466</div><div class="line">1593684608 all 3306 19969 2838 0 97 0 141 0 45 492234</div><div class="line">1593684608 all P4192 2592 2592 0 2357 0 139 0 139 492587</div><div class="line">1593684669 all 3306 19935 2800 0 97 0 141 0 45 493045</div><div class="line">1593684669 all P4192 2595 2595 0 2354 0 139 0 139 493383</div><div class="line">1593684731 all 3306 20020 2885 0 97 0 141 0 45 491010</div><div class="line">1593684731 all P4192 2674 2674 0 2362 0 139 0 139 491305</div><div class="line">1593684792 all 3306 10362 4319 0 114 0 141 0 45 948355</div><div class="line">1593684792 all P4192 3379 3379 0 2356 0 139 0 139 948710</div><div class="line">1593684854 all 3306 37006 19866 0 317 0 141 0 45 265711</div><div class="line">1593684854 all P4192 19610 19610 0 2359 0 139 0 139 265963</div><div class="line">1593684915 all 3306 37088 19947 0 317 0 141 0 45 264957</div><div class="line">1593684915 all P4192 19618 19617 0 2360 0 139 0 139 265226</div><div class="line">1593684977 all 3306 37008 19869 0 317 0 141 0 45 265688</div><div class="line">1593684977 all P4192 19625 19625 0 2356 0 139 0 139 265962</div><div class="line">1593685038 all 3306 36968 19830 0 317 0 141 0 45 265864</div><div class="line">1593685038 all P4192 19618 19618 0 2360 0 139 0 139 266145</div><div class="line">1593685099 all 3306 37308 20166 0 317 0 141 0 45 263501</div><div class="line">1593685099 all P4192 19869 19869 0 2361 0 139 0 139 263799</div></pre></td></tr></table></figure>
<h2 id="delay-ack-client卡顿导致server-rt偏高"><a href="#delay-ack-client卡顿导致server-rt偏高" class="headerlink" title="delay ack + client卡顿导致server rt偏高"></a>delay ack + client卡顿导致server rt偏高</h2><p>如下tcprt日志中，问题行显示执行SQL花了14357微秒（最后一列），到结果发送给client并收到client ack花了32811微秒（第6列）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">                                                         总RT             业务处理时间</div><div class="line">1592470141 263882 10.0.186.79:31910 10.0.186.97:3306 142 32811 98 0 5361894 14357 &lt;&lt; ?</div><div class="line">1592470141 296695 10.0.186.79:31910 10.0.186.97:3306 142 13530 98 0 5361895 13389</div><div class="line">1592470141 310226 10.0.186.79:31910 10.0.186.97:3306 142 13419 98 0 5361896 13253</div><div class="line">1592470141 266860 10.0.186.79:32078 10.0.186.97:3306 142 5163 100 0 5470897 4953</div><div class="line">1592470141 272024 10.0.186.79:32078 10.0.186.97:3306 142 24644 100 0 5470898 6024 &lt;&lt; ?</div><div class="line">1592470141 296669 10.0.186.79:32078 10.0.186.97:3306 142 10371 100 0 5470899 10249</div><div class="line">1592470141 307041 10.0.186.79:32078 10.0.186.97:3306 142 11951 100 0 5470900 11766</div></pre></td></tr></table></figure>
<h3 id="Delay-ACK-原理解析"><a href="#Delay-ACK-原理解析" class="headerlink" title="Delay ACK 原理解析"></a>Delay ACK 原理解析</h3><p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/5265f9caf013baf662e400cd99ef2663.png" alt="image.png"></p>
<h3 id="从tcpdump抓包来分析原因"><a href="#从tcpdump抓包来分析原因" class="headerlink" title="从tcpdump抓包来分析原因"></a>从tcpdump抓包来分析原因</h3><p>从实际抓包来看，32811等于：14358+18454 （红框+绿框），因为server只有收到ack后才会认为这个SQL执行完毕，但是可能由于delay ack导致client发下一个请求才将ack带回server，而client明显此时卡住了，发请求慢</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/aec0d182702772384748a4d31cc6e795.png" alt="image.png"></p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/3d8c092cfee6686708973760b710cd1a.png" alt="image.png"></p>
<p>同一时间点一批SQL都是ack慢了，跑了几个小时才抓到这么一点点ack慢导致SQL总rt偏高，统计曲线被平均掉了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">$grep &quot; R &quot; tcp-rt/rt-network-log | awk &apos;&#123; if(($8-$12)&gt;10000) print $0 &#125;&apos; | sort -k3n,4n</div><div class="line">1592470141 263882 10.0.186.79:31910 10.0.186.97:3306 142 32811 98 0 5361894 14357 0 45 </div><div class="line">1592470141 270617 10.0.186.79:32144 10.0.186.97:3306 142 25933 98 0 5339208 6317 0 45</div><div class="line">1592470141 270675 10.0.186.79:32056 10.0.186.97:3306 142 26154 103 0 5511828 15901 0 45</div><div class="line">1592470141 270687 10.0.186.79:31828 10.0.186.97:3306 142 25939 98 0 5399141 6850 0 45</div><div class="line">1592470141 270701 10.0.186.79:32106 10.0.186.97:3306 142 25990 95 0 5432836 7551 0 45</div><div class="line">1592470141 270728 10.0.186.79:32080 10.0.186.97:3306 142 25888 98 0 5303145 6811 0 45</div><div class="line">1592470141 270732 10.0.186.79:31966 10.0.186.97:3306 142 26101 98 0 5529183 15906 0 45 </div><div class="line">1592470141 270781 10.0.186.79:31942 10.0.186.97:3306 142 26039 98 0 5478295 15603 0 45 </div><div class="line">1592470141 270786 10.0.186.79:32136 10.0.186.97:3306 142 25826 94 0 5375531 6743 0 45</div><div class="line">1592470141 270803 10.0.186.79:32008 10.0.186.97:3306 142 25884 102 0 5411914 7446 0 45 </div><div class="line">1592470141 270815 10.0.186.79:31868 10.0.186.97:3306 142 25793 98 0 5378825 6711 0 45</div><div class="line">1592470141 270856 10.0.186.79:31836 10.0.186.97:3306 142 25762 97 0 5324126 6677 0 45</div><div class="line">1592470141 270889 10.0.186.79:32048 10.0.186.97:3306 142 25934 102 0 5462899 15559 0 45</div><div class="line">1592470141 270911 10.0.186.79:32024 10.0.186.97:3306 142 25901 96 0 5441077 15340 0 45 </div><div class="line">1592470141 270915 10.0.186.79:32036 10.0.186.97:3306 142 25901 98 0 5455513 15399 0 45 </div><div class="line">1592470141 270918 10.0.186.79:31892 10.0.186.97:3306 142 25713 96 0 5312555 6624 0 45</div><div class="line">1592470141 270921 10.0.186.79:32058 10.0.186.97:3306 142 25905 98 0 5499044 15596 0 45 </div><div class="line">1592470141 270927 10.0.186.79:31962 10.0.186.97:3306 142 25758 97 0 5417673 7319 0 45</div><div class="line">1592470141 271894 10.0.186.79:31922 10.0.186.97:3306 142 24768 98 0 5523611 6160 0 45</div><div class="line">1592470141 271897 10.0.186.79:32026 10.0.186.97:3306 142 24762 98 0 5390495 6148 0 45</div><div class="line">1592470141 271904 10.0.186.79:31850 10.0.186.97:3306 142 24761 98 0 5505279 6147 0 45</div><div class="line">1592470141 272024 10.0.186.79:32078 10.0.186.97:3306 142 24644 100 0 5470898 6024 0 45</div></pre></td></tr></table></figure>
<h3 id="原因总结"><a href="#原因总结" class="headerlink" title="原因总结"></a>原因总结</h3><p><strong>这个时间点发送压力的sysbench卡顿了20ms左右，导致一批请求发送慢，同时这个时候因为delay ack，server收到的ack慢了。</strong></p>
<h2 id="最后一个数据发送慢导致计时显示网络rt偏高"><a href="#最后一个数据发送慢导致计时显示网络rt偏高" class="headerlink" title="最后一个数据发送慢导致计时显示网络rt偏高"></a>最后一个数据发送慢导致计时显示网络rt偏高</h2><p>server 收到请求到查到结果后开始发送成为server-rt，从开始发送结果到client ack所有结果成为总rt（server-rt+网络传输时间）</p>
<p>如果 limit 164567, 1 的时候只有一个response，得到所有分片都返回来才开始发数据给client；但是当 limit 164567,5 的时候因为这个SQL下推到多个分片，第一个response很快发出来，但是最后一个response需要等很久。在tcprt眼中，这里传输花了很久（一旦开始response，所有时间都是传输时间），但实际这里response卡顿了。</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/07d3ca12864a69b622a6f69b933d9a82.png" alt=""></p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/78d2cc459976ef868fa85359b33bda5a.png" alt="image.png"></p>
<p>从 jstack 抓的堆栈看到返回最后一个包之前，DRDS 在做 result.close()，因为关闭流模式结果集太耗时所以卡了。可以做的一个优化是提前发出 sendPacketEnd，然后异步去做物理连接的 result.close() 和 conn.close() 。</p>
<p>这是因为close的时候要挨个把所有记录处理掉，但是这里还在挨个做对象转换并抛弃。</p>
<p>这可以算是通过tcprt发现了程序的bug或者说可以优化的地方。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/37112986" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/37112986</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;TCPRT-案例&quot;&gt;&lt;a href=&quot;#TCPRT-案例&quot; class=&quot;headerlink&quot; title=&quot;TCPRT 案例&quot;&gt;&lt;/a&gt;TCPRT 案例&lt;/h1&gt;&lt;p&gt;在分布式、微服务场景下如果rt出现问题一般都有pinpoint、Dapper、鹰眼之类的工具
    
    </summary>
    
      <category term="network" scheme="http://yoursite.com/categories/network/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="network" scheme="http://yoursite.com/tags/network/"/>
    
      <category term="Performance" scheme="http://yoursite.com/tags/Performance/"/>
    
      <category term="TCP" scheme="http://yoursite.com/tags/TCP/"/>
    
      <category term="tcprt" scheme="http://yoursite.com/tags/tcprt/"/>
    
  </entry>
  
  <entry>
    <title>MySQL线程池导致的延时卡顿排查</title>
    <link href="http://yoursite.com/2020/06/05/MySQL%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AF%BC%E8%87%B4%E7%9A%84%E5%BB%B6%E6%97%B6%E5%8D%A1%E9%A1%BF%E6%8E%92%E6%9F%A5/"/>
    <id>http://yoursite.com/2020/06/05/MySQL线程池导致的延时卡顿排查/</id>
    <published>2020-06-05T09:30:03.000Z</published>
    <updated>2020-07-16T06:07:34.853Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MySQL-线程池导致的延时卡顿排查"><a href="#MySQL-线程池导致的延时卡顿排查" class="headerlink" title="MySQL 线程池导致的延时卡顿排查"></a>MySQL 线程池导致的延时卡顿排查</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>简单小表的主键点查SQL，单条执行很快，但是放在业务端，有时快有时慢，取了一条慢sql，在MySQL侧查看，执行时间很短。</p>
<p>通过监控有显示逻辑慢SQL和物理SQL ，取一slow.log里显示有12秒执行时间的SQL，但是这次12秒的执行在MySQL上记录下来的执行时间都不到1ms。</p>
<p>所在节点的tsar监控没有异常，Tomcat manager监控上没有fgc，Tomcat实例规格 16C32g<em>8, MySQL  32c128g  </em>32 。</p>
<p>5-28号现象复现，从监控图上CPU、内存、网络都没发现异常，MySQL侧查到的SQL依然执行很快，Tomcat侧记录12S执行时间，当时Tomcat节点的网络流量、CPU压力都很小。</p>
<p>所以客户怀疑Tomcat有问题或者Tomcat上的代码写得有问题导致了这个问题，需要排查和解决掉。</p>
<h2 id="Tomcat上抓包分析"><a href="#Tomcat上抓包分析" class="headerlink" title="Tomcat上抓包分析"></a>Tomcat上抓包分析</h2><h3 id="慢的连接"><a href="#慢的连接" class="headerlink" title="慢的连接"></a>慢的连接</h3><p>经过抓包分析发现在慢的连接上，所有操作都很慢，包括set 命令，慢的时间主要分布在3秒以上，1-3秒的慢查询比较少，这明显不太符合分布规律。并且目前看慢查询基本都发生在MySQL的0库的部分连接上（后端有一堆MySQL组成的集群），下面抓包的4637端口是MySQL的服务端口：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/b8ed95b7081ee80eb23465ee0e9acc74.png" alt="image.png"></p>
<p>以上两个连接都很慢，对应的慢查询在MySQL里面记录很快。</p>
<p>慢的SQL的response按时间排序基本都在3秒以上：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/36a2a60f64011bc73fee06c291bcd79f.png" alt="image.png" style="zoom:67%;"></p>
<p>或者只看response time 排序，中间几个1秒多的都是 Insert语句。也就是1秒到3秒之间的没有，主要是3秒以上的查询</p>
<p>!<img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/07146ff29534a1070adbdb8cedd280c9.png" alt="image.png" style="zoom:67%;"></p>
<h3 id="快的连接"><a href="#快的连接" class="headerlink" title="快的连接"></a>快的连接</h3><p>同样一个查询SQL，发到同一个MySQL上(4637端口)，下面的连接上的所有操作都很快，下面是两个快的连接上的执行截图</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/d129dfe1a50b182f4d100ac7147f9099.png" alt="image.png"></p>
<p>别的MySQL上都比较快，比如5556分片上的所有response RT排序，只有偶尔极个别的慢SQL</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/01531d138b9bc8dafda76b7c8bbb5bc9.png" alt="image.png"></p>
<h2 id="MySQL相关参数"><a href="#MySQL相关参数" class="headerlink" title="MySQL相关参数"></a>MySQL相关参数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line">mysql&gt; show variables like &apos;%thread%&apos;;</div><div class="line">+--------------------------------------------+-----------------+</div><div class="line">| Variable_name                              | Value           |</div><div class="line">+--------------------------------------------+-----------------+</div><div class="line">| innodb_purge_threads                       | 1               |</div><div class="line">| innodb_MySQL_thread_extra_concurrency        | 0               |</div><div class="line">| innodb_read_io_threads                     | 16              |</div><div class="line">| innodb_thread_concurrency                  | 0               |</div><div class="line">| innodb_thread_sleep_delay                  | 10000           |</div><div class="line">| innodb_write_io_threads                    | 16              |</div><div class="line">| max_delayed_threads                        | 20              |</div><div class="line">| max_insert_delayed_threads                 | 20              |</div><div class="line">| myisam_repair_threads                      | 1               |</div><div class="line">| performance_schema_max_thread_classes      | 50              |</div><div class="line">| performance_schema_max_thread_instances    | -1              |</div><div class="line">| pseudo_thread_id                           | 12882624        |</div><div class="line">| MySQL_is_dump_thread                         | OFF             |</div><div class="line">| MySQL_threads_running_ctl_mode               | SELECTS         |</div><div class="line">| MySQL_threads_running_high_watermark         | 50000           |</div><div class="line">| rocksdb_enable_thread_tracking             | OFF             |</div><div class="line">| rocksdb_enable_write_thread_adaptive_yield | OFF             |</div><div class="line">| rocksdb_signal_drop_index_thread           | OFF             |</div><div class="line">| thread_cache_size                          | 100             |</div><div class="line">| thread_concurrency                         | 10              |</div><div class="line">| thread_handling                            | pool-of-threads |</div><div class="line">| thread_pool_high_prio_mode                 | transactions    |</div><div class="line">| thread_pool_high_prio_tickets              | 4294967295      |</div><div class="line">| thread_pool_idle_timeout                   | 60              |</div><div class="line">| thread_pool_max_threads                    | 100000          |</div><div class="line">| thread_pool_oversubscribe                  | 10              |</div><div class="line">| thread_pool_size                           | 96              |</div><div class="line">| thread_pool_stall_limit                    | 30              |</div><div class="line">| thread_stack                               | 262144          |</div><div class="line">| threadpool_workaround_epoll_bug            | OFF             |</div><div class="line">| tokudb_cachetable_pool_threads             | 0               |</div><div class="line">| tokudb_checkpoint_pool_threads             | 0               |</div><div class="line">| tokudb_client_pool_threads                 | 0               |</div><div class="line">+--------------------------------------------+-----------------+</div><div class="line">33 rows in set (0.00 sec)</div><div class="line"></div><div class="line">mysql&gt; </div><div class="line"></div><div class="line">22 rows in set (0.00 sec)</div><div class="line"></div><div class="line">mysql&gt; show create table XT_SCENES_PARAM \G</div><div class="line">*************************** 1. row ***************************</div><div class="line">       Table: XT_SCENES_PARAM</div><div class="line">Create Table: CREATE TABLE `xt_scenes_param` (</div><div class="line">  `SCENES` varchar(150) COLLATE utf8mb4_bin NOT NULL COMMENT &apos;????&apos;,</div><div class="line">  `CURRENT_USABLE_FLAG` varchar(150) COLLATE utf8mb4_bin DEFAULT NULL COMMENT &apos;????????&apos;,</div><div class="line">  `NEXT_USABLE_FLAG` varchar(150) COLLATE utf8mb4_bin DEFAULT NULL COMMENT &apos;????????????&apos;,</div><div class="line">  `PREVIOUS_USABLE_FLAG` varchar(150) COLLATE utf8mb4_bin DEFAULT NULL COMMENT &apos;????????&apos;,</div><div class="line">  `LAST_UPDATE_TIME` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &apos;???????&apos;,</div><div class="line">  `CHANGE_FLAG` char(1) COLLATE utf8mb4_bin DEFAULT NULL COMMENT &apos;?????1???? 2?????&apos;,</div><div class="line">  `SCENES_DESC` varchar(150) COLLATE utf8mb4_bin DEFAULT NULL COMMENT &apos;????&apos;,</div><div class="line">  PRIMARY KEY (`SCENES`)</div><div class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=&apos;????????&apos;</div><div class="line">1 row in set (0.00 sec)</div><div class="line"></div><div class="line">mysql&gt; explain SELECT `XT_SCENES_PARAM`.`SCENES` AS `SCENES`, `XT_SCENES_PARAM`.`CURRENT_USABLE_FLAG` AS `currentUseFlag`, `XT_SCENES_PARAM`.`PREVIOUS_USABLE_FLAG` AS `previousUseFlag`, `XT_SCENES_PARAM`.`LAST_UPDATE_TIME` AS `lastUpdateTime`, `XT_SCENES_PARAM`.`SCENES_DESC` AS `scenesDesc` FROM `XT_SCENES_PARAM` AS `XT_SCENES_PARAM` WHERE (`XT_SCENES_PARAM`.`SCENES` = &apos;QYXXCX&apos;);</div><div class="line"></div><div class="line">+----+-------------+-----------------+-------+---------------+---------+---------+-------+------+-------+</div><div class="line">| id | select_type | table           | type  | possible_keys | key     | key_len | ref   | rows | Extra |</div><div class="line">+----+-------------+-----------------+-------+---------------+---------+---------+-------+------+-------+</div><div class="line">|  1 | SIMPLE      | XT_SCENES_PARAM | const | PRIMARY       | PRIMARY | 602     | const |    1 | NULL  |</div><div class="line">+----+-------------+-----------------+-------+---------------+---------+---------+-------+------+-------+</div><div class="line">1 row in set (0.00 sec)</div></pre></td></tr></table></figure>
<h2 id="综上结论"><a href="#综上结论" class="headerlink" title="综上结论"></a>综上结论</h2><p>问题原因跟MySQL线程池比较相关，慢的连接总是慢，快的连接总是快。需要到MySQL Server下排查线程池相关参数。</p>
<p>同一个慢的连接上的回包，所有 ack 就很快（OS直接回，不需要进到MySQL），但是set就很慢，基本理解只要进到MySQL的就慢了，所以排除了网络原因（流量本身也很小，也没看到乱序、丢包之类的）</p>
<h2 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h2><p>18点的时候将4637端口对应的MySQL的 thread_pool_oversubscribe 从10调整到20后，基本没有慢查询了：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/92069e7521368e4d2519b3b861cc7faa.png" alt="image.png" style="zoom:50%;"></p>
<p>但是不太能理解的是从MySQL的观察来看，并发压力很小，很难抓到running thread比较高的情况。</p>
<p>MySQL记录的执行时间是指SQL语句开始解析后统计，中间的等锁、等Worker都不会记录在执行时间中，所以当时对应的SQL在MySQL日志记录中很快。</p>
<p><em>这里表现出高 RT 而不是超时，原因是 MySQL 线程池有另一个参数 thread_pool_stall_limit 防止线程卡死．请求如果在分组内等待超过 thread_pool_stall_limit 时间没被处理，则会退回传统模式，创建新线程来处理请求．这个参数的默认值是 500ms。另外这个等待时间是不会被记录到MySQL的慢查询日志中的</em></p>
<h2 id="Thread-Pool原理"><a href="#Thread-Pool原理" class="headerlink" title="Thread Pool原理"></a>Thread Pool原理</h2><p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/6fbe1c10f07dd1c26eba0c0e804fa9a8.png" alt="image.png"></p>
<p>MySQL 原有线程调度方式有每个连接一个线程(one-thread-per-connection)和所有连接一个线程（no-threads）。</p>
<p>no-threads一般用于调试，生产环境一般用one-thread-per-connection方式。one-thread-per-connection 适合于低并发长连接的环境，而在高并发或大量短连接环境下，大量创建和销毁线程，以及线程上下文切换，会严重影响性能。另外 one-thread-per-connection 对于大量连接数扩展也会影响性能。</p>
<p>为了解决上述问题，MariaDB、Percona、Oracle MySQL 都推出了线程池方案，它们的实现方式大体相似，这里以 Percona 为例来简略介绍实现原理，同时会介绍我们在其基础上的一些改进。</p>
<p>线程池由一系列 worker 线程组成，这些worker线程被分为<code>thread_pool_size</code>个group。用户的连接按 round-robin 的方式映射到相应的group 中，一个连接可以由一个group中的一个或多个worker线程来处理。</p>
<p><code>thread_pool_stall_limit</code> timer线程检测间隔。此参数设置过小，会导致创建过多的线程，从而产生较多的线程上下文切换，但可以及时处理锁等待的场景，避免死锁。参数设置过大，对长语句有益，但会阻塞短语句的执行。参数设置需视具体情况而定，例如99%的语句10ms内可以完成，那么我们可以将就<code>thread_pool_stall_limit</code>设置为10ms。</p>
<p>thread_pool_oversubscribe  一个group中活跃线程和等待中的线程超过<code>thread_pool_oversubscribe</code>时，不会创建新的线程。 此参数可以控制系统的并发数，同时可以防止调度上的死锁，考虑如下情况，A、B、C三个事务，A、B 需等待C提交。A、B先得到调度，同时活跃线程数达到了<code>thread_pool_max_threads</code>上限，随后C继续执行提交，此时已经没有线程来处理C提交，从而导致A、B一直等待。<code>thread_pool_oversubscribe</code>控制group中活跃线程和等待中的线程总数，从而防止了上述情况。</p>
<p>一包在手，万事无忧</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://www.atatech.org/articles/36343" target="_blank" rel="external">https://www.atatech.org/articles/36343</a></p>
<p><a href="http://mysql.taobao.org/monthly/2016/02/09/" target="_blank" rel="external">http://mysql.taobao.org/monthly/2016/02/09/</a></p>
<p><a href="https://dbaplus.cn/news-11-1989-1.html" target="_blank" rel="external">https://dbaplus.cn/news-11-1989-1.html</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;MySQL-线程池导致的延时卡顿排查&quot;&gt;&lt;a href=&quot;#MySQL-线程池导致的延时卡顿排查&quot; class=&quot;headerlink&quot; title=&quot;MySQL 线程池导致的延时卡顿排查&quot;&gt;&lt;/a&gt;MySQL 线程池导致的延时卡顿排查&lt;/h1&gt;&lt;h2 id=&quot;问
    
    </summary>
    
      <category term="MySQL" scheme="http://yoursite.com/categories/MySQL/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="MySQL" scheme="http://yoursite.com/tags/MySQL/"/>
    
      <category term="network" scheme="http://yoursite.com/tags/network/"/>
    
      <category term="Performance" scheme="http://yoursite.com/tags/Performance/"/>
    
      <category term="ThreadPool" scheme="http://yoursite.com/tags/ThreadPool/"/>
    
  </entry>
  
  <entry>
    <title>Perf IPC以及CPU利用率</title>
    <link href="http://yoursite.com/2020/05/31/Perf%20IPC%E4%BB%A5%E5%8F%8ACPU%E5%88%A9%E7%94%A8%E7%8E%87/"/>
    <id>http://yoursite.com/2020/05/31/Perf IPC以及CPU利用率/</id>
    <published>2020-05-31T04:30:03.000Z</published>
    <updated>2020-08-02T12:41:18.374Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Perf-IPC以及CPU利用率"><a href="#Perf-IPC以及CPU利用率" class="headerlink" title="Perf IPC以及CPU利用率"></a>Perf IPC以及CPU利用率</h1><h2 id="perf-使用"><a href="#perf-使用" class="headerlink" title="perf 使用"></a>perf 使用</h2><p>可以通过perf看到cpu的使用情况：</p>
<pre><code>$sudo perf stat -a -- sleep 10

 Performance counter stats for &apos;system wide&apos;:

 239866.330098      task-clock (msec)         #   23.985 CPUs utilized    /10*1000        (100.00%)
        45,709      context-switches          #    0.191 K/sec                    (100.00%)
         1,715      cpu-migrations            #    0.007 K/sec                    (100.00%)
        79,586      page-faults               #    0.332 K/sec
 3,488,525,170      cycles                    #    0.015 GHz                      (83.34%)
 9,708,140,897      stalled-cycles-frontend   #  278.29% /cycles frontend cycles idle     (83.34%)
 9,314,891,615      stalled-cycles-backend    #  267.02% /cycles backend  cycles idle     (66.68%)
 2,292,955,367      instructions              #    0.66  insns per cycle  insn/cycles
                                             #    4.23  stalled cycles per insn stalled-cycles-frontend/insn (83.34%)
   447,584,805      branches                  #    1.866 M/sec                    (83.33%)
     8,470,791      branch-misses             #    1.89% of all branches          (83.33%)
</code></pre><p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/f96e50b5f3d0825b68be5b654624f839.png" alt="image.png"></p>
<p>cycles：CPU时钟周期。CPU从它的指令集(instruction set)中选择指令执行。一个指令包含以下的步骤，每个步骤由CPU的一个叫做功能单元(functional unit)的组件来进行处理，每个步骤的执行都至少需要花费一个时钟周期。</p>
<ul>
<li>指令读取(instruction fetch， IF)</li>
<li>指令解码(instruction decode， ID)</li>
<li>执行(execute， EXE)</li>
<li>内存访问(memory access，MEM)</li>
<li>寄存器回写(register write-back， WB)</li>
</ul>
<p><img src="http://static.oschina.net/uploads/space/2018/0330/193750_LPcO_2896894.png" alt=""></p>
<p>五个步骤只能串行，但是可以做成pipeline提升效率，也就是第一个指令做第二步的时候，指令读取单元可以去读取下一个指令了，如果有一个指令慢就会造成stall，也就是pipeline有地方卡壳了。<br>另外cpu可以同时有多条pipeline，这也就是理论上最大的ipc.<br>stalled-cycles，则是指令管道未能按理想状态发挥并行作用，发生停滞的时钟周期。stalled-cycles-frontend指指令读取或解码的指令步骤，而stalled-cycles-backend则是指令执行步骤。第二列中的cycles idle其实意思跟stalled是一样的，由于指令执行停滞了，所以指令管道也就空闲了，千万不要误解为CPU的空闲率。这个数值是由stalled-cycles-frontend或stalled-cycles-backend除以上面的cycles得出的</p>
<ul>
<li>非流水线：</li>
</ul>
<p><img src="http://static.oschina.net/uploads/space/2018/0330/195430_76ME_2896894.png" alt="img"></p>
<p>对于非流水计算机而言，上一条指令的 5 个子过程全部执行完毕后才能开始下一条指令，每隔 5 个时 钟周期才有一个输出结果。因此，图3中用了 15 个时钟周期才完成 3 条指令，每条指令平均用时 5 个时钟周期。 非流水线工作方式的控制比较简单，但部件的利用率较低，系统工作速度较慢。</p>
<p>毫无疑问，非流水线效率很低下，5个单元同时只能有一个单元工作，每隔 5 个时 钟周期才有一个输出结果。每条指令用时5个时间周期。</p>
<ul>
<li>标量流水线, 标量（Scalar）流水计算机是<strong>只有一条指令流水线</strong>的计算机:</li>
</ul>
<p><img src="http://static.oschina.net/uploads/space/2018/0330/195701_ce0y_2896894.png" alt=""></p>
<p> 对标量流水计算机而言，上一条指令与下一条指令的 5 个子过程在时间上可以重叠执行，当流水线满 载时，每一个时钟周期就可以输出一个结果。因此，图中仅用了 9 个时钟周期就完成了 5 条指令，每条指令平均用时 1.8 个时钟周期。</p>
<p>采用标量流水线工作方式，<strong>虽然每条指令的执行时间并未缩短，但 CPU 运行指令的总体速度却能成倍 提高</strong>。当然，作为速度提高的代价，需要增加部分硬件才能实现标量流水。</p>
<ul>
<li>超标量流水线：</li>
</ul>
<p>所谓超标量（Superscalar）流 水计算机，是指它<strong>具有两条以上的指令流水线</strong></p>
<p><img src="http://static.oschina.net/uploads/space/2018/0330/200055_5w6G_2896894.png" alt=""></p>
<p>当流水线满载时，每一个时钟周期可以执行 2 条以上的指令。图中仅用了 9 个时钟周期就完成了 10 条指令，每条指令平均用时 0.9 个时钟周期。 超标量流水计算机是时间并行技术和空间并行技术的综合应用。</p>
<p>在流水计算机中，指令的处理是重叠进行的，前一条指令还没有结束，第二、三条指令就陆续开始工 作。由于多条指令的重叠处理，当后继指令所需的操作数刚好是前一指令的运算结果时，便发生数据相关冲突。由于这两条指令的执行顺序直接影响到操作数读取的内容，必须等前一条指令执行完毕后才能执行后一条指令。</p>
<p><strong>OoOE— Out-of-Order Execution 乱序执行也是在 Pentium Pro 开始引入的</strong>，它有些类似于多线程的概念。<strong>乱序执行是为了直接提升 ILP(Instruction Level Parallelism)指令级并行化的设计</strong>，在多个执行单元的超标量设计当中，一系列的执行单元可以<strong>同时运行</strong>一些<strong>没有数据关联性的若干指令</strong>，<strong>只有需要等待其他指令运算结果的数据会按照顺序执行</strong>，从而总体提升了运行效率。乱序执行引擎是一个很重要的部分，需要进行复杂的调度管理。</p>
<h2 id="ECS和perf"><a href="#ECS和perf" class="headerlink" title="ECS和perf"></a>ECS和perf</h2><p>在ECS会采集不到 cycles等，cpu-clock、page-faults都是内核中的软事件，cycles/instructions得采集cpu的PMU数据，ECS采集不到这些。</p>
<p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/a120388ff72d712a4fd176e7cea005cf.png" alt="image.png"></p>
<h2 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h2><p>NMI(non-maskable interrupt)，就是不可屏蔽的中断. NMI通常用于通知操作系统发生了无法恢复的硬件错误，也可以用于系统调试与采样，大多数服务器还提供了人工触发NMI的接口，比如NMI按钮或者iLO命令等。</p>
<p><a href="http://cenalulu.github.io/linux/numa/" target="_blank" rel="external">http://cenalulu.github.io/linux/numa/</a> numa原理和优缺点案例讲解</p>
<p>正在运行中的用户程序被中断之后，必须等到中断处理例程完成之后才能恢复运行，在此期间即使其它CPU是空闲的也不能换个CPU继续运行，就像被中断牢牢钉在了当前的CPU上，动弹不得，中断处理需要多长时间，用户进程就被冻结多长时间。</p>
<p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/1d20b6172d4effb7e27feedc06a820f9.png" alt="image.png"></p>
<p>Linux kernel把中断分为两部分：hard IRQ和soft IRQ，hard IRQ只处理中断最基本的部分，保证迅速响应，尽量在最短的时间里完成，把相对耗时的工作量留给soft IRQ；soft IRQ可以被hard IRQ中断，如果soft IRQ运行时间过长，也可能会被交给内核线程ksoftirqd去继续完成。<br><a href="https://mp.weixin.qq.com/s/AzcB1DwqRCoiofOOI88T9Q" target="_blank" rel="external">https://mp.weixin.qq.com/s/AzcB1DwqRCoiofOOI88T9Q</a> softirq导致一路CPU使用过高，其它CPU还是闲置，整个系统比较慢</p>
<p>Linux的进程调度有一个不太为人熟知的特性，叫做wakeup affinity，它的初衷是这样的：如果两个进程频繁互动，那么它们很有可能共享同样的数据，把它们放到亲缘性更近的scheduling domain有助于提高缓存和内存的访问性能，所以当一个进程唤醒另一个的时候，被唤醒的进程可能会被放到相同的CPU core或者相同的NUMA节点上。这个特性缺省是打开的，它有时候很有用，但有时候却对性能有伤害作用。设想这样一个应用场景：一个主进程给成百上千个辅进程派发任务，这成百上千个辅进程被唤醒后被安排到与主进程相同的CPU core或者NUMA节点上，就会导致负载严重失衡，CPU忙的忙死、闲的闲死，造成性能下降。<a href="https://mp.weixin.qq.com/s/DG1v8cUjcXpa0x2uvrRytA" target="_blank" rel="external">https://mp.weixin.qq.com/s/DG1v8cUjcXpa0x2uvrRytA</a></p>
<p><a href="http://linuxperf.com/?p=197" target="_blank" rel="external">http://linuxperf.com/?p=197</a><br>Linux kernel支持两种实时(real-time)调度策略(scheduling policy)：SCHED_FIFO和SCHED_RR<br>/proc/sys/kernel/sched_rt_period_us<br>缺省值是1,000,000 μs (1秒)，表示实时进程的运行粒度为1秒。（注：修改这个参数请谨慎，太大或太小都可能带来问题）。<br>/proc/sys/kernel/sched_rt_runtime_us<br>缺省值是 950,000 μs (0.95秒)，表示在1秒的运行周期里所有的实时进程一起最多可以占用0.95秒的CPU时间。<br>如果sched_rt_runtime_us=-1，表示取消限制，意味着实时进程可以占用100%的CPU时间（慎用，有可能使系统失去控制）。<br>所以，Linux kernel默认情况下保证了普通进程无论如何都可以得到5%的CPU时间，尽管系统可能会慢如蜗牛，但管理员仍然可以利用这5%的时间设法恢复系统，比如停掉失控的实时进程，或者给自己的shell进程赋予更高的实时优先级以便执行管理任务，等等。</p>
<p>进程自愿切换(Voluntary)和强制切换(Involuntary)的次数被统计在 /proc/<pid>/status 中，其中voluntary_ctxt_switches表示自愿切换的次数，nonvoluntary_ctxt_switches表示强制切换的次数，两者都是自进程启动以来的累计值。 或pidstat -w 1 来统计  <a href="http://linuxperf.com/?cat=10" target="_blank" rel="external">http://linuxperf.com/?cat=10</a><br>自愿切换发生的时候，进程不再处于运行状态，比如由于等待IO而阻塞(TASK_UNINTERRUPTIBLE)，或者因等待资源和特定事件而休眠(TASK_INTERRUPTIBLE)，又或者被debug/trace设置为TASK_STOPPED/TASK_TRACED状态；<br>强制切换发生的时候，进程仍然处于运行状态(TASK_RUNNING)，通常是由于被优先级更高的进程抢占(preempt)，或者进程的时间片用完了<br>如果一个进程的自愿切换占多数，意味着它对CPU资源的需求不高。如果一个进程的强制切换占多数，意味着对它来说CPU资源可能是个瓶颈，这里需要排除进程频繁调用sched_yield()导致强制切换的情况</pid></p>
<p>spinlock(自旋锁)是内核中最常见的锁，它的特点是：等待锁的过程中不休眠，而是占着CPU空转，优点是避免了上下文切换的开销，缺点是该CPU空转属于浪费, 同时还有可能导致cache ping-pong，spinlock适合用来保护快进快出的临界区。持有spinlock的CPU不能被抢占，持有spinlock的代码不能休眠 <a href="http://linuxperf.com/?p=138" target="_blank" rel="external">http://linuxperf.com/?p=138</a></p>
<p>每个逻辑 CPU 都维护着一个可运行队列，用来存放可运行的线程来调度。</p>
<h2 id="CPU-cache"><a href="#CPU-cache" class="headerlink" title="CPU cache"></a>CPU cache</h2><p><img src="https://images.gitbook.cn/227f3af0-5075-11e9-aece-c5816949b340" alt=""></p>
<p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/f5728a2afb29c653a3e1bf21f4d56056.png" alt="image.png"></p>
<pre><code>cat /proc/cpuinfo |grep -i cache
</code></pre><p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/ad19b92ccc97763aa7f78d8d1d514c84.png" alt="image.png"></p>
<p>如下 Linux getconf 命令的输出，除了 <em>_LINESIZE 指示了系统的 Cache Line 的大小是 64 字节外，还给出了 Cache 类别，大小。 其中 </em>_ASSOC 则指示了该 Cache 是几路关联 (Way Associative) 的。</p>
<pre><code>$sudo getconf -a |grep CACHE
LEVEL1_ICACHE_SIZE                 32768
LEVEL1_ICACHE_ASSOC                8
LEVEL1_ICACHE_LINESIZE             64
LEVEL1_DCACHE_SIZE                 32768
LEVEL1_DCACHE_ASSOC                8
LEVEL1_DCACHE_LINESIZE             64
LEVEL2_CACHE_SIZE                  262144
LEVEL2_CACHE_ASSOC                 4
LEVEL2_CACHE_LINESIZE              64
LEVEL3_CACHE_SIZE                  3145728
LEVEL3_CACHE_ASSOC                 12
LEVEL3_CACHE_LINESIZE              64
LEVEL4_CACHE_SIZE                  0
LEVEL4_CACHE_ASSOC                 0
LEVEL4_CACHE_LINESIZE              0
</code></pre><h2 id="Socket、核"><a href="#Socket、核" class="headerlink" title="Socket、核"></a>Socket、核</h2><p>一个Socket理解一个CPU，一个CPU又可以是多核的</p>
<h2 id="超线程（Hyperthreading，HT）"><a href="#超线程（Hyperthreading，HT）" class="headerlink" title="超线程（Hyperthreading，HT）"></a>超线程（Hyperthreading，HT）</h2><p>一个核还可以进一步分成几个逻辑核，来执行多个控制流程，这样可以进一步提高并行程度，这一技术就叫超线程，有时叫做 simultaneous multi-threading（SMT）。</p>
<p>超线程技术主要的出发点是，当处理器在运行一个线程，执行指令代码时，很多时候处理器并不会使用到全部的计算能力，部分计算能力就会处于空闲状态。而超线程技术就是通过多线程来进一步“压榨”处理器。pipeline进入stalled状态就可以切到其它超线程上</p>
<p>举个例子，如果一个线程运行过程中，必须要等到一些数据加载到缓存中以后才能继续执行，此时 CPU 就可以切换到另一个线程，去执行其他指令，而不用去处于空闲状态，等待当前线程的数据加载完毕。通常，一个传统的处理器在线程之间切换，可能需要几万个时钟周期。而一个具有 HT 超线程技术的处理器只需要 1 个时钟周期。因此就大大减小了线程之间切换的成本，从而最大限度地让处理器满负荷运转。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhengheng.me/2015/11/12/perf-stat/" target="_blank" rel="external">perf详解</a></p>
<p><a href="https://www.atatech.org/articles/109158" target="_blank" rel="external">CPU体系结构</a></p>
<p><a href="https://mp.weixin.qq.com/s/KaDJ1EF5Y-ndjRv2iUO3cA" target="_blank" rel="external">震惊，用了这么多年的 CPU 利用率，其实是错的</a>cpu占用不代表在做事情，可能是stalled，也就是流水线卡顿，但是cpu占用了，实际没事情做。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Perf-IPC以及CPU利用率&quot;&gt;&lt;a href=&quot;#Perf-IPC以及CPU利用率&quot; class=&quot;headerlink&quot; title=&quot;Perf IPC以及CPU利用率&quot;&gt;&lt;/a&gt;Perf IPC以及CPU利用率&lt;/h1&gt;&lt;h2 id=&quot;perf-使用&quot;&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="perf" scheme="http://yoursite.com/tags/perf/"/>
    
      <category term="IPC" scheme="http://yoursite.com/tags/IPC/"/>
    
      <category term="CPU" scheme="http://yoursite.com/tags/CPU/"/>
    
      <category term="pipeline" scheme="http://yoursite.com/tags/pipeline/"/>
    
  </entry>
  
  <entry>
    <title>程序员如何学习和构建网络知识体系</title>
    <link href="http://yoursite.com/2020/05/24/%E7%A8%8B%E5%BA%8F%E5%91%98%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%9E%84%E5%BB%BA%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"/>
    <id>http://yoursite.com/2020/05/24/程序员如何学习和构建网络知识体系/</id>
    <published>2020-05-24T09:30:03.000Z</published>
    <updated>2020-07-16T06:07:35.065Z</updated>
    
    <content type="html"><![CDATA[<h1 id="程序员如何学习和构建网络知识体系"><a href="#程序员如何学习和构建网络知识体系" class="headerlink" title="程序员如何学习和构建网络知识体系"></a>程序员如何学习和构建网络知识体系</h1><p>大家学习网络知识的过程中经常发现当时看懂了，很快又忘了，最典型的比如TCP三次握手、为什么要握手，大家基本都看过，但是种感觉还差那么一点点。都要看是因为面试官总要问，所以不能不知道啊。</p>
<p>我们来看一个典型的面试问题：</p>
<blockquote>
<p>问：为什么TCP是可靠的？<br>答：因为TCP有连接（或者回答因为TCP有握手）</p>
<p>追问：为什么有连接就可靠了？（面试的人估计心里在骂，你这不是傻逼么，有连接就可靠啊）</p>
<p>追问：这个TCP连接的本质是什么？网络上给你保留了一个带宽所以能可靠？<br>答：……懵了（或者因为TCP有ack，所以可靠）</p>
<p>追问：握手的本质是什么？为什么握手就可靠了<br>答：因为握手需要ack<br>追问：那这个ack也只是保证握手可靠，握手是怎么保证后面可靠的？握手本质做了什么事情？</p>
<p>追问：有了ack可靠后还会带来什么问题（比如发一个包ack一下，肯定是可考虑，但是效率不行，面试官想知道的是这里TCP怎么传输的，从而引出各个buffer、拥塞窗口的概念）</p>
</blockquote>
<p>基本上我发现99%的程序员会回答TCP相对UDP是可靠的，70%以上的程序员会告诉你可靠是因为有ack（其他的会告诉你可靠是因为握手或者有连接）</p>
<p>我的理解：</p>
<blockquote>
<p>物理上没有一个连接的东西在这里，udp也类似会占用端口、ip，但是大家都没说过udp的连接。而本质上我们说tcp的握手是指tcp是协商和维护一些状态信息的，这个状态信息就包含seq、ack、窗口/buffer，tcp握手就是协商出来这些初始值。这些状态才是我们平时所说的tcp连接的本质。</p>
</blockquote>
<p>这说明大部分程序员对问题的本质的理解上出了问题，或者教科书描述的过于教条不够接地气所以看完书本质没get到。</p>
<h2 id="再来看一个案例"><a href="#再来看一个案例" class="headerlink" title="再来看一个案例"></a>再来看一个案例</h2><p>我第一次看<a href="https://tools.ietf.org/html/rfc1180" target="_blank" rel="external">RFC1180</a>的时候是震惊的，觉得讲述的太好了，2000字就把一本教科书的知识阐述的无比清晰、透彻。但是我实际发现很快就忘了，而且对大部分程序员基本都是这样</p>
<blockquote>
<p>写的确实很好，清晰简洁，图文并茂，结构逻辑合理，但是对于90%的程序员没有什么卵用，当时看的时候很爽、也觉得自己理解了、学会了，实际上看完几周后就忘得差不多了。问题出在这种RFC偏理论多一点看起来完全没有体感无法感同身受，所以即使似乎当时看懂了，但是忘得也快，需要一篇结合实践的文章来帮助理解</p>
</blockquote>
<p>在这个问题上，让我深刻地理解到：</p>
<blockquote>
<p>一流的人看RFC就够了，差一些的人看《TCP/IP卷1》，再差些的人要看一个个案例带出来的具体知识的书籍了，比如<a href="https://book.douban.com/subject/26268767/" target="_blank" rel="external">《wireshark抓包艺术》</a>，人和人的学习能力有差别必须要承认。</p>
</blockquote>
<p>也就是我们要认识到每个个人的<a href="https://mp.weixin.qq.com/s/JlXWLpQSyj3Z_KMyUmzBPA" target="_blank" rel="external">学习能力的差异</a>，我超级认同这篇文章中的一个评论</p>
<blockquote>
<p>看完深有感触，尤其是后面的知识效率和工程效率型的区别。以前总是很中二的觉得自己看一遍就理解记住了，结果一次次失败又怀疑自己的智商是不是有问题，其实就是把自己当作知识效率型来用了。一个不太恰当的形容就是，有颗公主心却没公主命！</p>
</blockquote>
<p>嗯，大部分时候我们都觉得自己看一遍就理解了记住了能实用解决问题了，实际上了是马上忘了，停下来想想自己是不是这样的？在网络的相关知识上大部分看RFC、TCP卷1等东西是很难实际理解的，还是要靠实践来建立对知识的具体的理解，而网络相关的东西基本离大家有点远（大家不回去读tcp、ip源码，纯粹是靠对书本的理解），所以很难建立具体的概念，所以这里有个必杀技就是学会抓包和用wireshark看包，同时针对实际碰到的文题来抓包、看包分析。</p>
<p>比如我的这篇《<a href="https://mp.weixin.qq.com/s/x-ScSwEm3uQ2SFv-nAzNaA" target="_blank" rel="external">从计算机知识到落地能力，你欠缺了什么？</a>》就对上述问题最好的阐述，程序员最常碰到的网络问题就是为啥为啥不通？</p>
<p>这是最好建立对网络知识具体理解和实践的机会，你把《<a href="https://mp.weixin.qq.com/s/x-ScSwEm3uQ2SFv-nAzNaA" target="_blank" rel="external">从计算机知识到落地能力，你欠缺了什么？</a>》实践完再去看<a href="https://tools.ietf.org/html/rfc1180" target="_blank" rel="external">RFC1180</a> 就明白了。</p>
<h2 id="不要追求知识的广度"><a href="#不要追求知识的广度" class="headerlink" title="不要追求知识的广度"></a>不要追求知识的广度</h2><p>学习网络知识过程中，不建议每个知识点都去看，因为很快会忘记，我的方法是只看经常碰到的问题点，碰到一个点把他学透理解明白。</p>
<p>比如我曾经碰到过 <a href="https://plantegg.github.io/2019/01/09/就是要你懂ping--nslookup-OK-but-ping-fail/" target="_blank" rel="external">nslookup OK but ping fail–看看老司机是如何解决问题的，解决问题的方法肯定比知识点重要多了，同时透过一个问题怎么样通篇来理解一大块知识，让这块原理真正在你的只是提示中扎根下来</a> , 这个问题Google上很多人在搜索，说明很普遍，但是边上没碰到能把他说清楚，所以借着这个机会就把 Linux下的 NSS（name service switch）的原理搞懂了。要不然碰到问题大佬告诉你改下 /etc/hosts 或者  /etc/nsswitch 或者 /etc/resolv.conf 之类的问题就能解决，但是你一直不知道这三个文件怎么起作用的，也就是你碰到过这种问题也解决过但是下次碰到类似的问题你不一定能解决。</p>
<p>当然对我来说为了解决这个问题最后写了4篇跟域名解析相关的文章，从windows到linux，涉及到vpn、glibc、docker等各种场景，我把他叫做场景驱动。</p>
<p>关于<a href="https://mp.weixin.qq.com/s/JlXWLpQSyj3Z_KMyUmzBPA" target="_blank" rel="external">场景驱动学习的方法可以看这篇总结</a></p>
<h2 id="TCP是最复杂的，不要追求点点都到"><a href="#TCP是最复杂的，不要追求点点都到" class="headerlink" title="TCP是最复杂的，不要追求点点都到"></a>TCP是最复杂的，不要追求点点都到</h2><p>比如拥塞算法基本大家不会用到，了解下就行。还有拥塞窗口、慢启动，这个实际中碰到的概率不高，面试要问你基本上是属于炫技类型。实际碰到更多的是传输效率（<a href="https://mp.weixin.qq.com/s/fKWJrDNSAZjLsyobolIQKw" target="_blank" rel="external">对BDP、Buffer、各种窗口、rt的理解和运用</a>），还有为什么连不通、<a href="https://mp.weixin.qq.com/s/yH3PzGEFopbpA-jw4MythQ" target="_blank" rel="external">连接建立不起来</a>、为什么收到包不回复、为什么要reset、为什么丢包了之类的问题。</p>
<p>关于为什么连不通，我碰到了<a href="https://plantegg.github.io/2019/05/15/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%B8%8D%E9%80%9A%E6%98%AF%E4%B8%AA%E5%A4%A7%E9%97%AE%E9%A2%98--%E5%8D%8A%E5%A4%9C%E9%B8%A1%E5%8F%AB/" target="_blank" rel="external">这个问题</a>，随后在这个问题的基础上进行了总结，得到客户端建立连接的时候抛异常，可能的原因（握手失败，建不上连接）：</p>
<ul>
<li>网络不通，诊断：ping ip</li>
<li>端口不通,  诊断：telnet ip port</li>
<li>rp_filter 命中(rp_filter=1, 多网卡环境）， 诊断:  netstat -s | grep -i filter ;</li>
<li>snat/dnat的时候宿主机port冲突，内核会扔掉 syn包。诊断: sudo conntrack -S | grep  insert_failed //有不为0的</li>
<li>全连接队列满的情况，诊断： netstat -s | egrep “listen|LISTEN” </li>
<li>syn flood攻击, 诊断：同上</li>
<li>若远端服务器的内核参数 net.ipv4.tcp_tw_recycle 和 net.ipv4.tcp_timestamps 的值都为 1，则远端服务器会检查每一个报文中的时间戳（Timestamp），若 Timestamp 不是递增的关系，不会响应这个报文。配置 NAT 后，远端服务器看到来自不同的客户端的源 IP 相同，但 NAT 前每一台客户端的时间可能会有偏差，报文中的 Timestamp 就不是递增的情况。nat后的连接，开启timestamp。因为快速回收time_wait的需要，会校验时间该ip上次tcp通讯的timestamp大于本次tcp(nat后的不同机器经过nat后ip一样，保证不了timestamp递增），诊断：是否有nat和是否开启了timestamps</li>
<li>NAT 哈希表满导致 ECS 实例丢包 nf_conntrack full， 诊断: dmesg |grep conntrack</li>
</ul>
<h2 id="总结一下"><a href="#总结一下" class="headerlink" title="总结一下"></a>总结一下</h2><ul>
<li>一定要会用tcpdump和wireshark（纯工具，没有任何门槛）</li>
<li>多实践（因为网络知识离我们有点远、有点抽象）,用好各种工具，工具能帮我们看到、摸到</li>
<li>不要追求知识面的广度，深扣几个具体的知识点然后让这些点建立体系</li>
<li>不要为那些基本用不到的偏门知识花太多精力，天天用的都学不过来对吧。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;程序员如何学习和构建网络知识体系&quot;&gt;&lt;a href=&quot;#程序员如何学习和构建网络知识体系&quot; class=&quot;headerlink&quot; title=&quot;程序员如何学习和构建网络知识体系&quot;&gt;&lt;/a&gt;程序员如何学习和构建网络知识体系&lt;/h1&gt;&lt;p&gt;大家学习网络知识的过程中经常
    
    </summary>
    
      <category term="network" scheme="http://yoursite.com/categories/network/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="network" scheme="http://yoursite.com/tags/network/"/>
    
      <category term="TCP" scheme="http://yoursite.com/tags/TCP/"/>
    
  </entry>
  
  <entry>
    <title>就是要你懂TCP--半连接队列和全连接队列</title>
    <link href="http://yoursite.com/2020/04/07/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E5%8D%8A%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97--%E9%98%BF%E9%87%8C%E6%8A%80%E6%9C%AF%E5%85%AC%E4%BC%97%E5%8F%B7%E7%89%88%E6%9C%AC/"/>
    <id>http://yoursite.com/2020/04/07/就是要你懂TCP--半连接队列和全连接队列--阿里技术公众号版本/</id>
    <published>2020-04-07T09:30:03.000Z</published>
    <updated>2020-07-16T06:27:34.065Z</updated>
    
    <content type="html"><![CDATA[<h1 id="关于TCP-半连接队列和全连接队列"><a href="#关于TCP-半连接队列和全连接队列" class="headerlink" title="关于TCP 半连接队列和全连接队列"></a>关于TCP 半连接队列和全连接队列</h1><blockquote>
<p>最近碰到一个client端连接服务器总是抛异常的问题，然后定位分析并查阅各种资料文章，对TCP连接队列有个深入的理解</p>
<p>查资料过程中发现没有文章把这两个队列以及怎么观察他们的指标说清楚，希望通过这篇文章能把他们说清楚</p>
</blockquote>
<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><pre><code>场景：JAVA的client和server，使用socket通信。server使用NIO。

1.间歇性的出现client向server建立连接三次握手已经完成，但server的selector没有响应到这连接。
2.出问题的时间点，会同时有很多连接出现这个问题。
3.selector没有销毁重建，一直用的都是一个。
4.程序刚启动的时候必会出现一些，之后会间歇性出现。
</code></pre><h3 id="分析问题"><a href="#分析问题" class="headerlink" title="分析问题"></a>分析问题</h3><h4 id="正常TCP建连接三次握手过程："><a href="#正常TCP建连接三次握手过程：" class="headerlink" title="正常TCP建连接三次握手过程："></a>正常TCP建连接三次握手过程：</h4><p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/159a331ff8cdd4b8994dfe6a209d035f.png" alt="image.png"></p>
<ul>
<li>第一步：client 发送 syn 到server 发起握手；</li>
<li>第二步：server 收到 syn后回复syn+ack给client；</li>
<li>第三步：client 收到syn+ack后，回复server一个ack表示收到了server的syn+ack（此时client的56911端口的连接已经是established）</li>
</ul>
<p>从问题的描述来看，有点像TCP建连接的时候全连接队列（accept队列，后面具体讲）满了，尤其是症状2、4. 为了证明是这个原因，马上通过 netstat -s | egrep “listen” 去看队列的溢出统计数据：</p>
<pre><code>667399 times the listen queue of a socket overflowed
</code></pre><p>反复看了几次之后发现这个overflowed 一直在增加，那么可以明确的是server上全连接队列一定溢出了</p>
<p>接着查看溢出后，OS怎么处理：</p>
<pre><code># cat /proc/sys/net/ipv4/tcp_abort_on_overflow
0
</code></pre><p><strong>tcp_abort_on_overflow 为0表示如果三次握手第三步的时候全连接队列满了那么server扔掉client 发过来的ack（在server端认为连接还没建立起来）</strong></p>
<p>为了证明客户端应用代码的异常跟全连接队列满有关系，我先把tcp_abort_on_overflow修改成 1，1表示第三步的时候如果全连接队列满了，server发送一个reset包给client，表示废掉这个握手过程和这个连接（本来在server端这个连接就还没建立起来）。</p>
<p>接着测试，这时在客户端异常中可以看到很多connection reset by peer的错误，<strong>到此证明客户端错误是这个原因导致的（逻辑严谨、快速证明问题的关键点所在）</strong>。</p>
<p>于是开发同学翻看java 源代码发现socket 默认的backlog（这个值控制全连接队列的大小，后面再详述）是50，于是改大重新跑，经过12个小时以上的压测，这个错误一次都没出现了，同时观察到 overflowed 也不再增加了。</p>
<p>到此问题解决，<strong>简单来说TCP三次握手后有个accept队列，进到这个队列才能从Listen变成accept，默认backlog 值是50，很容易就满了</strong>。满了之后握手第三步的时候server就忽略了client发过来的ack包（隔一段时间server重发握手第二步的syn+ack包给client），如果这个连接一直排不上队就异常了。</p>
<blockquote>
<p>但是不能只是满足问题的解决，而是要去复盘解决过程，中间涉及到了哪些知识点是我所缺失或者理解不到位的；这个问题除了上面的异常信息表现出来之外，还有没有更明确地指征来查看和确认这个问题。</p>
</blockquote>
<h3 id="深入理解TCP握手过程中建连接的流程和队列"><a href="#深入理解TCP握手过程中建连接的流程和队列" class="headerlink" title="深入理解TCP握手过程中建连接的流程和队列"></a>深入理解TCP握手过程中建连接的流程和队列</h3><p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/2703fc07dfc4dd5b6e1bb4c2ce620e59.png" alt="image.png"><br>（图片来源：<a href="http://www.cnxct.com/something-about-phpfpm-s-backlog/）" target="_blank" rel="external">http://www.cnxct.com/something-about-phpfpm-s-backlog/）</a></p>
<p>如上图所示，这里有两个队列：syns queue(半连接队列）；accept queue（全连接队列）</p>
<p>三次握手中，在第一步server收到client的syn后，把这个连接信息放到半连接队列中，同时回复syn+ack给client（第二步）；</p>
<pre><code>题外话，比如syn floods 攻击就是针对半连接队列的，攻击方不停地建连接，但是建连接的时候只做第一步，第二步中攻击方收到server的syn+ack后故意扔掉什么也不做，导致server上这个队列满其它正常请求无法进来
</code></pre><p>第三步的时候server收到client的ack，如果这时全连接队列没满，那么从半连接队列拿出这个连接的信息放入到全连接队列中，否则按tcp_abort_on_overflow指示的执行。</p>
<p>这时如果全连接队列满了并且tcp_abort_on_overflow是0的话，server过一段时间再次发送syn+ack给client（也就是重新走握手的第二步），如果client超时等待比较短，client就很容易异常了。</p>
<p>在我们的os中retry 第二步的默认次数是2（centos默认是5次）：</p>
<pre><code>net.ipv4.tcp_synack_retries = 2
</code></pre><h3 id="如果TCP连接队列溢出，有哪些指标可以看呢？"><a href="#如果TCP连接队列溢出，有哪些指标可以看呢？" class="headerlink" title="如果TCP连接队列溢出，有哪些指标可以看呢？"></a>如果TCP连接队列溢出，有哪些指标可以看呢？</h3><p>上述解决过程有点绕，听起来蒙逼，那么下次再出现类似问题有什么更快更明确的手段来确认这个问题呢？</p>
<p>（<em>通过具体的、感性的东西来强化我们对知识点的理解和吸收</em>）</p>
<h4 id="netstat-s"><a href="#netstat-s" class="headerlink" title="netstat -s"></a>netstat -s</h4><pre><code>[root@server ~]#  netstat -s | egrep &quot;listen|LISTEN&quot; 
667399 times the listen queue of a socket overflowed
667399 SYNs to LISTEN sockets ignored
</code></pre><p>比如上面看到的 667399 times ，表示全连接队列溢出的次数，隔几秒钟执行下，如果这个数字一直在增加的话肯定全连接队列偶尔满了。</p>
<h4 id="ss-命令"><a href="#ss-命令" class="headerlink" title="ss 命令"></a>ss 命令</h4><pre><code>[root@server ~]# ss -lnt
Recv-Q Send-Q Local Address:Port  Peer Address:Port 
0        50               *:3306             *:* 
</code></pre><p><strong>上面看到的第二列Send-Q 值是50，表示第三列的listen端口上的全连接队列最大为50，第一列Recv-Q为全连接队列当前使用了多少</strong></p>
<p><strong>全连接队列的大小取决于：min(backlog, somaxconn) . backlog是在socket创建的时候传入的，somaxconn是一个os级别的系统参数</strong></p>
<p>这个时候可以跟我们的代码建立联系了，比如Java创建ServerSocket的时候会让你传入backlog的值：</p>
<pre><code>ServerSocket()
    Creates an unbound server socket.
ServerSocket(int port)
    Creates a server socket, bound to the specified port.
ServerSocket(int port, int backlog)
    Creates a server socket and binds it to the specified local port number, with the specified backlog.
ServerSocket(int port, int backlog, InetAddress bindAddr)
    Create a server with the specified port, listen backlog, and local IP address to bind to.
</code></pre><p>（来自JDK帮助文档：<a href="https://docs.oracle.com/javase/7/docs/api/java/net/ServerSocket.html）" target="_blank" rel="external">https://docs.oracle.com/javase/7/docs/api/java/net/ServerSocket.html）</a></p>
<p><strong>半连接队列的大小取决于：max(64,  /proc/sys/net/ipv4/tcp_max_syn_backlog)。 不同版本的os会有些差异</strong></p>
<blockquote>
<p>我们写代码的时候从来没有想过这个backlog或者说大多时候就没给他值（那么默认就是50），直接忽视了他，首先这是一个知识点的忙点；其次也许哪天你在哪篇文章中看到了这个参数，当时有点印象，但是过一阵子就忘了，这是知识之间没有建立连接，不是体系化的。但是如果你跟我一样首先经历了这个问题的痛苦，然后在压力和痛苦的驱动自己去找为什么，同时能够把为什么从代码层推理理解到OS层，那么这个知识点你才算是比较好地掌握了，也会成为你的知识体系在TCP或者性能方面成长自我生长的一个有力抓手</p>
</blockquote>
<h4 id="netstat-命令"><a href="#netstat-命令" class="headerlink" title="netstat 命令"></a>netstat 命令</h4><p>netstat跟ss命令一样也能看到Send-Q、Recv-Q这些状态信息，不过如果这个连接不是<strong>Listen状态</strong>的话，Recv-Q就是指收到的数据还在缓存中，还没被进程读取，这个值就是还没被进程读取的 bytes；而 Send 则是发送队列中没有被远程主机确认的 bytes 数</p>
<pre><code>$netstat -tn  
Active Internet connections (w/o servers)
Proto Recv-Q Send-Q Local Address   Foreign Address State  
tcp0  0 server:8182  client-1:15260 SYN_RECV   
tcp0 28 server:22    client-1:51708  ESTABLISHED
tcp0  0 server:2376  client-1:60269 ESTABLISHED
</code></pre><p> <strong>netstat -tn 看到的 Recv-Q 跟全连接半连接没有关系，这里特意拿出来说一下是因为容易跟 ss -lnt 的 Recv-Q 搞混淆，顺便建立知识体系，巩固相关知识点 </strong>  </p>
<h5 id="Recv-Q-和-Send-Q-的说明"><a href="#Recv-Q-和-Send-Q-的说明" class="headerlink" title="Recv-Q 和 Send-Q 的说明"></a>Recv-Q 和 Send-Q 的说明</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Recv-Q</div><div class="line">Established: The count of bytes not copied by the user program connected to this socket.</div><div class="line">Listening: Since Kernel 2.6.18 this column contains the current syn backlog.</div><div class="line"></div><div class="line">Send-Q</div><div class="line">Established: The count of bytes not acknowledged by the remote host.</div><div class="line">Listening: Since Kernel 2.6.18 this column contains the maximum size of the syn backlog.</div></pre></td></tr></table></figure>
<h6 id="通过-netstat-发现问题的案例"><a href="#通过-netstat-发现问题的案例" class="headerlink" title="通过 netstat 发现问题的案例"></a>通过 netstat 发现问题的案例</h6><p>自身太慢，比如如下netstat -t 看到的Recv-Q有大量数据堆积，那么一般是CPU处理不过来导致的：</p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/77ed9ba81f70f7940546f0a22dabf010.png" alt="image.png"></p>
<p>下面的case是接收方太慢，从应用机器的netstat统计来看，也是压力端回复太慢（本机listen 9108端口)</p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2020/png/33359/1579241362064-807d8378-6c54-4a2c-a888-ff2337df817c.png" alt="image.png" style="zoom:80%;"></p>
<p>send-q表示回复从9108发走了，没收到对方的ack，<strong>基本可以推断PTS到9108之间有瓶颈</strong></p>
<p>上面是通过一些具体的工具、指标来认识全连接队列（工程效率的手段）   </p>
<h3 id="实践验证一下上面的理解"><a href="#实践验证一下上面的理解" class="headerlink" title="实践验证一下上面的理解"></a>实践验证一下上面的理解</h3><p>把java中backlog改成10（越小越容易溢出），继续跑压力，这个时候client又开始报异常了，然后在server上通过 ss 命令观察到：</p>
<pre><code>Fri May  5 13:50:23 CST 2017
Recv-Q Send-QLocal Address:Port  Peer Address:Port
11         10         *:3306               *:*
</code></pre><p>按照前面的理解，这个时候我们能看到3306这个端口上的服务全连接队列最大是10，但是现在有11个在队列中和等待进队列的，肯定有一个连接进不去队列要overflow掉，同时也确实能看到overflow的值在不断地增大。</p>
<h4 id="Tomcat和Nginx中的Accept队列参数"><a href="#Tomcat和Nginx中的Accept队列参数" class="headerlink" title="Tomcat和Nginx中的Accept队列参数"></a>Tomcat和Nginx中的Accept队列参数</h4><p>Tomcat默认短连接，backlog（Tomcat里面的术语是Accept count）Ali-tomcat默认是200, Apache Tomcat默认100. </p>
<pre><code>#ss -lnt
Recv-Q Send-Q   Local Address:Port Peer Address:Port
0       100                 *:8080            *:*
</code></pre><p>Nginx默认是511</p>
<pre><code>$sudo ss -lnt
State  Recv-Q Send-Q Local Address:PortPeer Address:Port
LISTEN    0     511              *:8085           *:*
LISTEN    0     511              *:8085           *:*
</code></pre><p>因为Nginx是多进程模式，所以看到了多个8085，也就是多个进程都监听同一个端口以尽量避免上下文切换来提升性能   </p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>全连接队列、半连接队列溢出这种问题很容易被忽视，但是又很关键，特别是对于一些短连接应用（比如Nginx、PHP，当然他们也是支持长连接的）更容易爆发。 一旦溢出，从cpu、线程状态看起来都比较正常，但是压力上不去，在client看来rt也比较高（rt=网络+排队+真正服务时间），但是从server日志记录的真正服务时间来看rt又很短。</p>
<p>jdk、netty等一些框架默认backlog比较小，可能有些情况下导致性能上不去，比如这个 <a href="https://www.atatech.org/articles/12919" target="_blank" rel="external">《netty新建连接并发数很小的case》 </a><br>都是类似原因</p>
<p>希望通过本文能够帮大家理解TCP连接过程中的半连接队列和全连接队列的概念、原理和作用，更关键的是有哪些指标可以明确看到这些问题（<strong>工程效率帮助强化对理论的理解</strong>）。</p>
<p>另外每个具体问题都是最好学习的机会，光看书理解肯定是不够深刻的，请珍惜每个具体问题，碰到后能够把来龙去脉弄清楚，每个问题都是你对具体知识点通关的好机会。</p>
<h3 id="最后提出相关问题给大家思考"><a href="#最后提出相关问题给大家思考" class="headerlink" title="最后提出相关问题给大家思考"></a>最后提出相关问题给大家思考</h3><ol>
<li>全连接队列满了会影响半连接队列吗？</li>
<li>netstat -s看到的overflowed和ignored的数值有什么联系吗？</li>
<li>如果client走完了TCP握手的第三步，在client看来连接已经建立好了，但是server上的对应连接实际没有准备好，这个时候如果client发数据给server，server会怎么处理呢？（有同学说会reset，你觉得呢？）</li>
</ol>
<blockquote>
<p>提出这些问题就是以这个知识点为抓手，让你的知识体系开始自我生长</p>
</blockquote>
<hr>
<p>参考文章：</p>
<p><a href="http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html" target="_blank" rel="external">http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html</a></p>
<p><a href="http://www.cnblogs.com/zengkefu/p/5606696.html" target="_blank" rel="external">http://www.cnblogs.com/zengkefu/p/5606696.html</a></p>
<p><a href="http://www.cnxct.com/something-about-phpfpm-s-backlog/" target="_blank" rel="external">http://www.cnxct.com/something-about-phpfpm-s-backlog/</a></p>
<p><a href="http://jaseywang.me/2014/07/20/tcp-queue-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/" target="_blank" rel="external">http://jaseywang.me/2014/07/20/tcp-queue-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/</a></p>
<p><a href="http://jin-yang.github.io/blog/network-synack-queue.html#" target="_blank" rel="external">http://jin-yang.github.io/blog/network-synack-queue.html#</a></p>
<p><a href="http://blog.chinaunix.net/uid-20662820-id-4154399.html" target="_blank" rel="external">http://blog.chinaunix.net/uid-20662820-id-4154399.html</a></p>
<p><a href="https://www.atatech.org/articles/12919" target="_blank" rel="external">https://www.atatech.org/articles/12919</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;关于TCP-半连接队列和全连接队列&quot;&gt;&lt;a href=&quot;#关于TCP-半连接队列和全连接队列&quot; class=&quot;headerlink&quot; title=&quot;关于TCP 半连接队列和全连接队列&quot;&gt;&lt;/a&gt;关于TCP 半连接队列和全连接队列&lt;/h1&gt;&lt;blockquote&gt;

    
    </summary>
    
      <category term="TCP" scheme="http://yoursite.com/categories/TCP/"/>
    
    
      <category term="netstat" scheme="http://yoursite.com/tags/netstat/"/>
    
      <category term="ss" scheme="http://yoursite.com/tags/ss/"/>
    
      <category term="accept queue" scheme="http://yoursite.com/tags/accept-queue/"/>
    
      <category term="syn queue" scheme="http://yoursite.com/tags/syn-queue/"/>
    
      <category term="syn flood" scheme="http://yoursite.com/tags/syn-flood/"/>
    
      <category term="overflows" scheme="http://yoursite.com/tags/overflows/"/>
    
      <category term="dropped" scheme="http://yoursite.com/tags/dropped/"/>
    
      <category term="TCP queue" scheme="http://yoursite.com/tags/TCP-queue/"/>
    
  </entry>
  
  <entry>
    <title>10+倍性能提升全过程</title>
    <link href="http://yoursite.com/2020/02/23/10%E5%80%8D%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87%E5%85%A8%E8%BF%87%E7%A8%8B--2016%E5%B9%B4%E5%8F%8C11%E4%BC%98%E9%85%B7%E4%BC%9A%E5%91%98%E4%BF%83%E9%94%80%E4%BC%98%E5%8C%96%E8%BF%87%E7%A8%8B/"/>
    <id>http://yoursite.com/2020/02/23/10倍性能提升全过程--2016年双11优酷会员促销优化过程/</id>
    <published>2020-02-23T09:30:03.000Z</published>
    <updated>2020-05-11T14:09:51.401Z</updated>
    
    <content type="html"><![CDATA[<h1 id="10-倍性能提升全过程–优酷账号绑定淘宝账号的TPS从500到5400的优化历程"><a href="#10-倍性能提升全过程–优酷账号绑定淘宝账号的TPS从500到5400的优化历程" class="headerlink" title="10+倍性能提升全过程–优酷账号绑定淘宝账号的TPS从500到5400的优化历程"></a>10+倍性能提升全过程–优酷账号绑定淘宝账号的TPS从500到5400的优化历程</h1><h2 id="背景说明"><a href="#背景说明" class="headerlink" title="背景说明"></a>背景说明</h2><blockquote>
<p>2016年的双11在淘宝上买买买的时候，天猫和优酷土豆一起做了联合促销，在天猫双11当天购物满XXX元就赠送优酷会员，这个过程需要用户在优酷侧绑定淘宝账号(登录优酷、提供淘宝账号，优酷调用淘宝API实现两个账号绑定）和赠送会员并让会员权益生效(看收费影片、免广告等等）</p>
<p>这里涉及到优酷的两个部门：Passport(在上海，负责登录、绑定账号，下文中的优化过程主要是Passport部分）；会员(在北京，负责赠送会员，保证权益生效）</p>
<p>在双11活动之前，Passport的绑定账号功能一直在运行，只是没有碰到过大促销带来的挑战</p>
</blockquote>
<hr>
<p>整个过程分为两大块：</p>
<ol>
<li>整个系统级别，包括网络和依赖服务的性能等，多从整个系统视角分析问题；</li>
<li>但服务器内部的优化过程，将CPU从si/sy围赶us，然后在us从代码级别一举全歼。</li>
</ol>
<p>系统级别都是最容易被忽视但是成效最明显的，代码层面都是很细致的力气活。</p>
<h2 id="会员部分的架构改造"><a href="#会员部分的架构改造" class="headerlink" title="会员部分的架构改造"></a>会员部分的架构改造</h2><ul>
<li>接入中间件DRDS，让优酷的数据库支持拆分，分解MySQL压力</li>
<li>接入中间件vipserver来支持负载均衡</li>
<li>接入集团DRC来保障数据的高可用</li>
<li>对业务进行改造支持Amazon的全链路压测</li>
</ul>
<h2 id="主要的压测过程"><a href="#主要的压测过程" class="headerlink" title="主要的压测过程"></a>主要的压测过程</h2><p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/6b24a854d91aba4dcdbd4f0155683d93.png" alt="screenshot.png"></p>
<p><strong>上图是压测过程中主要的阶段中问题和改进,主要的问题和优化过程如下：</strong></p>
<pre><code>- docker bridge网络性能问题和网络中断si不均衡    (优化后：500-&gt;1000TPS)
- 短连接导致的local port不够                   (优化后：1000-3000TPS)
- 生产环境snat单核导致的网络延时增大             (优化后生产环境能达到测试环境的3000TPS)
- Spring MVC Path带来的过高的CPU消耗           (优化后：3000-&gt;4200TPS)
- 其他业务代码的优化(比如异常、agent等)          (优化后：4200-&gt;5400TPS)
</code></pre><p><strong>优化过程中碰到的比如淘宝api调用次数限流等一些业务原因就不列出来了</strong></p>
<hr>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>由于用户进来后先要登录并且绑定账号，实际压力先到Passport部分，在这个过程中最开始单机TPS只能到500，经过N轮优化后基本能达到5400 TPS，下面主要是阐述这个优化过程</p>
<h2 id="Passport部分的压力"><a href="#Passport部分的压力" class="headerlink" title="Passport部分的压力"></a>Passport部分的压力</h2><h3 id="Passport-核心服务分两个："><a href="#Passport-核心服务分两个：" class="headerlink" title="Passport 核心服务分两个："></a>Passport 核心服务分两个：</h3><ul>
<li>Login              主要处理登录请求</li>
<li>userservice    处理登录后的业务逻辑，比如将优酷账号和淘宝账号绑定</li>
</ul>
<p>为了更好地利用资源每台物理加上部署三个docker 容器，跑在不同的端口上(8081、8082、8083），通过bridge网络来互相通讯</p>
<h3 id="Passport机器大致结构"><a href="#Passport机器大致结构" class="headerlink" title="Passport机器大致结构"></a>Passport机器大致结构</h3><p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/b509b30218dd22e03149985cf5e15f8e.png" alt="screenshot.png"></p>
<!--这里的500 TPS到5400 TPS是指登录和将优酷账号和淘宝账号绑定的TPS，也是促销活动主要的瓶颈-->
<h3 id="userservice服务网络相关的各种问题"><a href="#userservice服务网络相关的各种问题" class="headerlink" title="userservice服务网络相关的各种问题"></a>userservice服务网络相关的各种问题</h3><hr>
<h4 id="太多SocketConnect异常-如上图）"><a href="#太多SocketConnect异常-如上图）" class="headerlink" title="太多SocketConnect异常(如上图）"></a>太多SocketConnect异常(如上图）</h4><p>在userservice机器上通过netstat也能看到大量的SYN_SENT状态，如下图：<br><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/99bf952b880f17243953da790ff0e710.png" alt="image.png"></p>
<h4 id="因为docker-bridge通过nat来实现，尝试去掉docker，让tomcat直接跑在物理机上"><a href="#因为docker-bridge通过nat来实现，尝试去掉docker，让tomcat直接跑在物理机上" class="headerlink" title="因为docker bridge通过nat来实现，尝试去掉docker，让tomcat直接跑在物理机上"></a>因为docker bridge通过nat来实现，尝试去掉docker，让tomcat直接跑在物理机上</h4><p>这时SocketConnect异常不再出现<br><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/6ed62fd6b50ad2785e5b57687d95ad6e.png" alt="image.png"></p>
<h4 id="从新梳理一下网络流程"><a href="#从新梳理一下网络流程" class="headerlink" title="从新梳理一下网络流程"></a>从新梳理一下网络流程</h4><p>docker(bridge)—-短连接—&gt;访问淘宝API(淘宝open api只能短连接访问），性能差，cpu都花在si上； </p>
<p>如果 docker(bridge)—-长连接到宿主机的某个代理上(比如haproxy）—–短连接—&gt;访问淘宝API， 性能就能好一点。问题可能是短连接放大了Docker bridge网络的性能损耗</p>
<h4 id="当时看到的cpu-si非常高，截图如下："><a href="#当时看到的cpu-si非常高，截图如下：" class="headerlink" title="当时看到的cpu si非常高，截图如下："></a>当时看到的cpu si非常高，截图如下：</h4><p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/4c1eff0f925f59977e2557acff5cf03b.png" alt="image.png"></p>
<p>去掉Docker后，性能有所提升，继续通过perf top看到内核态寻找可用的Local Port消耗了比较多的CPU，gif动态截图如下(可以点击看高清大图）：</p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/fff502ca73e3112e585560ffe4a4dbf1.gif" alt="perf-top-netLocalPort-issue.gif"></p>
<p><strong>注意图中ipv6_rcv_saddr_equal和inet_csk_get_port 总共占了30%的CPU</strong> (系统态的CPU使用率高意味着共享资源有竞争或者I/O设备之间有大量的交互。)</p>
<p><strong>一般来说一台机器默认配置的可用 Local Port 3万多个，如果是短连接的话，一个连接释放后默认需要60秒回收，30000/60 =500 这是大概的理论TPS值【这里只考虑连同一个server IP:port 的时候】</strong></p>
<p>这500的tps算是一个老中医的经验。不过有些系统调整过Local Port取值范围，比如从1024到65534，那么这个tps上限就是1000附近。</p>
<p>同时观察这个时候CPU的主要花在sy上，最理想肯定是希望CPU主要用在us上，截图如下：<br><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/05703c168e63e96821ea9f921d83712b.png" alt="image.png"></p>
<p><strong>规则：性能优化要先把CPU从SI、SY上的消耗赶到US上去(通过架构、系统配置）；然后提升 US CPU的效率(代码级别的优化）</strong></p>
<p>sy占用了30-50%的CPU，这太不科学了，同时通过 netstat 分析连接状态，确实看到很多TIME_WAIT：<br><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/2ae2cb8b0cb324b68ca22c48c019e029.png" alt="localportissue-time-wait.png"></p>
<p><strong>cpu要花在us上，这部分才是我们代码吃掉的</strong></p>
<p><strong><em>于是让PE修改了tcp相关参数：降低 tcp_max_tw_buckets和开启tcp_tw_reuse，这个时候TPS能从1000提升到3000</em></strong></p>
<p>鼓掌，赶紧休息，迎接双11啊</p>
<h2 id="测试环境优化到3000-TPS后上线继续压测"><a href="#测试环境优化到3000-TPS后上线继续压测" class="headerlink" title="测试环境优化到3000 TPS后上线继续压测"></a>测试环境优化到3000 TPS后上线继续压测</h2><p><strong>居然性能又回到了500，太沮丧了</strong>，其实最开始账号绑定慢，Passport这边就怀疑taobao api是不是在大压力下不稳定，一般都是认为自己没问题，有问题的一定是对方。我不觉得这有什么问题，要是知道自己有什么问题不早就优化掉了，但是这里缺乏证据支撑，也就是如果你觉得自己没有问题或者问题在对方，一定要拿出证据来(有证据那么大家可以就证据来讨论，而不是互相苍白地推诿）。</p>
<p>这个时候Passport更加理直气壮啊，好不容易在测试环境优化到3000，怎么一调taobao api就掉到500呢，这么点压力你们就扛不住啊。 但是taobao api那边给出调用数据都是1ms以内就返回了(alimonitor监控图表–拿证据说话）。</p>
<p>看到alimonitor给出的api响应时间图表后，我开始怀疑从优酷的机器到淘宝的机器中间链路上有瓶颈，但是需要设计方案来证明这个问题在链路上，要不各个环节都会认为自己没有问题的，问题就会卡死。但是当时Passport的开发也只能拿到Login和Userservice这两组机器的权限，中间的负载均衡、交换机都没有权限接触到。</p>
<p>在没有证据的情况下，肯定机房、PE配合你排查的欲望基本是没有的(被坑过很多回啊，你说我的问题，结果几天配合排查下来发现还是你程序的问题，凭什么我要每次都陪你玩？），所以我要给出证明问题出现在网络链路上，然后拿着这个证据跟网络的同学一起排查。</p>
<p>讲到这里我禁不住要插一句，在出现问题的时候，都认为自己没有问题这是正常反应，毕竟程序是看不见的，好多意料之外逻辑考虑不周全也是常见的，出现问题按照自己的逻辑自查的时候还是没有跳出之前的逻辑所以发现不了问题。但是好的程序员在问题的前面会尝试用各种手段去证明问题在哪里，而不是复读机一样我的逻辑是这样的，不可能出问题的。即使目的是证明问题在对方，只要能给出明确的证据都是负责任的，拿着证据才能理直气壮地说自己没有问题和干净地甩锅。</p>
<p><strong>在尝试过tcpdump抓包、ping等各种手段分析后，设计了场景证明问题在中间链路上。</strong></p>
<h3 id="设计如下三个场景证明问题在中间链路上："><a href="#设计如下三个场景证明问题在中间链路上：" class="headerlink" title="设计如下三个场景证明问题在中间链路上："></a>设计如下三个场景证明问题在中间链路上：</h3><ol>
<li>压测的时候在userservice ping 淘宝的机器；</li>
<li>将一台userservice机器从负载均衡上拿下来(没有压力），ping 淘宝的机器；</li>
<li>从公网上非优酷的机器 ping 淘宝的机器；</li>
</ol>
<p>这个时候奇怪的事情发现了，压力一上来<strong>场景1、2</strong>的两台机器ping淘宝的rt都从30ms上升到100-150ms，<strong>场景1</strong> 的rt上升可以理解，但是<strong>场景2</strong>的rt上升不应该，同时<strong>场景3</strong>中ping淘宝在压力测试的情况下rt一直很稳定(说明压力下淘宝的机器没有问题），到此确认问题在优酷到淘宝机房的链路上有瓶颈，而且问题在优酷机房出口扛不住这么大的压力。于是从上海Passport的团队找到北京Passport的PE团队，确认在优酷调用taobao api的出口上使用了snat，PE到snat机器上看到snat只能使用单核，而且对应的核早就100%的CPU了，因为之前一直没有这么大的压力所以这个问题一直存在只是没有被发现。</p>
<p><strong>于是PE去掉snat，再压的话 TPS稳定在3000左右</strong></p>
<hr>
<h2 id="到这里结束了吗？-从3000到5400TPS"><a href="#到这里结束了吗？-从3000到5400TPS" class="headerlink" title="到这里结束了吗？ 从3000到5400TPS"></a>到这里结束了吗？ 从3000到5400TPS</h2><p>优化到3000TPS的整个过程没有修改业务代码，只是通过修改系统配置、结构非常有效地把TPS提升了6倍，对于优化来说这个过程是最轻松，性价比也是非常高的。实际到这个时候也临近双11封网了，最终通过计算(机器数量*单机TPS）完全可以抗住双11的压力，所以最终双11运行的版本就是这样的。 但是有工匠精神的工程师是不会轻易放过这么好的优化场景和环境的(基线、机器、代码、工具都具备配套好了）</p>
<p><strong>优化完环境问题后，3000TPS能把CPU US跑上去，于是再对业务代码进行优化也是可行的了</strong>。</p>
<h3 id="进一步挖掘代码中的优化空间"><a href="#进一步挖掘代码中的优化空间" class="headerlink" title="进一步挖掘代码中的优化空间"></a>进一步挖掘代码中的优化空间</h3><p>双11前的这段封网其实是比较无聊的，于是和Passport的开发同学们一起挖掘代码中的可以优化的部分。这个过程中使用到的主要工具是这三个：火焰图、perf、perf-map-java。相关链接：<a href="http://www.brendangregg.com/perf.html" target="_blank" rel="external">http://www.brendangregg.com/perf.html</a> ; <a href="https://github.com/jrudolph/perf-map-agent" target="_blank" rel="external">https://github.com/jrudolph/perf-map-agent</a></p>
<h3 id="通过Perf发现的一个SpringMVC-的性能问题"><a href="#通过Perf发现的一个SpringMVC-的性能问题" class="headerlink" title="通过Perf发现的一个SpringMVC 的性能问题"></a>通过Perf发现的一个SpringMVC 的性能问题</h3><p>这个问题具体参考我之前发表的优化文章<a href="http://www.atatech.org/articles/65232" title="spring mvc issue" target="_blank" rel="external">http://www.atatech.org/articles/65232</a> 。 主要是通过火焰图发现spring mapping path消耗了过多CPU的性能问题，CPU热点都在methodMapping相关部分，于是修改代码去掉spring中的methodMapping解析后性能提升了40%，TPS能从3000提升到4200.</p>
<h3 id="著名的fillInStackTrace导致的性能问题"><a href="#著名的fillInStackTrace导致的性能问题" class="headerlink" title="著名的fillInStackTrace导致的性能问题"></a>著名的fillInStackTrace导致的性能问题</h3><p>代码中的第二个问题是我们程序中很多异常(fillInStackTrace），实际业务上没有这么多错误，应该是一些不重要的异常，不会影响结果，但是异常频率很高，对这种我们可以找到触发的地方，catch住，然后不要抛出去(也就是别触发fillInStackTrace)，打印一行error日志就行，这块也能省出10%的CPU，对应到TPS也有几百的提升。</p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/36ef4b16c3c400abf6eb7e6b0fbb2f58.png" alt="screenshot.png"></p>
<p>部分触发fillInStackTrace的场景和具体代码行(点击看高清大图）：<br><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/7eb2cbb4afc2c7d7007c35304c95342a.png" alt="screenshot.png"></p>
<p>对应的火焰图(点击看高清大图）：<br><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/894bd736dd03060e89e3fa49cc98ae5e.png" alt="screenshot.png"></p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/2bb7395a2cc6833c9c7587b38402a301.png" alt="screenshot.png"></p>
<h3 id="解析useragent-代码部分的性能问题"><a href="#解析useragent-代码部分的性能问题" class="headerlink" title="解析useragent 代码部分的性能问题"></a>解析useragent 代码部分的性能问题</h3><p>整个useragent调用堆栈和cpu占用情况，做了个汇总(useragent不启用TPS能从4700提升到5400）<br><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/8a4a97cb74724b8baa3b90072a1914e0.png" alt="screenshot.png"></p>
<p>实际火焰图中比较分散：<br><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/afacc681a9550cd087838c2383be54c8.png" alt="screenshot.png"></p>
<p><strong>最终通过对代码的优化勉勉强强将TPS从3000提升到了5400(太不容易了，改代码过程太辛苦，不如改配置来得快）</strong></p>
<p>优化代码后压测tps可以跑到5400，截图：</p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/38bb043c85c7b50007609484c7bf5698.png" alt="image.png"></p>
<h2 id="最后再次总结整个压测过程的问题和优化历程"><a href="#最后再次总结整个压测过程的问题和优化历程" class="headerlink" title="最后再次总结整个压测过程的问题和优化历程"></a>最后再次总结整个压测过程的问题和优化历程</h2><pre><code>- docker bridge网络性能问题和网络中断si不均衡    (优化后：500-&gt;1000TPS)
- 短连接导致的local port不够                   (优化后：1000-3000TPS）
- 生产环境snat单核导致的网络延时增大             (优化后能达到测试环境的3000TPS）
- Spring MVC Path带来的过高的CPU消耗           (优化后：3000-&gt;4200TPS)
- 其他业务代码的优化(比如异常、agent等）         (优化后：4200-&gt;5400TPS)
</code></pre><hr>
<h5 id="整个过程得到了淘宝API、优酷会员、优酷Passport、网络、蚂蚁等众多同学的帮助，本来是计划去上海跟Passport的同学一起复盘然后再写这篇文章的，结果一直未能成行，请原谅我拖延到现在才把大家一起辛苦工作的结果整理出来，可能过程中的数据会有一些记忆上的小错误。"><a href="#整个过程得到了淘宝API、优酷会员、优酷Passport、网络、蚂蚁等众多同学的帮助，本来是计划去上海跟Passport的同学一起复盘然后再写这篇文章的，结果一直未能成行，请原谅我拖延到现在才把大家一起辛苦工作的结果整理出来，可能过程中的数据会有一些记忆上的小错误。" class="headerlink" title="整个过程得到了淘宝API、优酷会员、优酷Passport、网络、蚂蚁等众多同学的帮助，本来是计划去上海跟Passport的同学一起复盘然后再写这篇文章的，结果一直未能成行，请原谅我拖延到现在才把大家一起辛苦工作的结果整理出来，可能过程中的数据会有一些记忆上的小错误。"></a>整个过程得到了淘宝API、优酷会员、优酷Passport、网络、蚂蚁等众多同学的帮助，本来是计划去上海跟Passport的同学一起复盘然后再写这篇文章的，结果一直未能成行，请原谅我拖延到现在才把大家一起辛苦工作的结果整理出来，可能过程中的数据会有一些记忆上的小错误。</h5>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;10-倍性能提升全过程–优酷账号绑定淘宝账号的TPS从500到5400的优化历程&quot;&gt;&lt;a href=&quot;#10-倍性能提升全过程–优酷账号绑定淘宝账号的TPS从500到5400的优化历程&quot; class=&quot;headerlink&quot; title=&quot;10+倍性能提升全过程–
    
    </summary>
    
      <category term="performance" scheme="http://yoursite.com/categories/performance/"/>
    
    
      <category term="performance" scheme="http://yoursite.com/tags/performance/"/>
    
      <category term="tuning" scheme="http://yoursite.com/tags/tuning/"/>
    
  </entry>
  
  <entry>
    <title>10+倍性能提升全过程</title>
    <link href="http://yoursite.com/2020/02/17/10+%E5%80%8D%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87%E5%85%A8%E8%BF%87%E7%A8%8B/"/>
    <id>http://yoursite.com/2020/02/17/10+倍性能提升全过程/</id>
    <published>2020-02-17T09:30:03.000Z</published>
    <updated>2020-05-15T04:58:31.892Z</updated>
    
    <content type="html"><![CDATA[<h1 id="10-倍性能提升全过程–优酷账号绑定淘宝账号的TPS从500到5400的优化历程"><a href="#10-倍性能提升全过程–优酷账号绑定淘宝账号的TPS从500到5400的优化历程" class="headerlink" title="10+倍性能提升全过程–优酷账号绑定淘宝账号的TPS从500到5400的优化历程"></a>10+倍性能提升全过程–优酷账号绑定淘宝账号的TPS从500到5400的优化历程</h1><h2 id="背景说明"><a href="#背景说明" class="headerlink" title="背景说明"></a>背景说明</h2><blockquote>
<p>2016年的双11在淘宝上买买买的时候，天猫和优酷土豆一起做了联合促销，在天猫双11当天购物满XXX元就赠送优酷会员，这个过程需要用户在优酷侧绑定淘宝账号(登录优酷、提供淘宝账号，优酷调用淘宝API实现两个账号绑定）和赠送会员并让会员权益生效(看收费影片、免广告等等）</p>
<p>这里涉及到优酷的两个部门：Passport(在上海，负责登录、绑定账号，下文中的优化过程主要是Passport部分）；会员(在北京，负责赠送会员，保证权益生效）</p>
<p>在双11活动之前，Passport的绑定账号功能一直在运行，只是没有碰到过大促销带来的挑战</p>
</blockquote>
<hr>
<p>整个过程分为两大块：</p>
<ol>
<li>整个系统级别，包括网络和依赖服务的性能等，多从整个系统视角分析问题；</li>
<li>但服务器内部的优化过程，将CPU从si/sy围赶us，然后在us从代码级别一举全歼。</li>
</ol>
<p>系统级别都是最容易被忽视但是成效最明显的，代码层面都是很细致的力气活。</p>
<p>整个过程都是在对业务和架构不是非常了解的情况下做出的。</p>
<h2 id="会员部分的架构改造"><a href="#会员部分的架构改造" class="headerlink" title="会员部分的架构改造"></a>会员部分的架构改造</h2><ul>
<li>接入中间件DRDS，让优酷的数据库支持拆分，分解MySQL压力</li>
<li>接入中间件vipserver来支持负载均衡</li>
<li>接入集团DRC来保障数据的高可用</li>
<li>对业务进行改造支持Amazon的全链路压测</li>
</ul>
<h2 id="主要的压测过程"><a href="#主要的压测过程" class="headerlink" title="主要的压测过程"></a>主要的压测过程</h2><p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/6b24a854d91aba4dcdbd4f0155683d93.png" alt="screenshot.png"></p>
<p><strong>上图是压测过程中主要的阶段中问题和改进,主要的问题和优化过程如下：</strong></p>
<pre><code>- docker bridge网络性能问题和网络中断si不均衡    (优化后：500-&gt;1000TPS)
- 短连接导致的local port不够                   (优化后：1000-3000TPS)
- 生产环境snat单核导致的网络延时增大             (优化后生产环境能达到测试环境的3000TPS)
- Spring MVC Path带来的过高的CPU消耗           (优化后：3000-&gt;4200TPS)
- 其他业务代码的优化(比如异常、agent等)          (优化后：4200-&gt;5400TPS)
</code></pre><p><strong>优化过程中碰到的比如淘宝api调用次数限流等一些业务原因就不列出来了</strong></p>
<hr>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>由于用户进来后先要登录并且绑定账号，实际压力先到Passport部分，在这个过程中最开始单机TPS只能到500，经过N轮优化后基本能达到5400 TPS，下面主要是阐述这个优化过程</p>
<h2 id="Passport部分的压力"><a href="#Passport部分的压力" class="headerlink" title="Passport部分的压力"></a>Passport部分的压力</h2><h3 id="Passport-核心服务分两个："><a href="#Passport-核心服务分两个：" class="headerlink" title="Passport 核心服务分两个："></a>Passport 核心服务分两个：</h3><ul>
<li>Login              主要处理登录请求</li>
<li>userservice    处理登录后的业务逻辑，比如将优酷账号和淘宝账号绑定</li>
</ul>
<p>为了更好地利用资源每台物理加上部署三个docker 容器，跑在不同的端口上(8081、8082、8083），通过bridge网络来互相通讯</p>
<h3 id="Passport机器大致结构"><a href="#Passport机器大致结构" class="headerlink" title="Passport机器大致结构"></a>Passport机器大致结构</h3><p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/b509b30218dd22e03149985cf5e15f8e.png" alt="screenshot.png"></p>
<!--这里的500 TPS到5400 TPS是指登录和将优酷账号和淘宝账号绑定的TPS，也是促销活动主要的瓶颈-->
<h3 id="userservice服务网络相关的各种问题"><a href="#userservice服务网络相关的各种问题" class="headerlink" title="userservice服务网络相关的各种问题"></a>userservice服务网络相关的各种问题</h3><hr>
<h4 id="太多SocketConnect异常-如上图）"><a href="#太多SocketConnect异常-如上图）" class="headerlink" title="太多SocketConnect异常(如上图）"></a>太多SocketConnect异常(如上图）</h4><p>在userservice机器上通过netstat也能看到大量的SYN_SENT状态，如下图：<br><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/99bf952b880f17243953da790ff0e710.png" alt="image.png"></p>
<h4 id="因为docker-bridge通过nat来实现，尝试去掉docker，让tomcat直接跑在物理机上"><a href="#因为docker-bridge通过nat来实现，尝试去掉docker，让tomcat直接跑在物理机上" class="headerlink" title="因为docker bridge通过nat来实现，尝试去掉docker，让tomcat直接跑在物理机上"></a>因为docker bridge通过nat来实现，尝试去掉docker，让tomcat直接跑在物理机上</h4><p>这时SocketConnect异常不再出现<br><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/6ed62fd6b50ad2785e5b57687d95ad6e.png" alt="image.png"></p>
<h4 id="从新梳理一下网络流程"><a href="#从新梳理一下网络流程" class="headerlink" title="从新梳理一下网络流程"></a>从新梳理一下网络流程</h4><p>docker(bridge)—-短连接—&gt;访问淘宝API(淘宝open api只能短连接访问），性能差，cpu都花在si上； </p>
<p>如果 docker(bridge)—-长连接到宿主机的某个代理上(比如haproxy）—–短连接—&gt;访问淘宝API， 性能就能好一点。问题可能是短连接放大了Docker bridge网络的性能损耗</p>
<h4 id="当时看到的cpu-si非常高，截图如下："><a href="#当时看到的cpu-si非常高，截图如下：" class="headerlink" title="当时看到的cpu si非常高，截图如下："></a>当时看到的cpu si非常高，截图如下：</h4><p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/4c1eff0f925f59977e2557acff5cf03b.png" alt="image.png"></p>
<p>去掉Docker后，性能有所提升，继续通过perf top看到内核态寻找可用的Local Port消耗了比较多的CPU，gif动态截图如下(可以点击看高清大图）：</p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/fff502ca73e3112e585560ffe4a4dbf1.gif" alt="perf-top-netLocalPort-issue.gif"></p>
<p><strong>注意图中ipv6_rcv_saddr_equal和inet_csk_get_port 总共占了30%的CPU</strong> (系统态的CPU使用率高意味着共享资源有竞争或者I/O设备之间有大量的交互。)</p>
<p><strong>一般来说一台机器默认配置的可用 Local Port 3万多个，如果是短连接的话，一个连接释放后默认需要60秒回收，30000/60 =500 这是大概的理论TPS值【这里只考虑连同一个server IP:port 的时候】</strong></p>
<p>这500的tps算是一个老中医的经验。不过有些系统调整过Local Port取值范围，比如从1024到65534，那么这个tps上限就是1000附近。</p>
<p>同时观察这个时候CPU的主要花在sy上，最理想肯定是希望CPU主要用在us上，截图如下：<br><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/05703c168e63e96821ea9f921d83712b.png" alt="image.png"></p>
<p><strong>规则：性能优化要先把CPU从SI、SY上的消耗赶到US上去(通过架构、系统配置）；然后提升 US CPU的效率(代码级别的优化）</strong></p>
<p>sy占用了30-50%的CPU，这太不科学了，同时通过 netstat 分析连接状态，确实看到很多TIME_WAIT：<br><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/2ae2cb8b0cb324b68ca22c48c019e029.png" alt="localportissue-time-wait.png"></p>
<p><strong>cpu要花在us上，这部分才是我们代码吃掉的</strong></p>
<p><strong><em>于是让PE修改了tcp相关参数：降低 tcp_max_tw_buckets和开启tcp_tw_reuse，这个时候TPS能从1000提升到3000</em></strong></p>
<p>鼓掌，赶紧休息，迎接双11啊</p>
<h2 id="测试环境优化到3000-TPS后上线继续压测"><a href="#测试环境优化到3000-TPS后上线继续压测" class="headerlink" title="测试环境优化到3000 TPS后上线继续压测"></a>测试环境优化到3000 TPS后上线继续压测</h2><p><strong>居然性能又回到了500，太沮丧了</strong>，其实最开始账号绑定慢，Passport这边就怀疑taobao api是不是在大压力下不稳定，一般都是认为自己没问题，有问题的一定是对方。我不觉得这有什么问题，要是知道自己有什么问题不早就优化掉了，但是这里缺乏证据支撑，也就是如果你觉得自己没有问题或者问题在对方，一定要拿出证据来(有证据那么大家可以就证据来讨论，而不是互相苍白地推诿）。</p>
<p>这个时候Passport更加理直气壮啊，好不容易在测试环境优化到3000，怎么一调taobao api就掉到500呢，这么点压力你们就扛不住啊。 但是taobao api那边给出调用数据都是1ms以内就返回了(alimonitor监控图表–拿证据说话）。</p>
<p>看到alimonitor给出的api响应时间图表后，我开始怀疑从优酷的机器到淘宝的机器中间链路上有瓶颈，但是需要设计方案来证明这个问题在链路上，要不各个环节都会认为自己没有问题的，问题就会卡死。但是当时Passport的开发也只能拿到Login和Userservice这两组机器的权限，中间的负载均衡、交换机都没有权限接触到。</p>
<p>在没有证据的情况下，肯定机房、PE配合你排查的欲望基本是没有的(被坑过很多回啊，你说我的问题，结果几天配合排查下来发现还是你程序的问题，凭什么我要每次都陪你玩？），所以我要给出证明问题出现在网络链路上，然后拿着这个证据跟网络的同学一起排查。</p>
<p>讲到这里我禁不住要插一句，在出现问题的时候，都认为自己没有问题这是正常反应，毕竟程序是看不见的，好多意料之外逻辑考虑不周全也是常见的，出现问题按照自己的逻辑自查的时候还是没有跳出之前的逻辑所以发现不了问题。但是好的程序员在问题的前面会尝试用各种手段去证明问题在哪里，而不是复读机一样我的逻辑是这样的，不可能出问题的。即使目的是证明问题在对方，只要能给出明确的证据都是负责任的，拿着证据才能理直气壮地说自己没有问题和干净地甩锅。</p>
<p><strong>在尝试过tcpdump抓包、ping等各种手段分析后，设计了场景证明问题在中间链路上。</strong></p>
<h3 id="设计如下三个场景证明问题在中间链路上："><a href="#设计如下三个场景证明问题在中间链路上：" class="headerlink" title="设计如下三个场景证明问题在中间链路上："></a>设计如下三个场景证明问题在中间链路上：</h3><ol>
<li>压测的时候在userservice ping 淘宝的机器；</li>
<li>将一台userservice机器从负载均衡上拿下来(没有压力），ping 淘宝的机器；</li>
<li>从公网上非优酷的机器 ping 淘宝的机器；</li>
</ol>
<p>这个时候奇怪的事情发现了，压力一上来<strong>场景1、2</strong>的两台机器ping淘宝的rt都从30ms上升到100-150ms，<strong>场景1</strong> 的rt上升可以理解，但是<strong>场景2</strong>的rt上升不应该，同时<strong>场景3</strong>中ping淘宝在压力测试的情况下rt一直很稳定(说明压力下淘宝的机器没有问题），到此确认问题在优酷到淘宝机房的链路上有瓶颈，而且问题在优酷机房出口扛不住这么大的压力。于是从上海Passport的团队找到北京Passport的PE团队，确认在优酷调用taobao api的出口上使用了snat，PE到snat机器上看到snat只能使用单核，而且对应的核早就100%的CPU了，因为之前一直没有这么大的压力所以这个问题一直存在只是没有被发现。</p>
<p><strong>于是PE去掉snat，再压的话 TPS稳定在3000左右</strong></p>
<hr>
<h2 id="到这里结束了吗？-从3000到5400TPS"><a href="#到这里结束了吗？-从3000到5400TPS" class="headerlink" title="到这里结束了吗？ 从3000到5400TPS"></a>到这里结束了吗？ 从3000到5400TPS</h2><p>优化到3000TPS的整个过程没有修改业务代码，只是通过修改系统配置、结构非常有效地把TPS提升了6倍，对于优化来说这个过程是最轻松，性价比也是非常高的。实际到这个时候也临近双11封网了，最终通过计算(机器数量*单机TPS）完全可以抗住双11的压力，所以最终双11运行的版本就是这样的。 但是有工匠精神的工程师是不会轻易放过这么好的优化场景和环境的(基线、机器、代码、工具都具备配套好了）</p>
<p><strong>优化完环境问题后，3000TPS能把CPU US跑上去，于是再对业务代码进行优化也是可行的了</strong>。</p>
<h3 id="进一步挖掘代码中的优化空间"><a href="#进一步挖掘代码中的优化空间" class="headerlink" title="进一步挖掘代码中的优化空间"></a>进一步挖掘代码中的优化空间</h3><p>双11前的这段封网其实是比较无聊的，于是和Passport的开发同学们一起挖掘代码中的可以优化的部分。这个过程中使用到的主要工具是这三个：火焰图、perf、perf-map-java。相关链接：<a href="http://www.brendangregg.com/perf.html" target="_blank" rel="external">http://www.brendangregg.com/perf.html</a> ; <a href="https://github.com/jrudolph/perf-map-agent" target="_blank" rel="external">https://github.com/jrudolph/perf-map-agent</a></p>
<h3 id="通过Perf发现的一个SpringMVC-的性能问题"><a href="#通过Perf发现的一个SpringMVC-的性能问题" class="headerlink" title="通过Perf发现的一个SpringMVC 的性能问题"></a>通过Perf发现的一个SpringMVC 的性能问题</h3><p>这个问题具体参考我之前发表的优化文章<a href="http://www.atatech.org/articles/65232" title="spring mvc issue" target="_blank" rel="external">http://www.atatech.org/articles/65232</a> 。 主要是通过火焰图发现spring mapping path消耗了过多CPU的性能问题，CPU热点都在methodMapping相关部分，于是修改代码去掉spring中的methodMapping解析后性能提升了40%，TPS能从3000提升到4200.</p>
<h3 id="著名的fillInStackTrace导致的性能问题"><a href="#著名的fillInStackTrace导致的性能问题" class="headerlink" title="著名的fillInStackTrace导致的性能问题"></a>著名的fillInStackTrace导致的性能问题</h3><p>代码中的第二个问题是我们程序中很多异常(fillInStackTrace），实际业务上没有这么多错误，应该是一些不重要的异常，不会影响结果，但是异常频率很高，对这种我们可以找到触发的地方，catch住，然后不要抛出去(也就是别触发fillInStackTrace)，打印一行error日志就行，这块也能省出10%的CPU，对应到TPS也有几百的提升。</p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/36ef4b16c3c400abf6eb7e6b0fbb2f58.png" alt="screenshot.png"></p>
<p>部分触发fillInStackTrace的场景和具体代码行(点击看高清大图）：<br><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/7eb2cbb4afc2c7d7007c35304c95342a.png" alt="screenshot.png"></p>
<p>对应的火焰图(点击看高清大图）：<br><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/894bd736dd03060e89e3fa49cc98ae5e.png" alt="screenshot.png"></p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/2bb7395a2cc6833c9c7587b38402a301.png" alt="screenshot.png"></p>
<h3 id="解析useragent-代码部分的性能问题"><a href="#解析useragent-代码部分的性能问题" class="headerlink" title="解析useragent 代码部分的性能问题"></a>解析useragent 代码部分的性能问题</h3><p>整个useragent调用堆栈和cpu占用情况，做了个汇总(useragent不启用TPS能从4700提升到5400）<br><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/8a4a97cb74724b8baa3b90072a1914e0.png" alt="screenshot.png"></p>
<p>实际火焰图中比较分散：<br><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/afacc681a9550cd087838c2383be54c8.png" alt="screenshot.png"></p>
<p><strong>最终通过对代码的优化勉勉强强将TPS从3000提升到了5400(太不容易了，改代码过程太辛苦，不如改配置来得快）</strong></p>
<p>优化代码后压测tps可以跑到5400，截图：</p>
<p><img src="http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/38bb043c85c7b50007609484c7bf5698.png" alt="image.png"></p>
<h2 id="最后再次总结整个压测过程的问题和优化历程"><a href="#最后再次总结整个压测过程的问题和优化历程" class="headerlink" title="最后再次总结整个压测过程的问题和优化历程"></a>最后再次总结整个压测过程的问题和优化历程</h2><pre><code>- docker bridge网络性能问题和网络中断si不均衡    (优化后：500-&gt;1000TPS)
- 短连接导致的local port不够                   (优化后：1000-3000TPS）
- 生产环境snat单核导致的网络延时增大             (优化后能达到测试环境的3000TPS）
- Spring MVC Path带来的过高的CPU消耗           (优化后：3000-&gt;4200TPS)
- 其他业务代码的优化(比如异常、agent等）         (优化后：4200-&gt;5400TPS)
</code></pre><hr>
<h5 id="整个过程得到了淘宝API、优酷会员、优酷Passport、网络、蚂蚁等众多同学的帮助，本来是计划去上海跟Passport的同学一起复盘然后再写这篇文章的，结果一直未能成行，请原谅我拖延到现在才把大家一起辛苦工作的结果整理出来，可能过程中的数据会有一些记忆上的小错误。"><a href="#整个过程得到了淘宝API、优酷会员、优酷Passport、网络、蚂蚁等众多同学的帮助，本来是计划去上海跟Passport的同学一起复盘然后再写这篇文章的，结果一直未能成行，请原谅我拖延到现在才把大家一起辛苦工作的结果整理出来，可能过程中的数据会有一些记忆上的小错误。" class="headerlink" title="整个过程得到了淘宝API、优酷会员、优酷Passport、网络、蚂蚁等众多同学的帮助，本来是计划去上海跟Passport的同学一起复盘然后再写这篇文章的，结果一直未能成行，请原谅我拖延到现在才把大家一起辛苦工作的结果整理出来，可能过程中的数据会有一些记忆上的小错误。"></a>整个过程得到了淘宝API、优酷会员、优酷Passport、网络、蚂蚁等众多同学的帮助，本来是计划去上海跟Passport的同学一起复盘然后再写这篇文章的，结果一直未能成行，请原谅我拖延到现在才把大家一起辛苦工作的结果整理出来，可能过程中的数据会有一些记忆上的小错误。</h5>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;10-倍性能提升全过程–优酷账号绑定淘宝账号的TPS从500到5400的优化历程&quot;&gt;&lt;a href=&quot;#10-倍性能提升全过程–优酷账号绑定淘宝账号的TPS从500到5400的优化历程&quot; class=&quot;headerlink&quot; title=&quot;10+倍性能提升全过程–
    
    </summary>
    
      <category term="performance" scheme="http://yoursite.com/categories/performance/"/>
    
    
      <category term="performance" scheme="http://yoursite.com/tags/performance/"/>
    
      <category term="tuning" scheme="http://yoursite.com/tags/tuning/"/>
    
  </entry>
  
  <entry>
    <title>TCP相关参数解释</title>
    <link href="http://yoursite.com/2020/01/26/TCP%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0%E8%A7%A3%E9%87%8A/"/>
    <id>http://yoursite.com/2020/01/26/TCP相关参数解释/</id>
    <published>2020-01-26T09:30:03.000Z</published>
    <updated>2020-07-16T06:27:33.953Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TCP相关参数解释"><a href="#TCP相关参数解释" class="headerlink" title="TCP相关参数解释"></a>TCP相关参数解释</h1><p>读懂TCP参数前得先搞清楚内核中出现的HZ、Tick、Jiffies三个值是什么意思</p>
<h2 id="HZ"><a href="#HZ" class="headerlink" title="HZ"></a>HZ</h2><p>它可以理解为1s，所以120*HZ就是120秒，HZ/5就是200ms。</p>
<p>HZ表示CPU一秒种发出多少次时间中断–IRQ-0，Linux中通常用HZ来做时间片的计算（<a href="http://blog.csdn.net/bdc995/article/details/4144031" target="_blank" rel="external">参考</a>）。</p>
<p>这个值在内核编译的时候可设定100、250、300或1000，一般设置的是1000</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#cat /boot/config-`uname -r` |grep &apos;CONFIG_HZ=&apos;</div><div class="line">CONFIG_HZ=1000 //一般默认1000, Linux核心每隔固定周期会发出timer interrupt (IRQ 0)，HZ是用来定义</div><div class="line">每一秒有几次timer interrupts。举例来说，HZ为1000，代表每秒有1000次timer interrupts</div></pre></td></tr></table></figure>
<p>HZ的设定：<br>#make menuconfig<br>processor type and features—&gt;Timer frequency (250 HZ)—&gt;</p>
<p>HZ的不同值会影响timer （节拍）中断的频率</p>
<h2 id="Tick"><a href="#Tick" class="headerlink" title="Tick"></a>Tick</h2><p>Tick是HZ的倒数，意即timer interrupt每发生一次中断的时间。如HZ为250时，tick为4毫秒(millisecond)。</p>
<h2 id="Jiffies"><a href="#Jiffies" class="headerlink" title="Jiffies"></a>Jiffies</h2><p>Jiffies为Linux核心变数(32位元变数，unsigned long)，它被用来纪录系统自开几以来，已经过多少的tick。每发生一次timer interrupt，Jiffies变数会被加一。值得注意的是，Jiffies于系统开机时，并非初始化成零，而是被设为-300*HZ (arch/i386/kernel/time.c)，即代表系统于开机五分钟后，jiffies便会溢位。那溢出怎么办?事实上，Linux核心定义几个macro(timer_after、time_after_eq、time_before与time_before_eq)，即便是溢位，也能藉由这几个macro正确地取得jiffies的内容。</p>
<p>另外，80x86架构定义一个与jiffies相关的变数jiffies_64 ，此变数64位元，要等到此变数溢位可能要好几百万年。因此要等到溢位这刻发生应该很难吧。那如何经由jiffies_64取得jiffies呢?事实上，jiffies被对应至jiffies_64最低的32位元。因此，经由jiffies_64可以完全不理会溢位的问题便能取得jiffies。</p>
<h2 id="数据取自于4-19内核代码中的-include-net-tcp-h"><a href="#数据取自于4-19内核代码中的-include-net-tcp-h" class="headerlink" title="数据取自于4.19内核代码中的 include/net/tcp.h"></a>数据取自于4.19内核代码中的 include/net/tcp.h</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">//rto的定义，不让修改，到每个ip的rt都不一样，必须通过rtt计算所得 </div><div class="line">#define TCP_RTO_MAX     ((unsigned)(120*HZ))</div><div class="line">#define TCP_RTO_MIN     ((unsigned)(HZ/5)) //在rt很小的环境中计算下来RTO基本等于TCP_RTO_MIN，</div><div class="line"></div><div class="line">/* Maximal number of ACKs sent quickly to accelerate slow-start. */</div><div class="line">#define TCP_MAX_QUICKACKS       16U //默认前16个ack必须quick ack来加速慢启动</div><div class="line"></div><div class="line">//默认delay ack不能超过200ms</div><div class="line">#define TCP_DELACK_MAX  ((unsigned)(HZ/5))      /* maximal time to delay before sending an ACK */</div><div class="line">#if HZ &gt;= 100</div><div class="line">//默认 delay ack 40ms，不能修改和关闭</div><div class="line">#define TCP_DELACK_MIN  ((unsigned)(HZ/25))     /* minimal time to delay before sending an ACK */</div><div class="line">#define TCP_ATO_MIN     ((unsigned)(HZ/25))</div><div class="line">#else</div><div class="line">#define TCP_DELACK_MIN  4U</div><div class="line">#define TCP_ATO_MIN     4U</div><div class="line">#endif</div><div class="line"></div><div class="line">#define TCP_SYNQ_INTERVAL       (HZ/5)  /* Period of SYNACK timer */</div><div class="line">#define TCP_KEEPALIVE_TIME      (120*60*HZ)     /* two hours */</div><div class="line">#define TCP_KEEPALIVE_PROBES    9               /* Max of 9 keepalive probes    */</div><div class="line">#define TCP_KEEPALIVE_INTVL     (75*HZ)</div><div class="line"></div><div class="line">/* cwnd init 默认大小是10个拥塞窗口，也可以通过sysctl_tcp_init_cwnd来设置，要求内核编译的时候支持*/</div><div class="line">#if IS_ENABLED(CONFIG_TCP_INIT_CWND_PROC)</div><div class="line">extern u32 sysctl_tcp_init_cwnd;</div><div class="line">/* TCP_INIT_CWND is rvalue */</div><div class="line">#define TCP_INIT_CWND           (sysctl_tcp_init_cwnd + 0)</div><div class="line">#else</div><div class="line">/* TCP initial congestion window as per rfc6928 */</div><div class="line">#define TCP_INIT_CWND           10</div><div class="line">#endif</div><div class="line"></div><div class="line">/* Flags in tp-&gt;nonagle 默认nagle算法关闭的*/</div><div class="line">#define TCP_NAGLE_OFF           1       /* Nagle&apos;s algo is disabled */</div><div class="line">#define TCP_NAGLE_CORK          2       /* Socket is corked         */</div><div class="line">#define TCP_NAGLE_PUSH          4       /* Cork is overridden for already queued data */</div><div class="line"></div><div class="line"></div><div class="line">#define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT</div><div class="line">                                  * state, about 60 seconds     */</div><div class="line">                                  </div><div class="line">#define TCP_SYN_RETRIES  6      /* This is how many retries are done</div><div class="line">                                 * when active opening a connection.</div><div class="line">                                 * RFC1122 says the minimum retry MUST</div><div class="line">                                 * be at least 180secs.  Nevertheless</div><div class="line">                                 * this value is corresponding to</div><div class="line">                                 * 63secs of retransmission with the</div><div class="line">                                 * current initial RTO.</div><div class="line">                                 */</div><div class="line"></div><div class="line">#define TCP_SYNACK_RETRIES 5    /* This is how may retries are done</div><div class="line">                                 * when passive opening a connection.</div><div class="line">                                 * This is corresponding to 31secs of</div><div class="line">                                 * retransmission with the current</div><div class="line">                                 * initial RTO.</div><div class="line">                                 */</div></pre></td></tr></table></figure>
<p>即使RTT很小（比如0.8ms），但是因为RTO有下限，最小必须是200ms，所以这是RTT再小也白搭；RTO最小值是内核编译是决定的，socket程序中无法修改，Linux TCP也没有任何参数可以改变这个值。</p>
<p>rto不能设置，而是根据到不同server的rtt计算得到</p>
<h3 id="delay-ack"><a href="#delay-ack" class="headerlink" title="delay ack"></a>delay ack</h3><p>正常情况下ack可以quick ack也可以delay ack，redhat在sysctl中可以设置这两个值</p>
<blockquote>
<p>/proc/sys/net/ipv4/tcp_ato_min</p>
</blockquote>
<p>默认都是推荐delay ack的，一定要修改成quick ack的话（3.10.0-327之后的内核版本）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$sudo ip route show</div><div class="line">default via 10.0.207.253 dev eth0 proto dhcp src 10.0.200.23 metric 1024</div><div class="line">10.0.192.0/20 dev eth0 proto kernel scope link src 10.0.200.23</div><div class="line">10.0.207.253 dev eth0 proto dhcp scope link src 10.0.200.23 metric 1024</div><div class="line"></div><div class="line">$sudo ip route change default via 10.0.207.253  dev eth0 proto dhcp src 10.0.200.23 metric 1024 quickack 1</div><div class="line"></div><div class="line">$sudo ip route show</div><div class="line">default via 10.0.207.253 dev eth0 proto dhcp src 10.0.200.23 metric 1024 quickack 1</div><div class="line">10.0.192.0/20 dev eth0 proto kernel scope link src 10.0.200.23</div><div class="line">10.0.207.253 dev eth0 proto dhcp scope link src 10.0.200.23 metric 1024</div></pre></td></tr></table></figure>
<p>默认开启delay ack的抓包情况如下，可以清晰地看到有几个40ms的ack</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/7f4590cccf73fd672268dbf0e6a1b309.png" alt="image.png"></p>
<p>第一个40ms 的ack对应的包， 3306收到 update请求后没有ack，而是等了40ms update也没结束，就ack了</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/b06d3148450fc24fa26b2a9cdfe07831.png" alt="image.png"></p>
<p>同样的机器，执行quick ack后的抓包</p>
<blockquote>
<p>sudo ip route change default via 10.0.207.253  dev eth0 proto dhcp src 10.0.200.23 metric 1024 quickack 1</p>
</blockquote>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/9fba9819e769494bc09a2a11245e4769.png" alt="image.png"></p>
<p><strong>同样场景下，改成quick ack后基本所有的ack都在0.02ms内发出去了。</strong></p>
<p>比较奇怪的是在delay ack情况下不是每个空ack都等了40ms，这么多包只看到4个delay了40ms，其它的基本都在1ms内就以空包就行ack了。</p>
<p>将 quick ack去掉后再次抓包仍然抓到了很多的40ms的ack。</p>
<p>Java中setNoDelay是指关掉nagle算法，但是delay ack还是存在的。</p>
<p>C代码中关闭的话：At the application level with the <code>TCP_QUICKACK</code> socket option. See <code>man 7 tcp</code> for further details. This option needs to be set with <code>setsockopt()</code> after each operation of TCP on a given socket</p>
<p>连接刚建立前16个包一定是quick ack的，目的是加快慢启动</p>
<p>一旦后面进入延迟ACK模式后，<a href="https://www.cnblogs.com/lshs/p/6038635.html" target="_blank" rel="external">如果接收的还没有回复ACK确认的报文总大小超过88bytes的时候就会立即回复ACK报文</a>。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://access.redhat.com/solutions/407743" target="_blank" rel="external">https://access.redhat.com/solutions/407743</a></p>
<p><a href="https://www.cnblogs.com/lshs/p/6038635.html" target="_blank" rel="external">https://www.cnblogs.com/lshs/p/6038635.html</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;TCP相关参数解释&quot;&gt;&lt;a href=&quot;#TCP相关参数解释&quot; class=&quot;headerlink&quot; title=&quot;TCP相关参数解释&quot;&gt;&lt;/a&gt;TCP相关参数解释&lt;/h1&gt;&lt;p&gt;读懂TCP参数前得先搞清楚内核中出现的HZ、Tick、Jiffies三个值是什么意思
    
    </summary>
    
      <category term="TCP" scheme="http://yoursite.com/categories/TCP/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="network" scheme="http://yoursite.com/tags/network/"/>
    
      <category term="TCP" scheme="http://yoursite.com/tags/TCP/"/>
    
      <category term="参数" scheme="http://yoursite.com/tags/%E5%8F%82%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>ssd/san/sas  磁盘 光纤性能比较</title>
    <link href="http://yoursite.com/2020/01/25/ssd%20san%E5%92%8Csas%20%E7%A3%81%E7%9B%98%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83/"/>
    <id>http://yoursite.com/2020/01/25/ssd san和sas 磁盘性能比较/</id>
    <published>2020-01-25T09:30:03.000Z</published>
    <updated>2020-08-26T10:57:01.311Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ssd-san-sas-磁盘-光纤性能比较"><a href="#ssd-san-sas-磁盘-光纤性能比较" class="headerlink" title="ssd/san/sas 磁盘 光纤性能比较"></a>ssd/san/sas 磁盘 光纤性能比较</h1><p>正好有机会用到一个san存储设备，跑了一把性能数据，记录一下</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/d57a004c846e193126ca01398e394319.png" alt="image.png"></p>
<p>所使用的测试命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">fio -ioengine=libaio -bs=4k -direct=1 -thread -rw=randwrite -size=1000G -filename=/data/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</div></pre></td></tr></table></figure>
<p>ssd（Solid State Drive）和san的比较是在同一台物理机上，所以排除了其他因素的干扰。</p>
<p>简要的结论：</p>
<ul>
<li><p>本地ssd性能最好、sas机械盘(RAID10)性能最差</p>
</li>
<li><p>san存储走特定的光纤网络，不是走tcp的san（至少从网卡看不到san的流量），性能居中</p>
</li>
<li><p>从rt来看 ssd:san:sas 大概是 1:3:15</p>
</li>
<li><p>san比本地sas机械盘性能要好，这也许取决于san的网络传输性能和san存储中的设备（比如用的ssd而不是机械盘）</p>
</li>
</ul>
<h2 id="NVMe-SSD-和-HDD的性能比较"><a href="#NVMe-SSD-和-HDD的性能比较" class="headerlink" title="NVMe SSD 和 HDD的性能比较"></a>NVMe SSD 和 HDD的性能比较</h2><p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/d64a0f78ebf471ac69d447ecb46d90f1.png" alt="image.png"></p>
<p>表中性能差异比上面测试还要大，SSD 的随机 IO 延迟比传统硬盘快百倍以上，一般在微妙级别；IO 带宽也高很多倍，可以达到每秒几个 GB；随机 IOPS 更是快了上千倍，可以达到几十万。</p>
<p><strong>HDD只有一个磁头，并发没有意义，但是SSD支持高并发写入读取。SSD没有磁头、不需要旋转，所以随机读取和顺序读取基本没有差别。</strong></p>
<h2 id="SSD-的性能特性和机制"><a href="#SSD-的性能特性和机制" class="headerlink" title="SSD 的性能特性和机制"></a>SSD 的性能特性和机制</h2><p>SSD 的内部工作方式和 HDD 大相径庭，我们先了解几个概念。</p>
<p><strong>单元（Cell）、页面（Page）、块（Block）</strong>。当今的主流 SSD 是基于 NAND 的，它将数字位存储在单元中。每个 SSD 单元可以存储一位或多位。对单元的每次擦除都会降低单元的寿命，所以单元只能承受一定数量的擦除。单元存储的位数越多，制造成本就越少，SSD 的容量也就越大，但是耐久性（擦除次数）也会降低。</p>
<p>一个页面包括很多单元，典型的页面大小是 4KB，页面也是要读写的最小存储单元。SSD 上没有“重写”操作，不像 HDD 可以直接对任何字节重写覆盖。一个页面一旦写入内容后就不能进行部分重写，必须和其它相邻页面一起被整体擦除重置。</p>
<p>多个页面组合成块。一个块的典型大小为 512KB 或 1MB，也就是大约 128 或 256 页。<strong>块是擦除的基本单位，每次擦除都是整个块内的所有页面都被重置。</strong></p>
<p><strong>擦除速度相对很慢，通常为几毫秒</strong>。所以对同步的 IO，发出 IO 的应用程序可能会因为块的擦除，而经历很大的写入延迟。为了尽量地减少这样的场景，保持空闲块的阈值对于快速的写响应是很有必要的。SSD 的垃圾回收（GC）的目的就在于此。GC 可以回收用过的块，这样可以确保以后的页写入可以快速分配到一个全新的页。</p>
<h3 id="写入放大（Write-Amplification-or-WA"><a href="#写入放大（Write-Amplification-or-WA" class="headerlink" title="写入放大（Write Amplification, or WA)"></a>写入放大（Write Amplification, or WA)</h3><p>这是 SSD 相对于 HDD 的一个缺点，即实际写入 SSD 的物理数据量，有可能是应用层写入数据量的多倍。一方面，页级别的写入需要移动已有的数据来腾空页面。另一方面，GC 的操作也会移动用户数据来进行块级别的擦除。所以对 SSD 真正的写操作的数据可能比实际写的数据量大，这就是写入放大。一块 SSD 只能进行有限的擦除次数，也称为编程 / 擦除（P/E）周期，所以写入放大效用会缩短 SSD 的寿命。</p>
<h3 id="耗损平衡-Wear-Leveling"><a href="#耗损平衡-Wear-Leveling" class="headerlink" title="耗损平衡 (Wear Leveling)"></a>耗损平衡 (Wear Leveling)</h3><p>对每一个块而言，一旦达到最大数量，该块就会死亡。对于 SLC 块，P/E 周期的典型数目是十万次；对于 MLC 块，P/E 周期的数目是一万；而对于 TLC 块，则可能是几千。为了确保 SSD 的容量和性能，我们需要在擦除次数上保持平衡，SSD 控制器具有这种“耗损平衡”机制可以实现这一目标。在损耗平衡期间，数据在各个块之间移动，以实现均衡的损耗，这种机制也会对前面讲的写入放大推波助澜。</p>
<h2 id="光纤和网线的性能差异"><a href="#光纤和网线的性能差异" class="headerlink" title="光纤和网线的性能差异"></a>光纤和网线的性能差异</h2><p>以下都是在4.19内核的UOS，光纤交换机为锐捷，服务器是华为鲲鹏920</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/553e1c5fff2dd04a668434f0da4f9d90.png" alt="image.png"></p>
<p>光纤稳定性好很多，平均rt是网线的三分之一，最大值则是网线的十分之一. 光纤的带宽大约是网线的1.5倍</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">[aliyun@uos15 11:00 /home/aliyun]  一下88都是光口、89都是电口。</div><div class="line">$ping -c 10 10.88.88.16 //光纤</div><div class="line">PING 10.88.88.16 (10.88.88.16) 56(84) bytes of data.</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=1 ttl=64 time=0.058 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=2 ttl=64 time=0.049 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=3 ttl=64 time=0.053 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=4 ttl=64 time=0.040 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=5 ttl=64 time=0.053 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=6 ttl=64 time=0.043 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=7 ttl=64 time=0.038 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=8 ttl=64 time=0.050 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=9 ttl=64 time=0.043 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=10 ttl=64 time=0.064 ms</div><div class="line"></div><div class="line">--- 10.88.88.16 ping statistics ---</div><div class="line">10 packets transmitted, 10 received, 0% packet loss, time 159ms</div><div class="line">rtt min/avg/max/mdev = 0.038/0.049/0.064/0.008 ms</div><div class="line"></div><div class="line">[aliyun@uos15 11:01 /home/aliyun]</div><div class="line">$ping -c 10 10.88.89.16 //电口</div><div class="line">PING 10.88.89.16 (10.88.89.16) 56(84) bytes of data.</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=1 ttl=64 time=0.087 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=2 ttl=64 time=0.053 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=3 ttl=64 time=0.095 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=4 ttl=64 time=0.391 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=5 ttl=64 time=0.051 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=6 ttl=64 time=0.343 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=7 ttl=64 time=0.045 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=8 ttl=64 time=0.341 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=9 ttl=64 time=0.054 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=10 ttl=64 time=0.066 ms</div><div class="line"></div><div class="line">--- 10.88.89.16 ping statistics ---</div><div class="line">10 packets transmitted, 10 received, 0% packet loss, time 149ms</div><div class="line">rtt min/avg/max/mdev = 0.045/0.152/0.391/0.136 ms</div><div class="line"></div><div class="line">[aliyun@uos15 11:02 /u01]</div><div class="line">$scp uos.tar aliyun@10.88.89.16:/tmp/</div><div class="line">uos.tar                                  100% 3743MB 111.8MB/s   00:33    </div><div class="line"></div><div class="line">[aliyun@uos15 11:03 /u01]</div><div class="line">$scp uos.tar aliyun@10.88.88.16:/tmp/</div><div class="line">uos.tar                                   100% 3743MB 178.7MB/s   00:20    </div><div class="line"></div><div class="line">[aliyun@uos15 11:07 /u01]</div><div class="line">$sudo ping -f 10.88.89.16</div><div class="line">PING 10.88.89.16 (10.88.89.16) 56(84) bytes of data.</div><div class="line">--- 10.88.89.16 ping statistics ---</div><div class="line">284504 packets transmitted, 284504 received, 0% packet loss, time 702ms</div><div class="line">rtt min/avg/max/mdev = 0.019/0.040/1.014/0.013 ms, ipg/ewma 0.048/0.042 ms</div><div class="line"></div><div class="line">[aliyun@uos15 11:07 /u01]</div><div class="line">$sudo ping -f 10.88.88.16</div><div class="line">PING 10.88.88.16 (10.88.88.16) 56(84) bytes of data.</div><div class="line">--- 10.88.88.16 ping statistics ---</div><div class="line">299748 packets transmitted, 299748 received, 0% packet loss, time 242ms</div><div class="line">rtt min/avg/max/mdev = 0.012/0.016/0.406/0.006 ms, pipe 2, ipg/ewma 0.034/0.014 ms</div></pre></td></tr></table></figure>
<p>光纤接口：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/b67715de1b8e143f6fc17ba574bcf0c4.png" alt="image.png" style="zoom:60%;"></p>
<h2 id="Cache、内存、磁盘、网络的延迟比较"><a href="#Cache、内存、磁盘、网络的延迟比较" class="headerlink" title="Cache、内存、磁盘、网络的延迟比较"></a>Cache、内存、磁盘、网络的延迟比较</h2><p><a href="http://cizixs.com/2017/01/03/how-slow-is-disk-and-network" target="_blank" rel="external">假设主频2.6G的CPU，每个指令只需要 0.38ns</a> </p>
<p>每次内存寻址需要 100ns </p>
<p>一次 CPU 上下文切换（系统调用）需要大约 1500ns，也就是 1.5us（这个数字参考了<a href="http://blog.tsunanet.net/2010/11/how-long-does-it-take-to-make-context.html" target="_blank" rel="external">这篇文章</a>，采用的是单核 CPU 线程平均时间）</p>
<p>SSD 随机读取耗时为 150us</p>
<p>从内存中读取 1MB 的连续数据，耗时大约为 250us</p>
<p>同一个数据中心网络上跑一个来回需要 0.5ms</p>
<p>从 SSD 读取 1MB 的顺序数据，大约需要 1ms （是内存速度的四分之一）</p>
<p>磁盘寻址时间为 10ms</p>
<p>从磁盘读取 1MB 连续数据需要 20ms</p>
<p>如果 CPU 访问 L1 缓存需要 1 秒，那么访问主存需要 3 分钟、从 SSD 中随机读取数据需要 3.4 天、磁盘寻道需要 2 个月，网络传输可能需要 1 年多的时间。</p>
<p><strong>2012 年延迟数字对比表：</strong></p>
<table>
<thead>
<tr>
<th>Work</th>
<th>Latency</th>
</tr>
</thead>
<tbody>
<tr>
<td>L1 cache reference</td>
<td>0.5 ns</td>
</tr>
<tr>
<td>Branch mispredict</td>
<td>5 ns</td>
</tr>
<tr>
<td>L2 cache reference</td>
<td>7 ns</td>
</tr>
<tr>
<td>Mutex lock/unlock</td>
<td>25 ns</td>
</tr>
<tr>
<td>Main memory reference</td>
<td>100 ns</td>
</tr>
<tr>
<td>Compress 1K bytes with Zippy</td>
<td>3,000 ns</td>
</tr>
<tr>
<td>Send 1K bytes over 1 Gbps network</td>
<td>10,000 ns</td>
</tr>
<tr>
<td>Read 4K randomly from SSD*</td>
<td>150,000 ns</td>
</tr>
<tr>
<td>Read 1 MB sequentially from memory</td>
<td>250,000 ns</td>
</tr>
<tr>
<td>Round trip within same datacenter</td>
<td>500,000 ns</td>
</tr>
<tr>
<td>Read 1 MB sequentially from SSD*</td>
<td>1,000,000 ns</td>
</tr>
<tr>
<td>Disk seek</td>
<td>10,000,000 ns</td>
</tr>
<tr>
<td>Read 1 MB sequentially from disk</td>
<td>20,000,000 ns</td>
</tr>
<tr>
<td>Send packet CA-&gt;Netherlands-&gt;CA</td>
<td>150,000,000 ns</td>
</tr>
</tbody>
</table>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://cizixs.com/2017/01/03/how-slow-is-disk-and-network" target="_blank" rel="external">http://cizixs.com/2017/01/03/how-slow-is-disk-and-network</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;ssd-san-sas-磁盘-光纤性能比较&quot;&gt;&lt;a href=&quot;#ssd-san-sas-磁盘-光纤性能比较&quot; class=&quot;headerlink&quot; title=&quot;ssd/san/sas 磁盘 光纤性能比较&quot;&gt;&lt;/a&gt;ssd/san/sas 磁盘 光纤性能比较&lt;/
    
    </summary>
    
      <category term="performance" scheme="http://yoursite.com/categories/performance/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="磁盘性能" scheme="http://yoursite.com/tags/%E7%A3%81%E7%9B%98%E6%80%A7%E8%83%BD/"/>
    
      <category term="san" scheme="http://yoursite.com/tags/san/"/>
    
      <category term="光纤" scheme="http://yoursite.com/tags/%E5%85%89%E7%BA%A4/"/>
    
  </entry>
  
  <entry>
    <title>如何制作本地yum repository</title>
    <link href="http://yoursite.com/2020/01/24/%E5%A6%82%E4%BD%95%E5%88%B6%E4%BD%9C%E6%9C%AC%E5%9C%B0yum%20repository/"/>
    <id>http://yoursite.com/2020/01/24/如何制作本地yum repository/</id>
    <published>2020-01-24T09:30:03.000Z</published>
    <updated>2020-07-28T01:44:55.916Z</updated>
    
    <content type="html"><![CDATA[<h1 id="如何制作本地yum-repository"><a href="#如何制作本地yum-repository" class="headerlink" title="如何制作本地yum repository"></a>如何制作本地yum repository</h1><p>某些情况下在没有外网的环境需要安装一些软件，但是软件依赖比较多，那么可以提前将所有依赖下载到本地，然后将他们制作成一个yum repo，安装的时候就会自动将依赖包都安装好。</p>
<h2 id="收集所有rpm包"><a href="#收集所有rpm包" class="headerlink" title="收集所有rpm包"></a>收集所有rpm包</h2><p>创建一个文件夹，比如 Yum，将收集到的所有rpm包放在里面，比如安装ansible和docker需要的依赖文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">-rwxr-xr-x 1 root root  73K 7月  12 14:22 audit-libs-python-2.8.4-4.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root 295K 7月  12 14:22 checkpolicy-2.5-8.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root  23M 7月  12 14:22 containerd.io-1.2.2-3.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root  26K 7月  12 14:22 container-selinux-2.9-4.el7.noarch.rpm</div><div class="line">-rwxr-xr-x 1 root root  37K 7月  12 14:22 container-selinux-2.74-1.el7.noarch.rpm</div><div class="line">-rwxr-xr-x 1 root root  14M 7月  12 14:22 docker-ce-cli-18.09.0-3.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root  29K 7月  12 14:22 docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm</div><div class="line">-r-xr-xr-x 1 root root  22K 7月  12 14:23 sshpass-1.06-2.el7.x86_64.rpm</div><div class="line">-r-xr-xr-x 1 root root  22K 7月  12 14:23 sshpass-1.06-1.el7.x86_64.rpm</div><div class="line">-r-xr-xr-x 1 root root 154K 7月  12 14:23 PyYAML-3.10-11.el7.x86_64.rpm</div><div class="line">-r-xr-xr-x 1 root root  29K 7月  12 14:23 python-six-1.9.0-2.el7.noarch.rpm</div><div class="line">-r-xr-xr-x 1 root root 397K 7月  12 14:23 python-setuptools-0.9.8-7.el7.noarch.rpm</div></pre></td></tr></table></figure>
<p>收集方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">//先安装yum工具</div><div class="line">yum install yum-utils -y</div><div class="line">//将 ansible 依赖包都下载下来</div><div class="line">repoquery -R --resolve --recursive ansible | xargs -r yumdownloader --destdir=/tmp/ansible</div><div class="line">//将ansible rpm自己下载回来</div><div class="line">yumdownloader --destdir=/tmp/ansible --resolve ansible</div><div class="line">//验证一下依赖关系是完整的</div><div class="line">//repotrack ansible</div></pre></td></tr></table></figure>
<h2 id="创建仓库索引"><a href="#创建仓库索引" class="headerlink" title="创建仓库索引"></a>创建仓库索引</h2><p>需要安装工具 createrepo：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># createrepo ./yum/</div><div class="line">Spawning worker 0 with 6 pkgs</div><div class="line">Spawning worker 1 with 6 pkgs</div><div class="line">Spawning worker 23 with 5 pkgs</div><div class="line">Workers Finished</div><div class="line">Saving Primary metadata</div><div class="line">Saving file lists metadata</div><div class="line">Saving other metadata</div><div class="line">Generating sqlite DBs</div><div class="line">Sqlite DBs complete</div></pre></td></tr></table></figure>
<p>会在yum文件夹下生成一个索引文件夹 repodata</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">drwxr-xr-x 2 root root 4.0K 7月  12 14:25 repodata</div><div class="line">[root@az1-drds-79 yum]# ls repodata/</div><div class="line">5e15c62fec1fe43c6025ecf4d370d632f4b3f607500016e045ad94b70f87bac3-filelists.xml.gz</div><div class="line">7a314396d6e90532c5c534567f9bd34eee94c3f8945fc2191b225b2861ace2b6-other.xml.gz</div><div class="line">ce9dce19f6b426b8856747b01d51ceaa2e744b6bbd5fbc68733aa3195f724590-primary.xml.gz</div><div class="line">ee33b7d79e32fe6ad813af92a778a0ec8e5cc2dfdc9b16d0be8cff6a13e80d99-filelists.sqlite.bz2</div><div class="line">f7e8177e7207a4ff94bade329a0f6b572a72e21da106dd9144f8b1cdf0489cab-primary.sqlite.bz2</div><div class="line">ff52e1f1859790a7b573d2708b02404eb8b29aa4b0c337bda83af75b305bfb36-other.sqlite.bz2</div><div class="line">repomd.xml</div></pre></td></tr></table></figure>
<h2 id="生成iso镜像文件"><a href="#生成iso镜像文件" class="headerlink" title="生成iso镜像文件"></a>生成iso镜像文件</h2><p>非必要步骤，如果需要带到客户环境可以先生成iso，不过不够灵活。</p>
<p>也可以不用生成iso，直接在drds.repo中指定 createrepo 的目录也可以，记得要先执行 yum clean all和yum update </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">#mkisofs -r -o docker_ansible.iso ./yum/</div><div class="line">I: -input-charset not specified, using utf-8 (detected in locale settings)</div><div class="line">Using PYTHO000.RPM;1 for  /python-httplib2-0.7.7-3.el7.noarch.rpm (python-httplib2-0.9.1-3.el7.noarch.rpm)</div><div class="line">Using MARIA006.RPM;1 for  /mariadb-5.5.56-2.el7.x86_64.rpm (mariadb-libs-5.5.56-2.el7.x86_64.rpm)</div><div class="line">Using LIBTO001.RPM;1 for  /libtomcrypt-1.17-25.el7.x86_64.rpm (libtomcrypt-1.17-26.el7.x86_64.rpm)</div><div class="line">  6.11% done, estimate finish Sun Jul 12 14:26:47 2020</div><div class="line"> 97.60% done, estimate finish Sun Jul 12 14:26:48 2020</div><div class="line">Total translation table size: 0</div><div class="line">Total rockridge attributes bytes: 14838</div><div class="line">Total directory bytes: 2048</div><div class="line">Path table size(bytes): 26</div><div class="line">Max brk space used 21000</div><div class="line">81981 extents written (160 MB)</div></pre></td></tr></table></figure>
<h2 id="将-生成的-iso挂载到目标机器上"><a href="#将-生成的-iso挂载到目标机器上" class="headerlink" title="将 生成的 iso挂载到目标机器上"></a>将 生成的 iso挂载到目标机器上</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># mkdir /mnt/iso</div><div class="line"># mount ./docker_ansible.iso /mnt/iso</div><div class="line">mount: /dev/loop0 is write-protected, mounting read-only</div></pre></td></tr></table></figure>
<h2 id="配置本地-yum-源"><a href="#配置本地-yum-源" class="headerlink" title="配置本地 yum 源"></a>配置本地 yum 源</h2><p>yum repository不是必须要求iso挂载，直接指向rpm文件夹（必须要有 createrepo 建立索引了）也可以</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># cat /etc/yum.repos.d/drds.repo </div><div class="line">[drds]</div><div class="line">name=drds Extra Packages for Enterprise Linux 7 - $basearch</div><div class="line">enabled=1</div><div class="line">failovermethod=priority</div><div class="line">baseurl=file:///mnt/repo #baseurl=http://192.168.1.91:8000/ 本地内网</div><div class="line">priority=1  #添加priority=1，数字越小优先级越高，也可以修改网络源的priority的值</div><div class="line">gpgcheck=0</div><div class="line">#gpgkey=file:///mnt/cdrom/RPM-GPG-KEY-CentOS-5    #注：这个你cd /mnt/cdrom/可以看到这个key，这里仅仅是个例子， 因为gpgcheck是0 ，所以gpgkey不需要了</div></pre></td></tr></table></figure>
<p>到此就可以在没有网络环境的机器上直接：yum install ansible docker -y 了 </p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>测试的话可以指定repo 源： yum install ansible –enablerepo=drds （drds 优先级最高）</p>
<p>本地会cache一些rpm的版本信息，可以执行 yum clean all 得到一个干净的测试环境</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">yum clean all</div><div class="line">yum list</div><div class="line">yum deplist ansible</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;如何制作本地yum-repository&quot;&gt;&lt;a href=&quot;#如何制作本地yum-repository&quot; class=&quot;headerlink&quot; title=&quot;如何制作本地yum repository&quot;&gt;&lt;/a&gt;如何制作本地yum repository&lt;/h1&gt;&lt;
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="yum" scheme="http://yoursite.com/tags/yum/"/>
    
      <category term="iso" scheme="http://yoursite.com/tags/iso/"/>
    
      <category term="createrepo" scheme="http://yoursite.com/tags/createrepo/"/>
    
      <category term="yum-utils" scheme="http://yoursite.com/tags/yum-utils/"/>
    
  </entry>
  
  <entry>
    <title>Linux 内存问题汇总</title>
    <link href="http://yoursite.com/2020/01/15/Linux%20%E5%86%85%E5%AD%98%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/"/>
    <id>http://yoursite.com/2020/01/15/Linux 内存问题汇总/</id>
    <published>2020-01-15T08:30:03.000Z</published>
    <updated>2020-08-26T10:57:01.287Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linux-内存问题汇总"><a href="#Linux-内存问题汇总" class="headerlink" title="Linux 内存问题汇总"></a>Linux 内存问题汇总</h1><h2 id="内存使用观察"><a href="#内存使用观察" class="headerlink" title="内存使用观察"></a>内存使用观察</h2><pre><code># free -m
         total       used       free     shared    buffers     cached
Mem:          7515       1115       6400          0        189        492
-/+ buffers/cache:        432       7082
Swap:            0          0          0
</code></pre><p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/f8d944e2c7a8611384acb820c4471007.png" alt="image.png" style="zoom:80%;"></p>
<p><strong>上图中-/+ buffers/cache: -是指userd去掉buffers/cached后真正使用掉的内存; +是指free加上buffers和cached后真正free的内存大小。</strong></p>
<h2 id="free"><a href="#free" class="headerlink" title="free"></a>free</h2><p>free是从 /proc/meminfo 读取数据然后展示：</p>
<blockquote>
<p>buff/cache = Buffers + Cached + SReclaimable</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[root@az1-drds-79 ~]# cat /proc/meminfo |egrep -i &quot;buff|cach|SReclai&quot;</div><div class="line">Buffers:          817764 kB</div><div class="line">Cached:         76629252 kB</div><div class="line">SwapCached:            0 kB</div><div class="line">SReclaimable:    7202264 kB</div><div class="line">[root@az1-drds-79 ~]# free -k</div><div class="line">             total       used       free     shared    buffers     cached</div><div class="line">Mem:      97267672   95522336    1745336          0     817764   76629352</div><div class="line">-/+ buffers/cache:   18075220   79192452</div><div class="line">Swap:            0          0          0</div></pre></td></tr></table></figure>
<h2 id="cached过高回收"><a href="#cached过高回收" class="headerlink" title="cached过高回收"></a>cached过高回收</h2><p>回收：<br>    echo 1/2/3 &gt;/proc/sys/vm/drop_cached</p>
<p>查看回收后：</p>
<pre><code>cat /proc/meminfo
</code></pre><h2 id="还有很多cached无法回收"><a href="#还有很多cached无法回收" class="headerlink" title="还有很多cached无法回收"></a>还有很多cached无法回收</h2><p>可能是正打开的文件占用了cached，比如 vim 打开了一个巨大的文件；比如 mount的 tmpfs； 比如 journald 日志等等</p>
<h3 id="通过vmtouch-查看"><a href="#通过vmtouch-查看" class="headerlink" title="通过vmtouch 查看"></a>通过<a href="https://hoytech.com/vmtouch/" target="_blank" rel="external">vmtouch</a> 查看</h3><pre><code># vmtouch -v test.x86_64.rpm 
test.x86_64.rpm
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 10988/10988

           Files: 1
     Directories: 0
  Resident Pages: 10988/10988  42M/42M  100%
         Elapsed: 0.000594 seconds

# ls -lh test.x86_64.rpm
-rw-r--r-- 1 root root 43M 10月  8 14:11 test.x86_64.rpm
</code></pre><p>如上，表示整个文件 test.x86_64.rpm 都被cached了，回收的话执行：</p>
<pre><code>vmtouch -e test.x86_64.rpm // 或者： echo 3 &gt;/proc/sys/vm/drop_cached
</code></pre><h3 id="遍历某个目录下的所有文件被cached了多少"><a href="#遍历某个目录下的所有文件被cached了多少" class="headerlink" title="遍历某个目录下的所有文件被cached了多少"></a>遍历某个目录下的所有文件被cached了多少</h3><pre><code># vmtouch -vt /var/log/journal/
/var/log/journal/20190829214900434421844640356160/user-1000@ad408d9cb9d94f9f93f2c2396c26b542-000000000011ba49-00059979e0926f43.journal
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 4096/4096
/var/log/journal/20190829214900434421844640356160/system@782ec314565e436b900454c59655247c-0000000000152f41-00059b2c88eb4344.journal
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 14336/14336
/var/log/journal/20190829214900434421844640356160/user-1000@ad408d9cb9d94f9f93f2c2396c26b542-00000000000f2181-000598335fcd492f.journal
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 4096/4096
/var/log/journal/20190829214900434421844640356160/system@782ec314565e436b900454c59655247c-0000000000129aea-000599e83996db80.journal
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 14336/14336
/var/log/journal/20190829214900434421844640356160/user-1000@ad408d9cb9d94f9f93f2c2396c26b542-000000000009f171-000595a722ead670.journal
…………
           Files: 48
 Directories: 2
 Touched Pages: 468992 (1G)
 Elapsed: 13.274 seconds
</code></pre><h2 id="pagecache-的产生和释放"><a href="#pagecache-的产生和释放" class="headerlink" title="pagecache 的产生和释放"></a>pagecache 的产生和释放</h2><ul>
<li>标准 I/O 是写的 (write(2)) 用户缓冲区 (Userpace Page 对应的内存)，<strong>然后再将用户缓冲区里的数据拷贝到内核缓冲区 (Pagecache Page 对应的内存)</strong>；如果是读的 (read(2)) 话则是先从内核缓冲区拷贝到用户缓冲区，再从用户缓冲区读数据，也就是 buffer 和文件内容不存在任何映射关系。</li>
<li>对于存储映射 I/O（Memory-Mapped I/O） 而言，则是直接将 Pagecache Page 给映射到用户地址空间，用户直接读写 Pagecache Page 中内容，效率相对标准IO更高一些</li>
</ul>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/51bf36aa14dc01e7ad309c1bb9d252e9.png" alt="image.png"></p>
<p>当 <strong>将用户缓冲区里的数据拷贝到内核缓冲区 (Pagecache Page 对应的内存)</strong> 最容易发生缺页中断，OS需要先分配Page（应用感知到的就是卡顿了）</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/d62ea00662f8342b7df3aab6b28e4cbb.png" alt="image.png">  </p>
<ul>
<li>Page Cache 是在应用程序读写文件的过程中产生的，所以在读写文件之前你需要留意是否还有足够的内存来分配 Page Cache；</li>
<li>Page Cache 中的脏页很容易引起问题，你要重点注意这一块；</li>
<li>在系统可用内存不足的时候就会回收 Page Cache 来释放出来内存，我建议你可以通过 sar 或者 /proc/vmstat 来观察这个行为从而更好的判断问题是否跟回收有关</li>
</ul>
<p>缺页后kswapd在短时间内回收不了足够多的 free 内存，或kswapd 还没有触发执行，操作系统就会进行内存页直接回收。这个过程中，应用会进行自旋等待直到回收的完成，从而产生巨大的延迟。</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/0a5cdeb75b7dee2068254cd4b7fe254d.png" alt=""></p>
<p>如果page被swapped，那么恢复进内场的过程也对延迟有影响，当被匿名内存页被回收后，如果下次再访问就会产生IO的延迟。</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/740b95056dace8ae6fb3b8f58d91572e.png" alt=""></p>
<h3 id="min-和-low的区别："><a href="#min-和-low的区别：" class="headerlink" title="min 和 low的区别："></a>min 和 low的区别：</h3><ol>
<li>min下的内存是保留给内核使用的；当到达min，会触发内存的direct reclaim</li>
<li>low水位比min高一些，当内存可用量小于low的时候，会触发 kswapd回收内存，当kswapd慢慢的将内存 回收到high水位，就开始继续睡眠</li>
</ol>
<h3 id="内存回收方式"><a href="#内存回收方式" class="headerlink" title="内存回收方式"></a>内存回收方式</h3><p>内存回收方式有两种，主要对应low ，min</p>
<ol>
<li>direct reclaim : 达到min水位线时执行</li>
<li>kswapd reclaim : 达到low水位线时执行</li>
</ol>
<p>为了减少缺页中断，首先就要保证我们有足够的内存可以使用。由于Linux会尽可能多的使用free的内存，运行很久的应用free的内存是很少的。下面的图中，紫色表示已经使用的内存，白色表示尚未分配的内存。当我们的内存使用达到水位的low值的时候，kswapd就会开始回收工作，而一旦内存分配超过了mini，就会进行内存的直接回收。</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/5933cc4c28f86aa08410a8af4ff4410d.png" alt=""></p>
<p>针对这种情况，我们需要采用预留内存的手段，系统参数vm.extra_free_kbytes就是用来做这个事情的。这个参数设置了系统预留给应用的内存，可以避免紧急需要内存时发生内存回收不及时导致的高延迟。从下面图中可以看到，通过vm.extra_free_kbytes的设置，预留内存可以让内存的申请处在一个安全的水位。<strong>需要注意的是，因为内核的优化，在3.10以上的内核版本这个参数已经被取消。</strong></p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/f55022d4eb181b92ba5d2e142ec940c8.png" alt=""></p>
<p>或者禁止： vm.swappiness  来避免swapped来减少延迟</p>
<h2 id="slabtop和-proc-slabinfo"><a href="#slabtop和-proc-slabinfo" class="headerlink" title="slabtop和/proc/slabinfo"></a>slabtop和/proc/slabinfo</h2><p>slabtop和/proc/slabinfo 查看cached使用情况 主要是：pagecache（页面缓存）， dentries（目录缓存）， inodes</p>
<h2 id="消失的内存"><a href="#消失的内存" class="headerlink" title="消失的内存"></a>消失的内存</h2><p>OS刚启动后就报内存不够了，什么都没跑就500G没了，cached和buffer基本没用，纯粹就是used占用高，top按内存排序没有超过0.5%的进程</p>
<p>参考： <a href="https://cloud.tencent.com/developer/article/1087455" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1087455</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line">[aliyun@uos15 18:40 /u02/backup_15/leo/benchmark/run]</div><div class="line">$free -g</div><div class="line">              total        used        free      shared  buff/cache   available</div><div class="line">Mem:            503         501           1           0           0           1</div><div class="line">Swap:            15          12           3</div><div class="line"></div><div class="line">$cat /proc/meminfo </div><div class="line">MemTotal:       528031512 kB</div><div class="line">MemFree:         1469632 kB</div><div class="line">MemAvailable:          0 kB</div><div class="line">VmallocTotal:   135290290112 kB</div><div class="line">VmallocUsed:           0 kB</div><div class="line">VmallocChunk:          0 kB</div><div class="line">Percpu:            81920 kB</div><div class="line">AnonHugePages:    950272 kB</div><div class="line">ShmemHugePages:        0 kB</div><div class="line">ShmemPmdMapped:        0 kB</div><div class="line">HugePages_Total:   252557   ----- 预分配太多，一个2M，加起来刚好500G了</div><div class="line">HugePages_Free:    252557</div><div class="line">HugePages_Rsvd:        0</div><div class="line">HugePages_Surp:        0</div><div class="line">Hugepagesize:       2048 kB</div><div class="line">Hugetlb:        517236736 kB</div><div class="line"></div><div class="line">以下是一台正常的机器对比：</div><div class="line">Percpu:            41856 kB</div><div class="line">AnonHugePages:  11442176 kB</div><div class="line">ShmemHugePages:        0 kB</div><div class="line">ShmemPmdMapped:        0 kB</div><div class="line">HugePages_Total:       0            ----没有做预分配</div><div class="line">HugePages_Free:        0</div><div class="line">HugePages_Rsvd:        0</div><div class="line">HugePages_Surp:        0</div><div class="line">Hugepagesize:       2048 kB</div><div class="line">Hugetlb:               0 kB</div><div class="line"></div><div class="line">[aliyun@uos16 18:43 /home/aliyun]</div><div class="line">$free -g</div><div class="line">              total        used        free      shared  buff/cache   available</div><div class="line">Mem:            503          20         481           0           1         480</div><div class="line">Swap:            15           0          15</div><div class="line"></div><div class="line">对有问题的机器执行：</div><div class="line"># echo 1024 &gt; /proc/sys/vm/nr_hugepages</div><div class="line">可以看到内存恢复正常了 </div><div class="line">root@uos15:/u02/backup_15/leo/benchmark/run# free -g</div><div class="line">              total        used        free      shared  buff/cache   available</div><div class="line">Mem:            503          10         492           0           0         490</div><div class="line">Swap:            15          12           3</div><div class="line">root@uos15:/u02/backup_15/leo/benchmark/run# cat /proc/meminfo </div><div class="line">MemTotal:       528031512 kB</div><div class="line">MemFree:        516106832 kB</div><div class="line">MemAvailable:   514454408 kB</div><div class="line">VmallocTotal:   135290290112 kB</div><div class="line">VmallocUsed:           0 kB</div><div class="line">VmallocChunk:          0 kB</div><div class="line">Percpu:            81920 kB</div><div class="line">AnonHugePages:    313344 kB</div><div class="line">ShmemHugePages:        0 kB</div><div class="line">ShmemPmdMapped:        0 kB</div><div class="line">HugePages_Total:    1024</div><div class="line">HugePages_Free:     1024</div><div class="line">HugePages_Rsvd:        0</div><div class="line">HugePages_Surp:        0</div><div class="line">Hugepagesize:       2048 kB</div><div class="line">Hugetlb:         2097152 kB</div></pre></td></tr></table></figure>
<h2 id="关于hugetlb"><a href="#关于hugetlb" class="headerlink" title="关于hugetlb"></a>关于hugetlb</h2><p> This is an entry in the TLB that points to a HugePage (a large/big page larger than regular 4K and predefined in size). HugePages are implemented via hugetlb entries, i.e. we can say that a HugePage is handled by a “hugetlb page entry”. The ‘hugetlb” term is also (and mostly) used synonymously with a HugePage (See Note 261889.1). In this document the term “HugePage” is going to be used but keep in mind that mostly “hugetlb” refers to the same concept.</p>
<p> hugetlb 是TLB中指向HugePage的一个entry(通常大于4k或预定义页面大小)。 HugePage 通过hugetlb entries来实现，也可以理解为HugePage 是hugetlb page entry的一个句柄。</p>
<p><strong>Linux下的大页分为两种类型：标准大页（Huge Pages）和透明大页（Transparent Huge Pages）</strong></p>
<p>标准大页管理是预分配的方式，而透明大页管理则是动态分配的方式</p>
<p>目前透明大页与传统HugePages联用会出现一些问题，导致性能问题和系统重启。Oracle 建议禁用透明大页（Transparent Huge Pages）</p>
<p>hugetlbfs比THP要好，开thp的机器碎片化严重（不开THP会有更严重的碎片化问题），最后和没开THP一样 <a href="https://www.atatech.org/articles/152660" target="_blank" rel="external">https://www.atatech.org/articles/152660</a></p>
<h2 id="THP"><a href="#THP" class="headerlink" title="THP"></a>THP</h2><p>Linux kernel在2.6.38内核增加了Transparent Huge Pages (THP)特性 ，支持大内存页(2MB)分配，默认开启。当开启时可以降低fork子进程的速度，但fork之后，每个内存页从原来4KB变为2MB，会大幅增加重写期间父进程内存消耗。同时每次写命令引起的复制内存页单位放大了512倍，会拖慢写操作的执行时间，导致大量写操作慢查询。例如简单的incr命令也会出现在慢查询中。因此Redis日志中建议将此特性进行禁用。  </p>
<p>THP 对redis、monglodb 这种cache类推荐关闭，对drds这种java应用最好打开</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">grep &quot;Huge&quot; /proc/meminfo</div><div class="line">AnonHugePages:   1286144 kB</div><div class="line">ShmemHugePages:        0 kB</div><div class="line">HugePages_Total:       0</div><div class="line">HugePages_Free:        0</div><div class="line">HugePages_Rsvd:        0</div><div class="line">HugePages_Surp:        0</div><div class="line">Hugepagesize:       2048 kB</div><div class="line">Hugetlb:               0 kB</div><div class="line">$grep -e AnonHugePages  /proc/*/smaps | awk  &apos;&#123; if($2&gt;4) print $0&#125; &apos; |  awk -F &quot;/&quot;  &apos;&#123;print $0; system(&quot;ps -fp &quot; $3)&#125; &apos;</div><div class="line"></div><div class="line">//查看pagesize（默认4K） </div><div class="line">$getconf PAGESIZE</div></pre></td></tr></table></figure>
<p>在透明大页功能打开时，造成系统性能下降的主要原因可能是 <code>khugepaged</code> 守护进程。该进程会在（它认为）系统空闲时启动，扫描系统中剩余的空闲内存，并将普通 4k 页转换为大页。该操作会在内存路径中加锁，而该守护进程可能会在错误的时间启动扫描和转换大页的操作，从而影响应用性能。</p>
<p>此外，当缺页异常(page faults)增多时，透明大页会和普通 4k 页一样，产生同步内存压缩(direct compaction)操作，以节省内存。该操作是一个同步的内存整理操作，如果应用程序会短时间分配大量内存，内存压缩操作很可能会被触发，从而会对系统性能造成风险。<a href="https://yq.aliyun.com/articles/712830" target="_blank" rel="external">https://yq.aliyun.com/articles/712830</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">#查看系统级别的 THP 使用情况，执行下列命令：</div><div class="line">cat /proc/meminfo  | grep AnonHugePages</div><div class="line">#类似地，查看进程级别的 THP 使用情况，执行下列命令：</div><div class="line">cat /proc/1730/smaps | grep AnonHugePages |grep -v &quot;0 kB&quot;</div></pre></td></tr></table></figure>
<h2 id="碎片化"><a href="#碎片化" class="headerlink" title="碎片化"></a>碎片化</h2><p>内存碎片严重的话会导致系统hang很久(回收、压缩内存）</p>
<p>尽量让系统的free多一点(比例高一点）可以调整 vm.min_free_kbytes(128G 以内 2G，256G以内 8G), 线上机器直接修改vm.min_free_kbytes<strong>会触发回收，导致系统hang住</strong> <a href="https://www.atatech.org/articles/163233" target="_blank" rel="external">https://www.atatech.org/articles/163233</a> <a href="https://www.atatech.org/articles/97130" target="_blank" rel="external">https://www.atatech.org/articles/97130</a></p>
<p>每个zone都有自己的min low high,如下，但是单位是page, 计算案例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</div><div class="line">#cat /proc/zoneinfo  |grep &quot;Node&quot;</div><div class="line">Node 0, zone      DMA</div><div class="line">Node 0, zone    DMA32</div><div class="line">Node 0, zone   Normal</div><div class="line">Node 1, zone   Normal</div><div class="line"></div><div class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</div><div class="line">#cat /proc/zoneinfo  |grep &quot;Node 0, zone&quot; -A10</div><div class="line">Node 0, zone      DMA</div><div class="line">  pages free     3975</div><div class="line">        min      20</div><div class="line">        low      25</div><div class="line">        high     30</div><div class="line">        scanned  0</div><div class="line">        spanned  4095</div><div class="line">        present  3996</div><div class="line">        managed  3975</div><div class="line">    nr_free_pages 3975</div><div class="line">    nr_alloc_batch 5</div><div class="line">--</div><div class="line">Node 0, zone    DMA32</div><div class="line">  pages free     382873</div><div class="line">        min      2335</div><div class="line">        low      2918</div><div class="line">        high     3502</div><div class="line">        scanned  0</div><div class="line">        spanned  1044480</div><div class="line">        present  513024</div><div class="line">        managed  450639</div><div class="line">    nr_free_pages 382873</div><div class="line">    nr_alloc_batch 584</div><div class="line">--</div><div class="line">Node 0, zone   Normal</div><div class="line">  pages free     11105097</div><div class="line">        min      61463</div><div class="line">        low      76828</div><div class="line">        high     92194</div><div class="line">        scanned  0</div><div class="line">        spanned  12058624</div><div class="line">        present  12058624</div><div class="line">        managed  11859912</div><div class="line">    nr_free_pages 11105097</div><div class="line">    nr_alloc_batch 12344</div><div class="line">    </div><div class="line">    low = 5/4 * min</div><div class="line">high = 3/2 * min</div><div class="line"></div><div class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</div><div class="line">#T=min;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</div><div class="line">sum=499 MB</div><div class="line"></div><div class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</div><div class="line">#T=low;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</div><div class="line">sum=624 MB</div><div class="line"></div><div class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</div><div class="line">#T=high;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</div><div class="line">sum=802 MB</div></pre></td></tr></table></figure>
<h2 id="定制内存"><a href="#定制内存" class="headerlink" title="定制内存"></a>定制内存</h2><p>物理内存700多G，要求OS只能用512G：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">24条32G的内存条，总内存768G</div><div class="line"># dmidecode -t memory |grep &quot;Size: 32 GB&quot;</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">root@uos15:/etc# dmidecode -t memory |grep &quot;Size: 32 GB&quot; | wc -l</div><div class="line">24</div><div class="line"></div><div class="line"># cat /boot/grub/grub.cfg  |grep 512</div><div class="line">  linux /vmlinuz-4.19.0-arm64-server root=UUID=dbc68010-8c36-40bf-b794-271e59ff5727 ro  splash quiet console=tty video=VGA-1:1280x1024@60 mem=512G DEEPIN_GFXMODE=$DEEPIN_GFXMODE</div><div class="line">    linux /vmlinuz-4.19.0-arm64-server root=UUID=dbc68010-8c36-40bf-b794-271e59ff5727 ro  splash quiet console=tty video=VGA-1:1280x1024@60 mem=512G DEEPIN_GFXMODE=$DEEPIN_GFXMODE</div></pre></td></tr></table></figure>
<p>​    </p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.atatech.org/articles/66885" target="_blank" rel="external">https://www.atatech.org/articles/66885</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1087455" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1087455</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Linux-内存问题汇总&quot;&gt;&lt;a href=&quot;#Linux-内存问题汇总&quot; class=&quot;headerlink&quot; title=&quot;Linux 内存问题汇总&quot;&gt;&lt;/a&gt;Linux 内存问题汇总&lt;/h1&gt;&lt;h2 id=&quot;内存使用观察&quot;&gt;&lt;a href=&quot;#内存使用观察&quot;
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="free" scheme="http://yoursite.com/tags/free/"/>
    
      <category term="Memory" scheme="http://yoursite.com/tags/Memory/"/>
    
      <category term="vmtouch" scheme="http://yoursite.com/tags/vmtouch/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes</title>
    <link href="http://yoursite.com/2020/01/12/kubernetes/"/>
    <id>http://yoursite.com/2020/01/12/kubernetes/</id>
    <published>2020-01-12T09:30:03.000Z</published>
    <updated>2020-08-28T10:52:01.364Z</updated>
    
    <content type="html"><![CDATA[<h1 id="kubernetes"><a href="#kubernetes" class="headerlink" title="kubernetes"></a>kubernetes</h1><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>系统参数修改</p>
<p>docker部署</p>
<p>kubeadm</p>
<p><a href="https://www.kubernetes.org.cn/4256.html" target="_blank" rel="external">https://www.kubernetes.org.cn/4256.html</a> </p>
<p><a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster" target="_blank" rel="external">https://github.com/opsnull/follow-me-install-kubernetes-cluster</a></p>
<p>镜像源被墙，可以用阿里云镜像源</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># 配置源</div><div class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</div><div class="line">[kubernetes]</div><div class="line">name=Kubernetes</div><div class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</div><div class="line">enabled=1</div><div class="line">gpgcheck=1</div><div class="line">repo_gpgcheck=1</div><div class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</div><div class="line">EOF</div><div class="line"></div><div class="line"># 安装</div><div class="line">yum install -y kubelet kubeadm kubectl ipvsadm</div></pre></td></tr></table></figure>
<h2 id="初始化集群"><a href="#初始化集群" class="headerlink" title="初始化集群"></a>初始化集群</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">kubeadm init --kubernetes-version=1.18.0  --apiserver-advertise-address=192.168.0.80   --image-repository registry.aliyuncs.com/google_containers  --service-cidr=10.10.0.0/16 --pod-network-cidr=10.122.0.0/16</div><div class="line"></div><div class="line"># node join command</div><div class="line">#kubeadm token create --print-join-command</div><div class="line">kubeadm join 192.168.0.80:6443 --token 1042rl.b4qn9iuz6xv1ri7b     --discovery-token-ca-cert-hash sha256:341a4bcfde9668077ef29211c2a151fe6e9334eea8955f645698706b3bf47a49 </div><div class="line"></div><div class="line">## 查看集群配置</div><div class="line">kubectl get configmap -n kube-system kubeadm-config -o yaml</div></pre></td></tr></table></figure>
<h2 id="apiserver高可用"><a href="#apiserver高可用" class="headerlink" title="apiserver高可用"></a>apiserver高可用</h2><p>默认只有一个apiserver，可以考虑用haproxy和keepalive来做一组apiserver的负载均衡：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">docker run -d --name kube-haproxy \</div><div class="line">-v /etc/haproxy:/usr/local/etc/haproxy:ro \</div><div class="line">-p 8443:8443 \</div><div class="line">-p 1080:1080 \</div><div class="line">--restart always \</div><div class="line">haproxy:1.7.8-alpine</div></pre></td></tr></table></figure>
<p>haproxy配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">#cat /etc/haproxy/haproxy.cfg </div><div class="line">global</div><div class="line">  log 127.0.0.1 local0 err</div><div class="line">  maxconn 50000</div><div class="line">  uid 99</div><div class="line">  gid 99</div><div class="line">  #daemon</div><div class="line">  nbproc 1</div><div class="line">  pidfile haproxy.pid</div><div class="line"></div><div class="line">defaults</div><div class="line">  mode http</div><div class="line">  log 127.0.0.1 local0 err</div><div class="line">  maxconn 50000</div><div class="line">  retries 3</div><div class="line">  timeout connect 5s</div><div class="line">  timeout client 30s</div><div class="line">  timeout server 30s</div><div class="line">  timeout check 2s</div><div class="line"></div><div class="line">listen admin_stats</div><div class="line">  mode http</div><div class="line">  bind 0.0.0.0:1080</div><div class="line">  log 127.0.0.1 local0 err</div><div class="line">  stats refresh 30s</div><div class="line">  stats uri     /haproxy-status</div><div class="line">  stats realm   Haproxy\ Statistics</div><div class="line">  stats auth    will:will</div><div class="line">  stats hide-version</div><div class="line">  stats admin if TRUE</div><div class="line"></div><div class="line">frontend k8s-https</div><div class="line">  bind 0.0.0.0:8443</div><div class="line">  mode tcp</div><div class="line">  #maxconn 50000</div><div class="line">  default_backend k8s-https</div><div class="line"></div><div class="line">backend k8s-https</div><div class="line">  mode tcp</div><div class="line">  balance roundrobin</div><div class="line">  server lab1 192.168.1.81:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</div><div class="line">  server lab2 192.168.1.82:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</div><div class="line">  server lab3 192.168.1.83:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</div></pre></td></tr></table></figure>
<h2 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml</div><div class="line"></div><div class="line">#或者老版本的calico</div><div class="line">curl https://docs.projectcalico.org/v3.15/manifests/calico.yaml -o calico.yaml</div></pre></td></tr></table></figure>
<h2 id="dashboard"><a href="#dashboard" class="headerlink" title="dashboard"></a>dashboard</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">kubectl apply -f  https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc7/aio/deploy/recommended.yaml</div><div class="line"></div><div class="line">#暴露 dashboard 服务端口 (recommended中如果已经定义了 30000这个nodeport，所以这个命令不需要了)</div><div class="line">kubectl port-forward -n kubernetes-dashboard  svc/kubernetes-dashboard 30000:443 --address 0.0.0.0</div></pre></td></tr></table></figure>
<p>dashboard login token：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#kubectl describe secrets -n kubernetes-dashboard   | grep token | awk &apos;NR==3&#123;print $2&#125;&apos;</div><div class="line">eyJhbGciOiJSUzI1NiIsImtpZCI6IndRc0hiMkdpWHRwN1FObTcyeUdhOHI0eUxYLTlvODd2U0NBcU1GY0t1Sk0ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLXRia3o5Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwYzM2MzBhOS0xMjBjLTRhNmYtYjM0ZS0zM2JhMTE1OWU1OWMiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6ZGVmYXVsdCJ9.SP4JEw0kGDmyxrtcUC3HALq99Xr99E-tie5fk4R8odLJBAYN6HxEx80RbTSnkeSMJNApbtwXBLrp4I_w48kTkr93HJFM-oxie3RVLK_mEpZBF2JcfMk6qhfz4RjPiqmG6mGyW47mmY4kQ4fgpYSmZYR4LPJmVMw5W2zo5CGhZT8rKtgmi5_ROmYpWcd2ZUORaexePgesjjKwY19bLEXFOwdsqekwEvj1_zaJhKAehF_dBdgW9foFXkbXOX0xAC0QNnKUwKPanuFOVZDg1fhyV-eyi6c9-KoTYqZMJTqZyIzscIwruIRw0oauJypcdgi7ykxAubMQ4sWEyyFafSEYWg</div></pre></td></tr></table></figure>
<p>dashboard 显示为空的话(留意报错信息，一般是用户权限，重新授权即可)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kubectl delete clusterrolebinding kubernetes-dashboard</div><div class="line">kubectl create clusterrolebinding kubernetes-dashboard --clusterrole=cluster-admin --serviceaccount=kube-system:kubernetes-dashboard --user=&quot;system:serviceaccount:kubernetes-dashboard:default&quot;</div></pre></td></tr></table></figure>
<p>其中：system:serviceaccount:kubernetes-dashboard:default 来自于报错信息中的用户名</p>
<h2 id="prometheus"><a href="#prometheus" class="headerlink" title="prometheus"></a>prometheus</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/coreos/kube-prometheus.git</div><div class="line">kubectl apply -f manifests/setup</div><div class="line">kubectl apply -f manifests/</div></pre></td></tr></table></figure>
<p>暴露grafana端口：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl port-forward --address 0.0.0.0 svc/grafana -n monitoring 3000:3000</div></pre></td></tr></table></figure>
<h2 id="部署应用"><a href="#部署应用" class="headerlink" title="部署应用"></a>部署应用</h2><h3 id="DRDS-deployment"><a href="#DRDS-deployment" class="headerlink" title="DRDS deployment"></a>DRDS deployment</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Namespace</div><div class="line">metadata:</div><div class="line">  name: drds</div><div class="line"></div><div class="line">---</div><div class="line"></div><div class="line">apiVersion: apps/v1</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: drds-deployment</div><div class="line">  namespace: drds</div><div class="line">  labels:</div><div class="line">    app: drds-server</div><div class="line">spec:</div><div class="line">  # 创建2个nginx容器</div><div class="line">  replicas: 3</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      app: drds-server</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: drds-server</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: drds-server</div><div class="line">        image: registry:5000/drds-image:v5_wisp_5.4.5-15940932</div><div class="line">        ports:</div><div class="line">        - containerPort: 8507</div><div class="line">        - containerPort: 8607</div><div class="line">        env:</div><div class="line">        - name: diamond_server_port</div><div class="line">          value: &quot;8100&quot;</div><div class="line">        - name: diamond_server_list</div><div class="line">          value: &quot;192.168.0.79,192.168.0.82&quot;</div><div class="line">        - name: drds_server_id</div><div class="line">          value: &quot;1&quot;</div></pre></td></tr></table></figure>
<h3 id="DRDS-Service"><a href="#DRDS-Service" class="headerlink" title="DRDS Service"></a>DRDS Service</h3><p>每个 drds 容器会通过8507提供服务，service通过3306来为一组8507做负载均衡，这个service的3306是在cluster-ip上，外部无法访问</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: drds-service</div><div class="line">  namespace: drds</div><div class="line">spec:</div><div class="line">  selector:</div><div class="line">    app: drds-server</div><div class="line">  ports:</div><div class="line">    - protocol: TCP</div><div class="line">      port: 3306</div><div class="line">      targetPort: 8507</div></pre></td></tr></table></figure>
<p>通过node port来访问 drds service（同时会有负载均衡）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl port-forward --address 0.0.0.0 svc/drds-service -n drds 3306:3306</div></pre></td></tr></table></figure>
<h3 id="部署mysql-statefulset应用"><a href="#部署mysql-statefulset应用" class="headerlink" title="部署mysql statefulset应用"></a>部署mysql statefulset应用</h3><p>一旦 PV 和 PVC 绑定后，<code>PersistentVolumeClaim</code> 绑定是排他性的，不管它们是如何绑定的。 PVC 跟 PV 绑定是一对一的映射。</p>
<p>创建 pv controller 和pvc</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">#cat mysql-pv.yaml </div><div class="line">apiVersion: v1</div><div class="line">kind: PersistentVolume</div><div class="line">metadata:</div><div class="line">  name: simple-pv-volume</div><div class="line">  labels:</div><div class="line">    type: local</div><div class="line">spec:</div><div class="line">  storageClassName: manual</div><div class="line">  capacity:</div><div class="line">    storage: 20Gi</div><div class="line">  accessModes:</div><div class="line">    - ReadWriteOnce</div><div class="line">  hostPath:</div><div class="line">    path: &quot;/mnt/simple&quot;</div><div class="line">---</div><div class="line">apiVersion: v1</div><div class="line">kind: PersistentVolumeClaim</div><div class="line">metadata:</div><div class="line">  name: pv-claim</div><div class="line">spec:</div><div class="line">  storageClassName: manual</div><div class="line">  accessModes:</div><div class="line">    - ReadWriteOnce</div><div class="line">  resources:</div><div class="line">    requests:</div><div class="line">      storage: 20Gi</div></pre></td></tr></table></figure>
<p>drds-pv-mysql-0 后面的mysql 会用来做存储，下面用到了三个mysql(需要三个pvc)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">#cat mysql-deployment.yaml </div><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: mysql</div><div class="line">spec:</div><div class="line">  ports:</div><div class="line">  - port: 3306</div><div class="line">  selector:</div><div class="line">    app: mysql</div><div class="line">  clusterIP: None</div><div class="line">---</div><div class="line">apiVersion: apps/v1 </div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: mysql</div><div class="line">spec:</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      app: mysql</div><div class="line">  strategy:</div><div class="line">    type: Recreate</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: mysql</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - image: mysql:5.7</div><div class="line">        name: mysql</div><div class="line">        env:</div><div class="line">          # Use secret in real usage</div><div class="line">        - name: MYSQL_ROOT_PASSWORD</div><div class="line">          value: &quot;123456&quot;</div><div class="line">        ports:</div><div class="line">        - containerPort: 3306</div><div class="line">          name: mysql</div><div class="line">        volumeMounts:</div><div class="line">        - name: mysql-persistent-storage</div><div class="line">          mountPath: /var/lib/mysql</div><div class="line">      volumes:</div><div class="line">      - name: mysql-persistent-storage</div><div class="line">        persistentVolumeClaim:</div><div class="line">          claimName: pv-claim</div></pre></td></tr></table></figure>
<p>清理：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">kubectl delete deployment,svc mysql</div><div class="line">kubectl delete pvc mysql-pv-claim</div><div class="line">kubectl delete pv mysql-pv-volume</div></pre></td></tr></table></figure>
<h2 id="配置-Pod-使用-ConfigMap"><a href="#配置-Pod-使用-ConfigMap" class="headerlink" title="配置 Pod 使用 ConfigMap"></a>配置 Pod 使用 ConfigMap</h2><p>ConfigMap 允许你将配置文件与镜像文件分离，以使容器化的应用程序具有可移植性。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">775  26/08/20 15:22:52 mkdir -p configure-pod-container/configmap/</div><div class="line">776  26/08/20 15:23:01 wget https://kubernetes.io/examples/configmap/game.properties -O configure-pod-container/configmap/game.properties</div><div class="line">777  26/08/20 15:23:04 wget https://kubernetes.io/examples/configmap/ui.properties -O configure-pod-container/configmap/ui.properties</div><div class="line">778  26/08/20 15:23:25 cat configure-pod-container/configmap/game.properties</div><div class="line">779  26/08/20 15:23:41 kubectl create configmap game-config --from-file=configure-pod-container/configmap/</div><div class="line">780  26/08/20 15:23:48 kubectl get configmaps </div><div class="line">781  26/08/20 15:23:59 kubectl describe configmaps game-config </div><div class="line">782  26/08/20 15:24:44 kubectl get configmaps game-config -o yaml</div><div class="line">783  26/08/20 15:26:31 wget https://kubernetes.io/examples/configmap/game-env-file.properties -O configure-pod-container/configmap/game-env-file.properties</div><div class="line">784  26/08/20 15:26:36 find configure-pod-container/</div><div class="line">785  26/08/20 15:26:42 cat configure-pod-container/configmap/game-env-file.properties</div><div class="line">786  26/08/20 15:27:00 kubectl create configmap game-config-env-file --from-env-file=configure-pod-container/configmap/game-env-file.properties</div><div class="line">787  26/08/20 15:28:10 kubectl get configmap -n kube-system kubeadm-config -o yaml</div><div class="line">788  26/08/20 15:28:11 kubectl get configmap game-config-env-file -o yaml</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;kubernetes&quot;&gt;&lt;a href=&quot;#kubernetes&quot; class=&quot;headerlink&quot; title=&quot;kubernetes&quot;&gt;&lt;/a&gt;kubernetes&lt;/h1&gt;&lt;h2 id=&quot;部署&quot;&gt;&lt;a href=&quot;#部署&quot; class=&quot;headerli
    
    </summary>
    
      <category term="docker" scheme="http://yoursite.com/categories/docker/"/>
    
    
      <category term="docker" scheme="http://yoursite.com/tags/docker/"/>
    
      <category term="kubernetes" scheme="http://yoursite.com/tags/kubernetes/"/>
    
  </entry>
  
</feed>
